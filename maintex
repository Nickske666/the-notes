\documentclass[oneside,a4paper]{book}\usepackage{afterpage}\usepackage{amsfonts}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsthm}\usepackage{bm}\usepackage{bussproofs}\usepackage{cancel}\usepackage{color}\usepackage{courier}\usepackage{enumerate}\usepackage{fix-cm}\usepackage{float}\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,headheight=15pt]{geometry}\usepackage[hidelinks]{hyperref}\usepackage{interval}\usepackage{lastpage}\usepackage{libertine}\usepackage{listings}\usepackage{makeidx}\usepackage{mdframed}\usepackage{minted}\usepackage[libertine]{newtxmath}\usepackage{proof}\usepackage{rotating}\usepackage{todonotes}\usepackage{verbatim}\usepackage{fancyhdr}\pagestyle{fancy}\lhead{}\chead{{\large{}\textsc{the notes}}}\rhead{}\lfoot{Tom Sydney Kerckhove}\cfoot{}\rfoot{\thepage{}}\renewcommand{\headrulewidth}{0.40000pt}\renewcommand{\footrulewidth}{0.40000pt}\makeindex\theoremstyle{definition}\newtheorem{thm}{Theorem}[chapter]\newtheorem{prop}[thm]{Property}\newtheorem{pro}[thm]{Proposition}\newtheorem{nte}[thm]{Note}\newtheorem{ex}[thm]{Example}\newtheorem{cex}[thm]{Counterexample}\newtheorem{con}[thm]{Concequence}\newtheorem{lem}[thm]{Lemma}\newmdtheoremenv{de}[thm]{Definition}\newmdtheoremenv{alg}[thm]{Algorithm}\renewcommand{\arraystretch}{1.25000}\renewcommand{\qedsymbol}{$\square$}\renewcommand{\leq}{\leqslant{}}\renewcommand{\geq}{\geqslant{}}\setlength\parindent{0pt}\begin{document}\begin{titlepage}
\thispagestyle{empty}\hbox{
\rule{1.0pt}{\textheight}\hspace{1.00000pt}
\rule{2.0pt}{\textheight}\hspace{2.00000pt}
\rule{3.0pt}{\textheight}\hspace{3.00000pt}
\rule{4.0pt}{\textheight}\hspace{4.00000pt}
\rule{5.0pt}{\textheight}\hspace{5.00000pt}
\rule{6.0pt}{\textheight}\hspace{6.00000pt}
\rule{7.0pt}{\textheight}\hspace{7.00000pt}
\hspace*{0.05\textwidth}\parbox[b]{0.75\textwidth}{
\noindent{}{\fontsize{80}{90}\selectfont{}\textbf{The Notes}}\\
[2.5\baselineskip]
{\huge{}\textsc{Tom Sydney Kerckhove}}\\
[4.0\baselineskip]{\large{}\begin{tabular}{ll}Started&September 28, 2015\\Compiled&\today\\Commit&\input{commit.tex}\\\end{tabular}}\vspace{0.5\textheight}}}\end{titlepage}\tableofcontents{}\newpage{}\vspace{1.00000cm}The code for this pdf was generated by running the `the notes' generator with the following configuration.\begin{verbatim}Config {conf_selection = [All], conf_visualDebug = False, conf_verbose = False, 
conf_ignoreReferenceErrors = False, conf_omitTodos = False, conf_subtitle = Noth
ing, conf_texFileName = "main", conf_bibFileName = "main", conf_pdfFileName = "t
he-notes"}\end{verbatim}
\newpage{}\chapter{Logic}\section{Abstract Logic}It is hard to speak about logic in a pure mathematical fashion as it originated, and still borders on, philosophy. \begin{de}A \index{formula}\textbf{formula} is a string of characters. \end{de}\begin{nte}In fact a formula can be equivalently be defined in other ways but this definition suffices. \end{nte}\begin{de}\label{definition:theory}\label{definition:logic}\label{definition:axiom}\label{definition:grammar}\label{definition:semantics}\label{definition:sentence}A \index{theory}\textbf{theory} or \index{logic}\textbf{logic} is a mathematical framework for proving properties about a certain object domain. Those properties are called \index{theorem}theorems. A \index{theory}theory consists of a \index{grammar}\textbf{grammar}, a set of \index{axiom}\textbf{axiom}s and \index{semantics}\textbf{semantics} for formulae. \begin{enumerate}\item{}A \index{grammar}\textbf{grammar} defines well-formed formulae. A well-formed \index{formula}formula is also called a \index{sentence}\textbf{sentence}. A \index{formula}\textbf{formula} represents an expression if it adheres to the \index{grammar}grammar. \item{}An \index{axiom}\textbf{axiom} is a \index{theorem}theorem that can be asserted without \index{inference}inference. \item{}Semantics dictate the \emph{meaning} of formulae in the \index{logic}logic. \end{enumerate}\end{de}\begin{nte}Theorems are obtained from the axioms by a finite amount of applications of the inference rules. \end{nte}\begin{de}\label{definition:world}A logical \index{world}\textbf{world} is a set of boolean expressions that are true within the framework of certain theory. \todo[color=red,inline,size=\small]{There is a reference to ``boolean expression'' missing here. }\end{de}\begin{de}An axiom schema defines multiple (possibly even infinitely many) axioms via the use of a variable. \end{de}\begin{de}\label{definition:knowledge-base}A \index{knowledge base}\textbf{knowledge base} is a set of boolean expressions in the context of a certain logical world. In a given world, a valid \index{knowledge base}knowledge base is a subset of that world. \end{de}\begin{de}\label{definition:theorem}A \index{theorem}\textbf{theorem} $f$ is a well-formed formula that is provable in a theory $\mathbb{T}$. This is de:: Noted as $\vdash{}f$. \end{de}\begin{de}\label{definition:entails}Let $\mathbb{T}$ be a \index{theory}theory and $KB$ a \index{knowledge base}knowledge base. We say that a \index{knowledge base}knowledge base $KB$ \index{entails}\textbf{entails} a boolean expression $\alpha{}$ if $\alpha{}$ is true in all worlds where $KB$ is a valid \index{knowledge base}knowledge base. \end{de}\begin{de}\label{definition:model}We say a world $m$ is a \index{model}\textbf{model} of an expression $\alpha{}$ if $\alpha{}$ is true in $m$. \end{de}\begin{de}The set of all models of an expression $\alpha{}$ is de:: Noted as $M\kern-2pt\left(\alpha{}\right)$.. \end{de}\begin{nte}With a little notation overloading we also de:: Note ``The intersection of the set of all models of the expressions in a set $S$. '' as $M\kern-2pt\left(S\right)$. \[M\kern-2pt\left(S\right)={\bigcap_{{s\in{}S}}}{M\kern-2pt\left(s\right)}\]\end{nte}\begin{de}Another way of expressing the fact that an expression $\alpha{}$ is entailed by a \index{knowledge base}knowledge base $KB$: $KB\vDash{}\alpha{}$ is using models. \[M\kern-2pt\left(KB\right)\subseteq{}M\kern-2pt\left(\alpha{}\right)\]\end{de}\begin{de}\label{definition:inference}An \index{inference}\textbf{inference} $i$ in a theory $\mathbb{T}$ is a procedure for proving sentences from a \index{knowledge base}knowledge base. If a theorem $\mathbb{T}$ can be proven using $i$ we de:: Note this as ${\vdash}_{i}f$. \end{de}\begin{de}An inference rule is written as follows. It means that if theorems ${f}_{1}, {f}_{2}, \dotsc{}, {f}_{n}$ can be asserted, we may assert ${f}_{0}$ as a theorem. \[\infer{{f}_{0}}{{f}_{1},\,{f}_{2},\,\dotsc{},\,{f}_{n}}\]The sentences above the line are called the \index{hypotheses}\textbf{hypotheses} or antecedents and the sentence below the line is called the \index{conclusion}\textbf{conclusion}. \end{de}\begin{de}\label{definition:sound}An \index{inference}inference $i$ is called \index{sound}\textbf{sound} if every \index{theorem}theorem produced by $i$ is a true \index{formula}formula. \[\forall{}\alpha{}, KB:\ KB{\vdash}_{i}\alpha{}\Rightarrow{}KB\vDash{}\alpha{}\]\end{de}\begin{de}An \index{inference}inference $i$ is called \index{complete}\textbf{complete} if every true \index{formula}formula can be established as a theorem by $i$. \[\forall{}\alpha{}, KB:\ KB\vDash{}\alpha{}\Rightarrow{}KB{\vdash}_{i}\alpha{}\]\end{de}\begin{ex}Let $\mathbb{I}$ be a theory with a \index{grammar}grammar ${G}_{\mathbb{I}}$ a set ${I}_{\mathbb{I}}$ of inference rules and a set ${A}_{\mathbb{I}}$ of axioms. \begin{enumerate}\item{}${G}_{\mathbb{I}}$ defines a formula to be well-formed if it is of the following form: \begin{itemize}\item{}`${i}_{1}={i}_{2}$' where ${i}_{1}$ and ${i}_{2}$ are integer expressions. \item{}`${i}_{1}<{i}_{2}$' where ${i}_{1}$ and ${i}_{2}$ are integer expressions. \item{}`$\neg{}b$' where $b$ is a boolean expression. \item{}`${b}_{1}\Rightarrow{}{b}_{2}$' where ${b}_{1}$ and ${b}_{2}$ are boolean expressions. \end{itemize}An `integer expression' is an expression of one the following forms. \begin{itemize}\item{}$0$\item{}A variable $n$\item{}$S\kern-2pt\left(n\right)$ Where $n$ is an integer expression. \end{itemize}\item{}The axioms are $\vdash{}0<S\kern-2pt\left(0\right)$ and the axioms defined by the following axiom schema:\[\vdash{}f<g\Rightarrow{}S\kern-2pt\left(f\right)<S\kern-2pt\left(g\right)\]\end{enumerate}In this example theory, the following could be a sound, but not complete, inference rule:\[\infer{\forall{}f:\ P\kern-2pt\left(f\right)}{S\kern-2pt\left(0\right),\,\forall{}f:\ P\kern-2pt\left(f\right)\Rightarrow{}P\kern-2pt\left(S\kern-2pt\left(f\right)\right)}\]This rule is called \index{induction}\textbf{induction}.\end{ex}\begin{de}\label{definition:modus-ponens}The \index{modus ponens}\textbf{modus ponens} \index{inference}inference rule is common to many theories. \[\infer{q}{p,\,p\Rightarrow{}q}\]\end{de}\section{Propositional Logic}\begin{de}\label{definition:propositional-logic}The \index{Propositional logic}\textbf{Propositional logic} has a \index{grammar}grammar ${G}_{\mathbb{I}}$ and only two axioms. \begin{enumerate}\item{}${G}_{\mathbb{I}}$ defines well formed formulas recursively with the following cases. \begin{itemize}\item{}``true'' and ``false'' are sentences. \item{}So-called propositional symbols, boolean variables, are sentences. \item{}If $S$ is a sentence, then $\neg{}S$ is a sentence and it is true only if $S$ is not. \item{}If ${S}_{1}$ and ${S}_{2}$ are sentences, then ${S}_{1}\vee{}{S}_{2}$ is a sentence and it is true only if one of ${S}_{1}$ and ${S}_{2}$ are true. \item{}If ${S}_{1}$ and ${S}_{2}$ are sentences, then ${S}_{1}\wedge{}{S}_{2}$ is a sentence and it is true only if both ${S}_{1}$ and ${S}_{2}$ are true. \end{itemize}\item{}The sentences ``true'' and ``false'' are respesctively asserted to be true and false. \end{enumerate}In propositional logic, a world defines a truth value to every propositional symbol. \end{de}\begin{nte}There are some very common notational shorthands in propositional logic.\begin{itemize}\item{}``${S}_{1}\Rightarrow{}{S}_{2}$'' for ``$\neg{}{S}_{1}\vee{}{S}_{2}$''. \item{}``${S}_{1}\Leftrightarrow{}{S}_{2}$'' for ``$\left({S}_{1}\Rightarrow{}{S}_{2}\right)\wedge{}\left({S}_{2}\Rightarrow{}{S}_{1}\right)$''. \end{itemize}\end{nte}\begin{nte}Truth tables are a very common and naive way of reasoning about sentences propositional logic. The validity of a proposition is checked by enumerating the truth table for the sentence and checking whether all the values in the column for the sentence are true. \begin{figure}[H]\centering{}$\begin{array}[c]{|c|c|}\hline A&\neg{}A\\\hline \hline true&false\\\hline false&true\\\hline \end{array}$\end{figure}\begin{figure}[H]\centering{}$\begin{array}[c]{|c|c|c|c|}\hline A&B&A\vee{}B\\\hline \hline false&false&false\\\hline false&true&true\\\hline true&false&true\\\hline true&true&true\\\hline \end{array}$$\quad{}$$\begin{array}[c]{|c|c|c|c|}\hline A&B&A\wedge{}B\\\hline \hline false&false&false\\\hline false&true&false\\\hline true&false&false\\\hline true&true&true\\\hline \end{array}$\end{figure}\begin{figure}[H]\centering{}$\begin{array}[c]{|c|c|c|c|}\hline A&B&A\Rightarrow{}B\\\hline \hline false&false&true\\\hline false&true&true\\\hline true&false&false\\\hline true&true&true\\\hline \end{array}$$\quad{}$$\begin{array}[c]{|c|c|c|c|}\hline A&B&A\Leftrightarrow{}B\\\hline \hline false&false&true\\\hline false&true&false\\\hline true&false&false\\\hline true&true&true\\\hline \end{array}$$\quad{}$$\begin{array}[c]{|c|c|c|c|}\hline A&B&A\oplus{}B\\\hline \hline false&false&false\\\hline false&true&true\\\hline true&false&true\\\hline true&true&false\\\hline \end{array}$\caption{Elementary truth tables}\end{figure}\end{nte}\subsection{Normal forms}\subsubsection{Conjunctive Normal Form}\begin{de}A sentence in propositional logic is said to be in \index{conjunctive normal form}\textbf{conjunctive normal form} or \index{clausal normal form}\textbf{clausal normal form} (\index{CNF}\textbf{CNF}) if it is a conjunction of clauses where a clause is a disjunction of literals. \end{de}\begin{thm}Every sentence propositional logic can be converted into an equivalent formula that is in CNF. \newline{}\textit{no proof}\newline{}There is a famous transformation called the \index{Tseitin transformation}\textbf{Tseitin transformation} that does exactly this. \cite{tseitin68}\end{thm}\subsection{Inference in propositional logic}\begin{de}The \index{inference}inference \index{rule of resolution}\textbf{rule of resolution} is an inference in proposition logic. Let $a$ and $b$ be propositional formulae in CNF.. \[a={a}_{1}\vee{}{a}_{2}\vee{}\dotsc{}\vee{}{a}_{k}\quad{}b={b}_{1}\vee{}{b}_{2}\vee{}\dotsc{}\vee{}{b}_{l}\]Suppose also that, for some $i$ and $j$, ${a}_{i}=\neg{}{b}_{j}$ holds. \[\infer{{a}_{1}\vee{}{a}_{2}\vee{}\dotsc{}\vee{}{a}_{i-1}\vee{}{a}_{i+1}\vee{}\dotsc{}\vee{}{a}_{k}\vee{}{b}_{1}\vee{}{b}_{2}\vee{}\dotsc{}\vee{}{b}_{j-1}\vee{}{b}_{j+1}\vee{}\dotsc{}\vee{}{b}_{k}}{{a}_{1}\vee{}{a}_{2}\vee{}\dotsc{}\vee{}{a}_{k},\,{b}_{1}\vee{}{b}_{2}\vee{}\dotsc{}\vee{}{b}_{l}}\]\end{de}\begin{thm}This \index{inference}inference is \index{sound}sound and \index{complete}complete.. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{nte}Eventhough this \index{inference}inference is \index{sound}sound and \index{complete}complete, finding proofs can be difficult as search spaces become exponentially large. \todo[color=red,inline,size=\small]{Citation needed}\end{nte}\section{First Order Logic}\begin{de}While propositional logic is about simple facts, first order logic is about complex facts involving objects, relations, functions, etc.... \index{first order logic}\textbf{first order logic} is a \index{theory}theory. It is an extension of propositional logic with predicates, functions, variables and their quantifiers. Remember that these symbols are just that, symbols. \end{de}\begin{de}A \index{term}\textbf{term} in first order logic is either a constant symbol, a variable or a $k$-ary function symbol applied to terms. \end{de}\begin{ex}``$1$'', ``$2$'', ``$3$'', ``$x$'', ``$f\kern-2pt\left(x\right)$'' are terms in first order logic. \end{ex}\begin{de}A sentence in first order logic is called \index{atomic}\textbf{atomic} if it is a constant symbol or a function of only constant symbols. \end{de}\begin{ex}``$1$'', ``$Small\kern-2pt\left(1\right)$'', ``$Smaller\kern-2pt\left(1, 2\right)$'' are atomic sentences in first order logic. \end{ex}\begin{de}A sentence in first order logic is called \index{composite}\textbf{composite} if it atomic, contains free variables an quantifiers, or is composed of composite sentences joined by connectives. \end{de}\begin{ex}``$1$'', ``$Greater\kern-2pt\left(2, 1\right)$'', ``$Great\kern-2pt\left(x\right)$'', ``$\forall{}y:\ Great\kern-2pt\left(x\right)\vee{}Greater\kern-2pt\left(x, y\right)$'' are composite sentences in first order logic. \end{ex}\begin{de}A \index{model}model in first order logic consists of instantiations of objects, relations and functions. Any constants not in the model is asserted to be false. \end{de}\subsection{Quantifiers}Quantifiers bind free variables. \begin{de}The \index{existential quantifier}\textbf{existential quantifier} $\exists{}$ . A sentence $\exists{}x:\ P\kern-2pt\left(x\right)$, in the context of a model $m$ is defined to hold true if there exists a $x$ in $m$ such that the predicate $P$ holds for $x$. \end{de}\begin{de}The \index{universal quantifier}\textbf{universal quantifier} $\forall{}$ . A sentence $\forall{}x:\ P\kern-2pt\left(x\right)$, in the context of a model $m$ is defined to hold true if the predicate $P$ holds for every instantiation of $x$ in $m$. \end{de}\begin{thm}The order of multiple contiguous existential quantifiers does not matter. \[\left(\exists{}x:\ \exists{}y:\ P\kern-2pt\left(x, y\right)\right)\Leftrightarrow{}\left(\exists{}y:\ \exists{}x:\ P\kern-2pt\left(x, y\right)\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}The order of multiple contiguous universal quantifiers does not matter. \[\left(\forall{}x:\ \forall{}y:\ P\kern-2pt\left(x, y\right)\right)\Leftrightarrow{}\left(\forall{}y:\ \forall{}x:\ P\kern-2pt\left(x, y\right)\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{cex}The order of different quantifiers \textbf{does} matter. \[\neg{}\left(\left(\exists{}x:\ \forall{}y:\ P\kern-2pt\left(x, y\right)\right)\Leftrightarrow{}\left(\forall{}y:\ \exists{}x:\ P\kern-2pt\left(x, y\right)\right)\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{cex}\begin{thm}Each quantifier can be expressed in terms of the other. \[\left(\forall{}x:\ P\kern-2pt\left(x\right)\right)\Leftrightarrow{}\left(\neg{}\exists{}x:\ \neg{}P\kern-2pt\left(x\right)\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsection{Situational Calculus}The use of \index{situational calculus}\textbf{situational calculus} is to model situations. In situational calculus, facts hold at a certain moment and/or in a certain situation. This is modeled by adding a situational argument to every non-eternal predicate. Situational calculus can be used to model change, non-change, actions, perceptions, etc.... \subsubsection{The frame problem}Now that we can model situations using frames, there is a need for so called \index{effect axioms}\textbf{effect axioms} that model changes due to actions. In addition to modeling change, one must also model non-change. The frame problem is that the number of frame axioms can be become large and even infinite. This poses problems in automated inference. To solve the problem, we will use so called \index{successor state axioms}\textbf{successor state axioms} that model how each non-eternal predicate is affected or not affected by actions. These successor state axioms model the fact that a predicate is true if and only if an action made it true or it was already true and no action made it false. \subsubsection{Planning using first order logic}First order logic can be used to plan actions based on a knowledge base of known facts. The idea is to decide what the goal situation is and to model it. Then, automated inference can be used to find out whether the given knowledge base entails the goal situation. \subsection{Inference in first order logic}Inference in first order logic is more complicated than inference in propositional logic. In general, there are two approaches: Propositionalisation and ``lifted'' inference. \begin{de}\index{propositionalisation}\textbf{propositionalisation} is an \index{inference}inference in first order logic. It consists of replacing all quantified variables with so called \index{grounding variables}\textbf{grounding variables} using each possible term. This turns the problem into a propositional logic problem and it can then be solved as discussed before. \end{de}The problem with proportionalisation is that the solver may need to create a lot of unnecessary symbols. Even worse, the amount of created symbols could be infinite. \begin{thm}\index{Herbrand's theorem}\textbf{Herbrand's theorem}. \newline{}If a sentence in entailed by a first order logic knowledge base, then there exists a proof using only a finite subset of the propositionalized knowledge base. \cite{herbrand-theorem}\end{thm}Given this theorem, we can propose a naive algorithm to test whether a given sentence $\alpha{}$ is entailed by a given first order logic knowledge base. The algorithm consists of enumerating all finite subsets of the propositionalised knowledge base $KB\cup{}\neg{}\alpha{}$ and checking whether they are satisfiable one by one using propositional resolution. Note that this algorithm will stop if the given sentence is entailed by the given knowledge base but will never stop otherwise. This is intrinsic to the problem. First order logic is only semi-decidable. \index{lifted inference}\textbf{lifted inference} is a set of \index{inference}inferences in first order logic. It consists of trying to infer sentences \emph{without} instantiating variables at all using propositional inference by lifting its inferences. \begin{de}The \index{generalized modus ponens}\textbf{generalized modus ponens} is an \index{inference}inference in first order logic. Let ${p'}_{1}, \dotsc{}, {p'}_{n}$ and ${p}_{1}, \dotsc{}, {p}_{n}$ be sentences in first order logic. Let $\theta{}$ be a substitution and $Subst\kern-2pt\left(\theta{}, q\right)$ its application to $q$. Suppose $Subst\kern-2pt\left(\theta{}, {p'}_{i}\right)=Subst\kern-2pt\left(\theta{}, {p}_{i}\right)$ holds. \[\infer{Subst\kern-2pt\left(\theta{}, q\right)}{{p'}_{1}, \dotsc{}, {p'}_{n},\,{p}_{1}\wedge{}\dotsb{}\wedge{}{p}_{n}\Rightarrow{}q}\]\end{de}\begin{thm}The generalized modus ponens is not \index{complete}complete. \todo[color=red,inline,size=\small]{There either is a proof missing here or a confirmation that no proof is required at all.}\end{thm}There also exists a lifted variant of resolution. \todo[color=red,inline,size=\small]{Describe this variant}It is sound and refutation-complete. \todo[color=red,inline,size=\small]{define refutation-complete}\section{Hoare Logic}\index{Hoare logic}\textbf{Hoare logic} is used to reason about imperative computer programs in abstract machines that have a \index{state}state. \begin{de}\label{definition:state}A \index{state}\textbf{state} is an assignment of values to abstract symbols. \end{de}\begin{nte}In computers these values are typically finite strings of bits but they can be arbitrary values in theory. In logical reasoning, the values are typically (unbounded) integers. \end{nte}\begin{ex}$a\mapsto{}4, b\mapsto{}3$ could be a \index{state}state. \end{ex}\begin{de}\label{definition:evaluation}The \index{evaluation}\textbf{evaluation} ${\left[|b|\right]}_{S}$ of a symbol $b$ with respect to a program state $S$ is the value of $b$ in $S$. \[{\left[|b|\right]}_{S}=a\Leftrightarrow{}b\mapsto{}a\in{}a\]\end{de}\begin{de}\label{definition:instruction}An \index{instruction}\textbf{instruction} in such an abstract machine is a procedure of modifying that \index{state}state. \end{de}\begin{nte}While we say `modify' it is perfectly valid to model a modification as a reconstruction with different variables. There is no real difference in math, but this difference manifests itself physically in real machines. \end{nte}\begin{de}\label{definition:assertion}\label{definition:satisfies}An \index{assertion}\textbf{assertion} is a predicate on the set of posssible program states. A program \index{state}state $S$ \index{satisfies}\textbf{satisfies} an \index{assertion}assertion if the \index{assertion}assertion holds for that program state. \todo[color=red,inline,size=\small]{define assertions recursively, see the separation logic part}\end{de}\begin{ex}$x\mapsto{}5, y\mapsto{}10\vDash{}x<y\wedge{}x>0$\end{ex}\begin{ex}$x\mapsto{}25\vDash{}\exists{}y:\ y>x$ holds. Note that the $y$ doesn't have to be the value of a variable. \end{ex}\begin{de}$a:=b$ represents the instruction to assign the value of $b$ to the variable a. \end{de}\begin{ex}If the program state holds $a\mapsto{}3, b\mapsto{}5$ and the instruction $a:=b$ is performed, the state afterwards would be $a\mapsto{}5, b\mapsto{}5$. \end{ex}\begin{de}\label{definition:hoare-logic}\label{definition:hoare-triple}\label{definition:precondition}\label{definition:postcondition}\label{definition:partial-correctness}\label{definition:total-correctness}\index{Hoare logic}\textbf{Hoare logic} is a \index{theory}theory. In \index{Hoare logic}Hoare logic, well-formed \index{formula}formulae are \index{Hoare triple}\textbf{Hoare triple}s. \[\left\{P\right\}\,A\,\left\{Q\right\}\]Here, $P$ and $Q$ are assertions and $A$ is a sequence of instructions for an abstract machine. $P$ is called the \index{precondition}\textbf{precondition} and $Q$ is called the \index{postcondition}\textbf{postcondition}. An \index{assertion}\textbf{assertion} is a \index{predicate}predicate on the set of states. A true sencence in \index{Hoare logic}Hoare logic describes the fact that the program $A$ will, started in any machine \index{state}state satisfying $P$ will, if it terminates, yield a \index{state}state satisfying $Q$. This is called \index{partial correctness}\textbf{partial correctness}. If a \index{Hoare triple}Hoare triple is partially correct and $A$ is guaranteed to \emph{terminate} as well, this is called \index{total correctness}\textbf{total correctness}. \end{de}\begin{nte}An employee that needs to implement correct programs for given pre- and postconditions should look for the strongest preconditions and the weakest postconditions. Specifications as such will leave him with the least amount of work to do. The following Hoare specification would give him the best job in the world. \[\left\{false\right\}\,A\,\left\{P\right\}\]Any program $A$ is totally correct with respect to this specification. \newline{}The second best job in the world would be the following specification. \[\left\{\right\}\,A\,\left\{true\right\}\]Any program $A$ is partially with respect to this specification. The only thing the programmer would have to do is to make sure that the program terminates. \end{nte}\begin{de}\label{definition:consequence}The  rule of \index{consequence}consequence is an \index{inference}inference in \index{Hoare logic}Hoare logic. \[\infer{\left\{P'\right\}\,A\,\left\{Q'\right\}}{\left\{P\right\}\,A\,\left\{Q\right\},\,P'\Rightarrow{}P,\,Q\Rightarrow{}Q'}\]A precondition can be replaced with a stronger precondition and a postcondition can be replaced by a weaker postcondition. \end{de}\begin{ex}\[\infer{\left\{x>3\right\}\,y:=x+2\,\left\{y>0\right\}}{\left\{x>1\right\}\,y:=x+2\,\left\{y>2\right\},\,x>3\Rightarrow{}x>1,\,y>0\Rightarrow{}y>2}\]\end{ex}\begin{de}\label{definition:conjunction}The  rule of \index{conjunction}\textbf{conjunction} is an \index{inference}inference in \index{Hoare logic}Hoare logic. \[\infer{\left\{P\right\}\,A\,\left\{Q\wedge{}R\right\}}{\left\{P\right\}\,A\,\left\{Q\right\},\,\left\{P\right\}\,A\,\left\{R\right\}}\]\end{de}\begin{ex}\[\infer{\left\{true\right\}\,x:=3\,\left\{x>2\wedge{}x>4\right\}}{\left\{true\right\}\,x:=3\,\left\{x>2\right\},\,\left\{true\right\}\,x:=3\,\left\{x<4\right\}}\]\end{ex}\begin{de}\label{definition:sequential-composition}The  rule of \index{sequential composition}\textbf{sequential composition} is an \index{inference}inference in \index{Hoare logic}Hoare logic. \[\infer{\left\{P\right\}\,A;\ B\,\left\{R\right\}}{\left\{P\right\}\,A\,\left\{Q\right\},\,\left\{Q\right\}\,B\,\left\{R\right\}}\]Instructions can be sequenced as long as their conditions line up. \end{de}\begin{ex}\[\infer{\left\{x>0\right\}\,x:=x+3;\ x:=x+1\,\left\{x>4\right\}}{\left\{x>0\right\}\,x:=x+3\,\left\{x>3\right\},\,\left\{x>3\right\}\,x:=x+1\,\left\{x>4\right\}}\]\end{ex}\begin{de}\label{definition:skip}The \index{skip}\textbf{skip} \index{Hoare triple}Hoare triple is an \index{axiom schema}axiom schema in \index{Hoare logic}Hoare logic. \[\forall{}P:\ \left\{P\right\}\,\index{skip}skip\,\left\{P\right\}\]\end{de}\begin{de}\label{definition:abort}The \index{abort}\textbf{abort} \index{Hoare triple}Hoare triple is an \index{axiom schema}axiom schema in \index{Hoare logic}Hoare logic. \[\forall{}P:\ \left\{false\right\}\,\index{abort}abort\,\left\{P\right\}\]\end{de}\begin{de}\label{definition:substitution}$P\left[e / x\right]$ is the expression obtained from $P$ by \index{substitution}\textbf{substitution} of every occurence of $x$ by $e$. Read it as ``$P$ with $e$ instead of $x$. ''. \end{de}\begin{ex}``$\left(y:=x\right)\left[z / y\right]$'' = ``$z:=x$''\end{ex}\begin{ex}``$\left(y:=x\right)\left[x+1 / x\right]$'' = ``$x:=x+1$''\end{ex}\begin{de}\label{definition:assignment}The \index{assignment}\textbf{assignment} of variables is an \index{axiom schema}axiom schema in \index{Hoare logic}Hoare logic. \[\forall{}P, e, x:\ \left\{P\left[e / x\right]\right\}\,x:=e\,\left\{P\right\}\]\end{de}\begin{ex}$\left\{y>z-2\right\}\,x:=x+1\,\left\{y>z-2\right\}$\end{ex}\begin{ex}$\left\{2+2=5\right\}\,x:=x+1\,\left\{2+2=5\right\}$\end{ex}\begin{ex}$\left\{y>0\right\}\,x:=y\,\left\{x>0\right\}$\end{ex}\begin{ex}$\left\{x+1>0\right\}\,x:=x+1\,\left\{x>0\right\}$Make sure to read this twice. Notice that it is not at all useful in this context. \end{ex}\begin{ex}The \index{assignment}assignment \index{axiom schema}axiom schema can \textbf{not} be used to prove following \index{Hoare triple}Hoare triple. \[\left\{x>0\right\}\,x:=x+1\,\left\{x>1\right\}\]\todo[color=red,inline,size=\small]{Why? More of an explanation is missing here.}\end{ex}\begin{nte}There are limits to the assignment axiom schema. It is assumed that the assigned expression is side-effect-free. This always holds in mathematics, but infrequently in real machines. \end{nte}\begin{de}\label{definition:forward-assignment}There is also a `forward version' of the assignment axiom. \[\left\{P\right\}\,x:=e\,\left\{\exists{}{x}^{old}:\ \left(P\left[{x}^{old} / x\right]\right)\wedge{}\left(x=e\left[{x}^{old} / x\right]\right)\right\}\]\end{de}\begin{de}\label{definition:free-variable}A variable $x$ is said to be a \index{free variable}\textbf{free variable} in an expression $P$ if $P$ doesn't quantify $x$ either existentially or universally. \newline{}$FV\kern-2pt\left(P\right)$ is the set of all \index{free variable}free variables in an expression $P$. \end{de}\begin{de}\label{definition:modify}A program $A$ is said to \index{modify}\textbf{modify} a variable $x$ if at any point, $A$ assigns to $x$. \newline{}$modifies\kern-2pt\left(A\right)$ is the set of all variables that $A$ modifies. \end{de}\begin{de}The \index{rule of constancy}\textbf{rule of constancy} is an \index{inference}inference in Hoare Logic. Let $R$ be an assertion. \[\infer{\left\{P\wedge{}R\right\}\,A\,\left\{Q\wedge{}R\right\}}{\left\{P\right\}\,A\,\left\{Q\right\},\,FV\kern-2pt\left(R\right)\cap{}modifies\kern-2pt\left(A\right)\underset{set}{=}\emptyset{}}\]This is known as ``Whatever $A$ doesn't modify, stays the same. ''. \end{de}\begin{ex}\[\infer{\left\{x=0\wedge{}y=3\right\}\,x:=x+1\,\left\{x=1\wedge{}y=3\right\}}{\left\{x=0\right\}\,x:=x+1\,\left\{x=1\right\},\,FV\kern-2pt\left(y=3\right)\cap{}modifies\kern-2pt\left(x:=x+1\right)\underset{set}{=}\emptyset{}}\]\end{ex}\begin{ex}\[\infer{\left\{x=4\wedge{}y=3\right\}\,x:=\sqrt{y}\,\left\{z=2\wedge{}y=3\right\}}{\left\{x=4\right\}\,x:=\sqrt{y}\,\left\{z=2\right\},\,FV\kern-2pt\left(y=3\right)\cap{}modifies\kern-2pt\left(x:=\sqrt{y}\right)\underset{set}{=}\emptyset{}}\]\end{ex}\todo[color=red,inline,size=\small]{The assignment axiom for arrays}\begin{de}The \index{conditional rule}\textbf{conditional rule} is an \index{inference}inference in Hoare Logic. \[\infer{\left\{P\right\}\,\text{\textbf{if }}c\text{\textbf{ then }}A\text{\textbf{ else }}B\text{\textbf{ end}}\,\left\{Q\right\}}{\left\{P\wedge{}c\right\}\,A\,\left\{Q\right\},\,\left\{P\wedge{}\neg{}c\right\}\,B\,\left\{Q\right\}}\]\end{de}\begin{ex}\[\infer{\left\{y>0\right\}\,\text{\textbf{if }}x>0\text{\textbf{ then }}y:=y+x\text{\textbf{ else }}y:=y-x\text{\textbf{ end}}\,\left\{y>0\right\}}{\left\{y>0\wedge{}x>0\right\}\,y:=y+x\,\left\{y>0\right\},\,\left\{y>0\wedge{}\neg{}\left(x>0\right)\right\}\,y:=y-x\,\left\{y>0\right\}}\]\end{ex}\begin{de}\label{definition:loop-rule}The \index{loop rule}\textbf{loop rule} is an \index{inference}inference in Hoare Logic. \[\infer{\left\{P\right\}\,\text{\textbf{from }}A\text{\textbf{ until }}c\text{\textbf{ loop }}B\text{\textbf{ end}}\,\left\{i\wedge{}c\right\}}{\left\{P\right\}\,A\,\left\{i\right\},\,\left\{i\wedge{}\neg{}c\right\}\,B\,\left\{i\right\}}\]The first triple is called the \index{initiation}\textbf{initiation} and  the second is called the \index{consecution}\textbf{consecution} or \index{inductiveness}\textbf{inductiveness}. This rule is also sometimes written as follows. 
\begin{prooftree}\AxiomC{$\left\{P\right\}\,A\,\left\{i\right\}$}\AxiomC{$\left\{i\wedge{}\neg{}c\right\}\,B\,\left\{i\right\}$}\AxiomC{$i\wedge{}c\Rightarrow{}Q$}\LeftLabel{[loop]}\TrinaryInfC{$\left\{P\right\}\,\text{\textbf{from }}A\text{\textbf{ until }}c\text{\textbf{ loop }}B\text{\textbf{ end}}\,\left\{Q\right\}$}\end{prooftree}
\end{de}\begin{ex}\label{example:loop-rule-example}\[\infer{\left\{y>3\wedge{}n>0\right\}\,\text{\textbf{from }}i:=0\text{\textbf{ until }}i=n\text{\textbf{ loop }}\begin{array}{l}i:=i+1\\y:=y+1\\\end{array}\text{\textbf{ end}}\,\left\{y>3+n\right\}}{\left\{y>3\wedge{}n>0\right\}\,i:=0\,\left\{y>3+i\right\},\,\left\{y>3+i\wedge{}\left(\neg{}i=n\right)\right\}\,\begin{array}{l}i:=i+1\\y:=y+1\\\end{array}\,\left\{y>3+i\right\},\,\left(y>3+i\wedge{}i=n\right)\Rightarrow{}y>3+n}\]\end{ex}\todo[color=red,inline,size=\small]{Loop rule with do while instead of just while.}\subsection{Termination}To show total correctness, rather than just partial correctness, termination must also be proven. Termination is asserted for all but the loop triples if all the antecedents terminate. \begin{de}To prove the total correctness of a loop triple, we must first prove partial correctness and then loop termination as follows. There must exist a set $S$ with a total ordering $\leq{}$ such that $S$ has a least element $\bot{}$ with respect to $\leq{}$. Three more conditions must hold. \begin{enumerate}\item{}$\left\{P\right\}\,A\,\left\{v\geq{}\bot{}\right\}$\item{}$\left\{v\geq{}\bot{}\right\}$ is an invariant of the loop. \[\left\{v\geq{}\bot{}\right\}\,A\,\left\{v\geq{}\bot{}\right\}\]\item{}$v$ decreases with ever iteration. \[\forall{}v':\ v<v'\Rightarrow{}\left\{v=v'\wedge{}\neg{}c\right\}\,B\,\left\{v<v'\right\}\]\end{enumerate}\end{de}\begin{ex}This program is totally correct. \[\left\{y>3\wedge{}n>0\right\}\,\text{\textbf{from }}i:=0\text{\textbf{ until }}i=n\text{\textbf{ loop }}\begin{array}{l}i:=i+1\\y:=y+1\\\end{array}\text{\textbf{ end}}\,\left\{y>3+n\right\}\]\begin{proof}Partial correctness was already proven in an earlier example\footnote{See example \ref{example:loop-rule-example} on page \pageref{example:loop-rule-example}. }. Only termination is left to prove. Let $\left(n-i\right)$ be the variant of the loop. \begin{itemize}\item{}The variant stays positive after initialization. \[\left\{y>3\wedge{}n>0\right\}\,i:=0\,\left\{\left(n-i\right)\geq{}0\right\}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\item{}$\left\{\left(n-i\right)\geq{}0\right\}$ is an invariant of the loop. \[\left\{\left(n-i\right)\geq{}0\right\}\,\begin{array}{l}i:=i+1\\y:=y+1\\\end{array}\,\left\{\left(n-i\right)\geq{}0\right\}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\item{}$\left(n-i\right)$ decreases with ever iteration. \[\forall{}v':\ \left(n-i\right)<v'\Rightarrow{}\left\{\left(n-i\right)=v'\wedge{}\neg{}c\right\}\,B\,\left\{\left(n-i\right)<v'\right\}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{itemize}\end{proof}\end{ex}\begin{ex}The following \index{Hoare triple}Hoare triple is true. \[\left\{x>0\right\}\,x:=x+1;\ \index{skip}skip\,\left\{x>1\right\}\]\begin{proof}\begin{prooftree}\AxiomC{}\LeftLabel{[ass]}\UnaryInfC{$\left\{x+1>1\right\}\,x:=x+1\,\left\{x>1\right\}$}\AxiomC{}\LeftLabel{[EM]}\UnaryInfC{$x>0\Rightarrow{}x+1>1$}\LeftLabel{[conseq]}\BinaryInfC{$\left\{x>0\right\}\,x:=x+1\,\left\{x>1\right\}$}\AxiomC{}\LeftLabel{[skip]}\UnaryInfC{$\left\{x>1\right\}\,\index{skip}skip\,\left\{x>1\right\}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\left\{x>0\right\}\,x:=x+1;\ \index{skip}skip\,\left\{x>1\right\}$}\end{prooftree}\end{proof}\end{ex}\nocite{software-verification-axiomatic-semantics-part1}\begin{ex}\textbf{Exam Question: Software Verification @ ETH, December 2014}\newline{}The following \index{Hoare triple}Hoare triple represents a partially correct program. \[\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\begin{array}{l}\text{\textbf{from}}\\\quad{}\,k:=0\\\text{\textbf{until }}\,k=n\text{\textbf{ loop}}\\\quad{}\begin{array}{l}\begin{array}{l}\text{\textbf{if }}\,k\text{ mod }3=0\\\text{\textbf{then}}\\\quad{}b\left[k\right]:=a\left[k\right]+1\\\text{\textbf{end}}\\\end{array}\\\,k:=k+1\\\end{array}\\\text{\textbf{end}}\\\end{array}\\\left\{\forall{}i:\ \left(\left(0\leq{}i<n\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}\]\begin{proof}The tree for this proof would be large enough to fill an A2 or even an A1 page and must therefore be broken down. The first thing to do for a loop like this, is to find an invariant of the loop. It needs to be strong enough to, in conjunction with the guard, imply the postcondition. A good first suggestion could be the following. \[Inv=\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]Now that we have a loop invariant, we can use the \index{loop rule}loop rule\footnote{See definition \ref{definition:loop-rule} on page \pageref{definition:loop-rule}. } to prove this Hoare triple. This means that we now have three smaller parts to prove. \begin{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\,k:=0\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}The rule of conjunction allows us to split this up again. \begin{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\\\end{array}\]\end{mdframed}We will use the assignment axiom schema to prove this triple. The following hoare triple is an instance of the assignment axiom schema. \begin{prooftree}\AxiomC{}\LeftLabel{[ass]}\UnaryInfC{$\begin{array}{c}\left\{\forall{}i:\ \left(\left(0\leq{}i<0\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\\\end{array}$}\end{prooftree}It's precondition can be rewritten as follows. \[\forall{}i:\ \left(\left(0\leq{}i<0\right)\Rightarrow{}a\left[i\right]=0\right)\Leftrightarrow{}\forall{}i:\ \left(false\Rightarrow{}a\left[i\right]=0\right)\Leftrightarrow{}true\]Thus we have the following triple. \[\left\{true\right\}\,\,k:=0\,\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\]Because anything implies true, we can use the rule of consequence\footnote{See figure \ref{figure:fig:consequence-in-this-exam-thingy1} on page \pageref{figure:fig:consequence-in-this-exam-thingy1}. }. \begin{sidewaysfigure}[!p]\centering{}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{true\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\\\end{array}$}\AxiomC{$\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\Rightarrow{}true$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right\}\\\end{array}$}\end{prooftree}\label{figure:fig:consequence-in-this-exam-thingy1}\caption{Application of the rule of consequence}\end{sidewaysfigure}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}\]\end{mdframed}Analogous to the previous point, the assignment axiom schema gives us this triple. \begin{prooftree}\AxiomC{}\LeftLabel{[ass]}\UnaryInfC{$\begin{array}{c}\left\{\forall{}i:\ \left(\left(0\leq{}i<0\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}$}\end{prooftree}The precondition can again be rewritten as $true$. \begin{align*}\forall{}i:\ \left(\left(0\leq{}i<0\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)&\Leftrightarrow{}\forall{}i:\ \left(false\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\ & \Leftrightarrow{}\forall{}i:\ \left(false\Rightarrow{}\left(b\left[i\right]=1\right)\right)\\ & \Leftrightarrow{}true\end{align*}This gives us the following triple. \[\left\{true\right\}\,\,k:=0\,\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\]Once more we use the rule of consequence to use this triple to proof the given triple\footnote{See figure \ref{figure:fig:consequence-in-this-exam-thingy2} on page \pageref{figure:fig:consequence-in-this-exam-thingy2}. }. \begin{sidewaysfigure}[!p]\centering{}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{true\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}$}\AxiomC{$\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\Rightarrow{}true$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{\left(n>0\right)\wedge{}\left(\forall{}i:\ \left(0\leq{}i<n\right)\Rightarrow{}\left(a\left[i\right]=b\left[i\right]=0\right)\right)\right\}\\\,k:=0\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}$}\end{prooftree}\label{figure:fig:consequence-in-this-exam-thingy2}\caption{Application of the rule of consequence}\end{sidewaysfigure}\end{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\end{array}\right\}\\\begin{array}{l}\begin{array}{l}\text{\textbf{if }}\,k\text{ mod }3=0\\\text{\textbf{then}}\\\quad{}b\left[k\right]:=a\left[k\right]+1\\\text{\textbf{end}}\\\end{array}\\\,k:=k+1\\\end{array}\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}Here we use the rule of sequenctial composition to split this up into two parts. We'll use the following inbetween-assertion. \[\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]\begin{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\end{array}\right\}\\\begin{array}{l}\text{\textbf{if }}\,k\text{ mod }3=0\\\text{\textbf{then}}\\\quad{}b\left[k\right]:=a\left[k\right]+1\\\text{\textbf{end}}\\\end{array}\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}First we apply the conditional rule to this if-then-else construction to split it up into two parts. \begin{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\end{array}\right\}\\b\left[k\right]:=a\left[k\right]+1\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}We'll first have to rewrite the postcondition to get some concretisation of the universal quantifier. \[\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\end{array}\right\}\\b\left[k\right]:=a\left[k\right]+1\\\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}b\left[k\right]=1\\\end{array}\right\}\\\end{array}\]The assignment axiom then gets us the following. \begin{prooftree}\AxiomC{}\LeftLabel{[ass]}\UnaryInfC{$\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\\\end{array}\right\}\\b\left[k\right]:=a\left[k\right]+1\\\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}b\left[k\right]=1\\\end{array}\right\}\\\end{array}$}\end{prooftree}Now we only have to prove that the precondition of the original triple implies the precondition of the second and apply the rule of consequence. \[\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\Rightarrow{}\\\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\\\end{array}\\\end{array}\]The first element in the conjunction at the conclusion can be omitted because they are in the hypothesis. \[\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\Rightarrow{}\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\\\end{array}\]We can intuitively see that in the conjunction of the precondition, the second an fourth elements imply the first element of the postcondition ... \[\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\wedge{}\,k\text{ mod }3=0\\\Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]... and the fourth elemnt implies the second element of the postcondition by itself. \[\,k\text{ mod }3=0\Rightarrow{}\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\]\todo[color=red,inline,size=\small]{Can this be more rigorous?}The rule of consequence can now be used to prove this triple\footnote{See figure \ref{figure:fig:consequence-in-this-exam-thingy2b} on page \pageref{figure:fig:consequence-in-this-exam-thingy2b}. }. \begin{sidewaysfigure}[!p]\centering{}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\\\end{array}\right\}\\b\left[k\right]:=a\left[k\right]+1\\\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}b\left[k\right]=1\\\end{array}\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\Rightarrow{}\\\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\left(k\text{ mod }3=0\right)\Rightarrow{}a\left[k\right]+1=1\\\end{array}\\\end{array}$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3=0\\\end{array}\right\}\\b\left[k\right]:=a\left[k\right]+1\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}$}\end{prooftree}\label{figure:fig:consequence-in-this-exam-thingy2b}\caption{Application of the rule of consequence}\end{sidewaysfigure}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\end{array}\right\}\\\index{skip}skip\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}To prove this triple, we have to prove that the precondition implies the postcondition and then apply the rule of consequence to the skip axiom schema. The first part of the postcondition can be imediately asserted as it is part of a conjunction in the precondition. \[\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\end{array}\]Left to prove is now the following implication. \[\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]The only difference is in the hypothesis of the second implication. If we strip the unnecessary as follows, it becomes intuitively clear that this implication holds. \[\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\wedge{}\,k\text{ mod }3\not=0\\\Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]\todo[color=red,inline,size=\small]{can this be done more rigorously?}The rule of consequence can now be used to prove this triple\footnote{See figure \ref{figure:fig:consequence-in-this-exam-thingy2c} on page \pageref{figure:fig:consequence-in-this-exam-thingy2c}. }. \begin{sidewaysfigure}[!p]\centering{}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\end{array}\right\}\\\index{skip}skip\\\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\end{array}\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\end{array}\\\Rightarrow{}\\\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\end{array}$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,k\not=n\\\wedge{}\,k\text{ mod }3\not=0\\\end{array}\right\}\\\index{skip}skip\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}$}\end{prooftree}\label{figure:fig:consequence-in-this-exam-thingy2c}\caption{Application of the rule of consequence}\end{sidewaysfigure}\end{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\,k:=k+1\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}\]\end{mdframed}First we'll use the rule of constancy to get rid of the first part of each condition\footnote{See figure \ref{figure:fig:consequence-in-this-exam-thingy3} on page \pageref{figure:fig:consequence-in-this-exam-thingy3}. }. \begin{sidewaysfigure}[!p]\centering{}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\,k:=k+1\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}FV\kern-2pt\left(\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\right)\\\cap{}\\modifies\kern-2pt\left(\,k:=k+1\right)\\\underset{set}{=}\emptyset{}\\\end{array}$}\LeftLabel{[const]}\BinaryInfC{$\begin{array}{c}\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\,k:=k+1\\\left\{\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\right\}\\\end{array}$}\end{prooftree}\label{figure:fig:consequence-in-this-exam-thingy3}\caption{Application of the rule of constancy}\end{sidewaysfigure}This leaves us with the following triple to prove. \[\begin{array}{c}\left\{\forall{}i:\ \left(\left(0\leq{}i<k+1\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\,k:=k+1\\\left\{\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\right\}\\\end{array}\]This is an instantiation of the assignment axiom schema. \end{enumerate}\item{}\noindent{}\begin{mdframed}\[\begin{array}{c}\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\\\wedge{}\forall{}i:\ \left(\left(0\leq{}i<k\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\\\wedge{}\,\,k=n\\ \Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<n\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]\end{mdframed}Simply replacing $k$ by $n$ in the hypothesis, gets us this implication. \[\begin{array}{c}\forall{}i:\ \left(\left(0\leq{}i<k\right)\Rightarrow{}a\left[i\right]=0\right)\wedge{}\forall{}i:\ \left(\left(0\leq{}i<n\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\ \Rightarrow{}\forall{}i:\ \left(\left(0\leq{}i<n\right)\wedge{}\left(i\text{ mod }3=0\right)\right)\Rightarrow{}\left(b\left[i\right]=1\right)\\\end{array}\]This is of the form $P\wedge{}Q\Rightarrow{}Q$ and is therefore clearly true. \end{enumerate}\end{proof}\end{ex}\section{Separation Logic}\subsection{Predicates}\begin{de}\label{definition:heap}A \index{heap}\textbf{heap} is a partial function from a set of locations (pointers) to values. The difference between a \index{heap}heap and a program state is that values on the heap can represent other locations on the heap. In this new model, variables will evaluate to locations. The \index{heap}heap will then tell us what the value is that is stored at that location. \end{de}\begin{nte}Watch out, this means that variables aren't stored like they were before, now only references to variables are kept in the store. \end{nte}\begin{de}\label{definition:separation-logic}\index{Separation logic}\textbf{Separation logic} is an extension to \index{Hoare logic}Hoare logic\footnote{See definition \ref{definition:hoare-logic} on page \pageref{definition:hoare-logic}. } that facilitates local reasoning. \index{Separation logic}Separation logic offers spatial connectives that allow for more modular reasoning. In \index{Separation logic}Separation logic program states comprise of both a variable store and a heap. \end{de}\begin{de}Assertion satisfaction needs to be redefined in \index{Separation logic}Separation logic to incorporate the heap. The expression that represents ``A program state $s$, together with a heap $h$, \index{satisfies}\textbf{satisfies} an \index{assertion}assertion $P$. '' is de:: Noted as $s, h\vDash{}P$. It is inductively defined as follows. \begin{itemize}\item{}$s, h\vDash{}false$ never holds. \item{}$s, h\vDash{}true$ always holds. \item{}$s, h\vDash{}P\wedge{}Q\Leftrightarrow{}s, h\vDash{}P\wedge{}s, h\vDash{}Q$\item{}$s, h\vDash{}P\vee{}Q\Leftrightarrow{}s, h\vDash{}P\vee{}s, h\vDash{}Q$\item{}$s, h\vDash{}P\Rightarrow{}Q\Leftrightarrow{}s, h\vDash{}P\Rightarrow{}s, h\vDash{}Q$\item{}$s, h\vDash{}e=f\Leftrightarrow{}{\left[|e|\right]}_{s}={\left[|f|\right]}_{s}$\end{itemize}\end{de}\begin{de}$emp$ represents the assertion that the heap is empty. $s, h\vDash{}emp\Leftrightarrow{}h\underset{set}{=}\emptyset{}$\end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/107649f205a9717d.eps}\caption{A situation in which $emp$ holds. }\end{figure}\end{ex}\begin{de}\label{definition:points-to}$s, h\vDash{}e\mapsto{}f\Leftrightarrow{}\left\{{\left[|e|\right]}_{s}\mapsto{}{\left[|f|\right]}_{s}\right\}\in{}h$... or informally: ``There exists a value $f$ on the heap that $e$ (which may also be a value on the heap) points to and no others. ''. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/9f727bbb64516f94.eps}\caption{A situation in which $e\mapsto{}8$ holds. }\end{figure}\end{ex}\begin{de}$s, h\vDash{}P*Q\Leftrightarrow{}\exists{}{h}_{1}, {h}_{2}:\ {h}_{1}\bot{}{h}_{2}\wedge{}{h}_{1}\circ{}{h}_{2}=h\wedge{}s, {h}_{1}\vDash{}P\wedge{}s, {h}_{2}\vDash{}Q$... or informally: ``The heap can be divided into two parts ${h}_{1}$ and ${h}_{2}$, one where $s, {h}_{1}\vDash{}P$ holds and one where $s, {h}_{2}\vDash{}Q$. ''. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/88773c76fc9f7e96.eps}\caption{A situation in which $s, h\vDash{}x\mapsto{}5*5\mapsto{}z*z\mapsto{}10$ holds. }\end{figure}\end{ex}\begin{thm}\label{theorem:empty-heap-separating-conjunction}The empty heap can be separatingly conjuncted with any assertion. \[P\Rightarrow{}P*emp\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}The notation $e\mapsto{}{f}_{0}, {f}_{1}, \dotsc{}, {f}_{n}$ is a shorthand for the following. \[e\mapsto{}{f}_{0}*e+1\mapsto{}{f}_{1}*\dotsc{}*e+n\mapsto{}{f}_{n}\]\end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/43b26bcc78530c9f.eps}\caption{A situation in which $e\mapsto{}1, 5$ holds. }\end{figure}\end{ex}\begin{de}$s, h\vDash{}P\rightarrow{}\kern-8pt*Q$ is said to hold if and only if ``Extending $h$ with a disjoint part $h'$ that satisfies $P$ results in a new heap satisfying $Q$. ''. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.3333333333333333\textwidth{}]{/tmp/c562160a21fc31b4.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.3333333333333333\textwidth{}]{/tmp/b65bea329852db2a.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.3333333333333333\textwidth{}]{/tmp/c1684d1e2f81f1ca.eps}\caption{}\end{figure}The first situation is an example of a situation in which $s, h\vDash{}\left(x\mapsto{}5\right)\rightarrow{}\kern-8pt*\left(x\mapsto{}5*y\mapsto{}6\right)$ holds.. This assertion holds because the heap could be extended with the disjunct heap from the second situation to produce a heap (the one on the right), that satisfies $x\mapsto{}5*y\mapsto{}6$. \end{ex}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/c33d07de8bff20fa.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/26b61268bedf6281.eps}\caption{Two situations $A$ on the left and $B$ on the right. }\end{figure}\begin{figure}[H]\centering{}$\begin{array}[c]{|c|c|c|c|c|}\hline &A&B\\\hline \hline x\mapsto{}6, 7&false&true\\\hline x\mapsto{}6, 7*true&true&true\\\hline x\mapsto{}6, 7*y\mapsto{}6, 7&true&true\\\hline x\mapsto{}6, 7\wedge{}y\mapsto{}6, 7&false&true\\\hline \left(x\mapsto{}6, 7*true\right)\wedge{}\left(y\mapsto{}6, 7*true\right)&true&true\\\hline \end{array}$\caption{Assertions on the situations}\end{figure}The first assertion doesn't hold for situation $A$ because there are more elements on the heap than just the two mentioned in $x\mapsto{}4, 4$. The third assertion doesn't hold for situation $B$ because the heap cannot be divided into two parts. The fourth assertion doesn't hold for situation $A$ because there are too many elements on the heap. \end{ex}\begin{ex}\textbf{Exam Question: Software Verification @ ETH, December 2014}\newline{}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/6982c0f9b3e80fd0.eps}\caption{A situation in which $x\mapsto{}6, 7*\neg{}\left(x\mapsto{}6, 7\right)$ holds. }\end{figure}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/26b61268bedf6281.eps}\caption{A situation in which $x\mapsto{}6, 7\wedge{}y\mapsto{}6, 7$ holds. }\end{figure}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/39d4727c47ee1f45.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/7c7198a9c73fc215.eps}\caption{Two situations in which $emp\Rightarrow{}x=y$ holds. }\end{figure}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/548f850015b9aab0.eps}\caption{A situation in which $\exists{}i:\ x\mapsto{}i*i\mapsto{}i$ holds. }\end{figure}\end{ex}\begin{ex}\textbf{Exam Question: Software Verification @ ETH, December 2013}\newline{}A well-formed binary tree $t$ is defined by the following grammar. \[t\quad{}\equiv{}\quad{}n\mid{}\left({t}_{1}, {t}_{2}\right)\]These symbols have the following semantics. \begin{itemize}\item{}$tree\kern-2pt\left(n, i\right)\Leftrightarrow{}i\mapsto{}n$\item{}$tree\kern-2pt\left(\left({t}_{1}, {t}_{2}\right), i\right)\Leftrightarrow{}\exists{}l, r:\ i\mapsto{}l, r*tree\kern-2pt\left({t}_{1}, l\right)*tree\kern-2pt\left({t}_{2}, r\right)$\end{itemize}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/ea596c0c1ffc5c82.eps}\caption{An example of a situation in which $tree\kern-2pt\left(\left(7, 8\right), x\right)$ holds. }\end{figure}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/90c205826216457e.eps}\caption{An example of a situation in which $tree\kern-2pt\left(5, x\right)\wedge{}tree\kern-2pt\left(5, y\right)$ holds. }\end{figure}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/a900b54ff850b864.eps}\caption{An example of a situation in which $tree\kern-2pt\left(\left(\left(3, 2\right), 1\right), x\right)$ holds. }\end{figure}\end{ex}\subsection{Heap operations}\begin{de}${\left[e\right]}_{h}$ represents the location of the variable that is said to be stored in $e$. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/edf878da73d4105e.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/c7568f81eef0a974.eps}\caption{An example situation before and after a $x:={\left[y\right]}_{h}$ assignment. }\end{figure}Note that this example is slightly misleading because the variable stored in y is a value and not a pointer. The next example should clear up the confusion. \end{ex}\begin{ex}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/782b09917bbb7574.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/26f753400b91f675.eps}\caption{An example situation before and after a $x:={\left[y\right]}_{h}$ assignment. }\end{figure}\end{ex}\begin{de}${\left[e\right]}_{h}:=f$ represents the instruction to assign the value of $f$ as the contents of ${\left[e\right]}_{h}$ on the heap. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/d8483cbd80864ce6.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/6baaaf592262e3fc.eps}\caption{An example situation before and after a ${\left[x+1\right]}_{h}:=y$ instruction. }\end{figure}\end{ex}\begin{de}$cons\kern-2pt\left({e}_{1}, \dotsc{}, {e}_{n}\right)$ represents the instruction to allocate $n$ consecutive locations that are not in the heap yet, say ${l}_{1}, \dotsc{}, {l}_{n}$ and assign the values of ${e}_{1}, \dotsc{}, {e}_{n}$ to the contents of ${l}_{1}, \dotsc{}, {l}_{n}$ respectively. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/712f99430d7fd2cc.eps}\caption{The result of $x:=cons\kern-2pt\left(1, 2, 5\right)$ starting from an empty heap. }\end{figure}\end{ex}\begin{de}\label{definition:disposal}$dispose\kern-2pt\left(e\right)$ represents the instruction to fetch $e$ to get its location $l$ and remove $l$ from the heap. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/471342486add5aac.eps}\hspace{0.50000cm}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/662134c7055591c1.eps}\caption{An example situation before and after a $dispose\kern-2pt\left(x\right)$ instruction. }\end{figure}\end{ex}\begin{ex}\label{example:separation-logic-example-program}In this example, we'll look at the following simple program and what it does to its store/heap situation. \[\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\y:={\left[y\right]}_{h}\\\end{array}\]Starting from an empty heap, after the first two instructions the situation will be as follows. \noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/830f5f1a16e58e1a.eps}\caption{}\end{figure}The following two instructions leave the situation as follows. \noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/faee018dc0856adc.eps}\caption{}\end{figure}After the fifth instruction, the situation looks as follows. \noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/bdc5a020660859d7.eps}\caption{}\end{figure}The dispose instruction disposes of some allocated heap space but there still exist pointers to it. \noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/e64504cb2c83255e.eps}\caption{}\end{figure}Lastly, the last instruction leaves the heap as follows. \noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.25\textwidth{}]{/tmp/84f3d265e63310ec.eps}\caption{}\end{figure}\end{ex}\subsection{Axioms and inference rules}\begin{de}\label{definition:heap-mutation}$\left\{e\mapsto{}x\right\}\,{\left[e\right]}_{h}:=f\,\left\{e\mapsto{}f\right\}$ is an \index{axiom schema}axiom schema in \index{Separation logic}Separation logic. This is called the \index{heap mutation}\textbf{heap mutation} \index{axiom schema}axiom schema. \end{de}\begin{de}$\left\{e\mapsto{}x\right\}\,dispose\kern-2pt\left(e\right)\,\left\{emp\right\}$ is an \index{axiom schema}axiom schema in \index{Separation logic}Separation logic. \end{de}\begin{de}$\left\{X=x\wedge{}e\mapsto{}Y\right\}\,x:={\left[e\right]}_{h}\,\left\{e\left[X / x\right]\mapsto{}Y\wedge{}Y=x\right\}$ is an \index{axiom schema}axiom schema in \index{Separation logic}Separation logic. \newline{}It means ``If we know that a variable $x$ has location $X$ and a expression $e$ points to a value $Y$ then, after we assign the location of $e$ to $x$, we know that the expression $e$ with all $X$'s replaced by $x$ will now point to the value $Y$ and that $x$ now has location $Y$. ''. \end{de}\begin{de}\label{definition:allocation}$\left\{emp\right\}\,x:=cons\kern-2pt\left({e}_{0}, \dotsc{}, {e}_{n}\right)\,\left\{x\mapsto{}{e}_{0}, \dotsc{}, {e}_{n}\right\}$ is an \index{axiom schema}axiom schema in \index{Separation logic}Separation logic. Note that $x$ must not appear in any of ${e}_{0}, \dotsc{}, {e}_{n}$. This is called the \index{allocation}\textbf{allocation} \index{axiom schema}axiom schema. \end{de}\begin{de}\label{definition:frame-rule}\label{definition:frame}The \index{frame rule}\textbf{frame rule} is the most important rule in \index{Separation logic}Separation logic. \begin{prooftree}\AxiomC{$\left\{P\right\}\,C\,\left\{Q\right\}$}\LeftLabel{[frame]}\UnaryInfC{$\left\{P*r\right\}\,C\,\left\{Q*r\right\}$}\end{prooftree}Here, $r$ is called the \index{frame}\textbf{frame}. Note that no variable modified by $C$ may appear free in $r$. \end{de}\begin{de}The interpretation of a triple in \index{Separation logic}Separation logic needs to be extended from its interpretation in \index{Hoare logic}Hoare logic. In \index{Separation logic}Separation logic a triple $\left\{P\right\}\,C\,\left\{Q\right\}$ means ``If $C$ is executed on a state satisfying $P$ then it will not fault, and if it terminates, that state will satisfy $Q$ afterwards. ''. \end{de}\begin{ex}Retaking the program from an earlier example\footnote{See example \ref{example:separation-logic-example-program} on page \pageref{example:separation-logic-example-program}. }, we will now prove the following triple. \[\left\{emp\right\}\,\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\y:={\left[y\right]}_{h}\\\end{array}\,\left\{y\mapsto{}4*true\right\}\]In \index{Separation logic}Separation logic proofs we reason forwards rather than backwards like in \index{Hoare logic}Hoare logic. We start with an empty heap and see the first instruction: $x:=cons\kern-2pt\left(3, 3\right)$. Applying the \index{allocation}allocation \index{axiom schema}axiom schema gets us the following triple. \[\left\{emp\right\}\,x:=cons\kern-2pt\left(3, 3\right)\,\left\{x\mapsto{}3, 3\right\}\]We get something similar by applying the same axiom schema to the second instruction. \[\left\{emp\right\}\,y:=cons\kern-2pt\left(4, 4\right)\,\left\{y\mapsto{}4, 4\right\}\]But now these need to be combined before we can go on. Note first that we can separatingly conjoin an empty heap to the postcondition of the first triple as follows\footnote{See theorem \ref{theorem:empty-heap-separating-conjunction} on page \pageref{theorem:empty-heap-separating-conjunction}. }. \[x\mapsto{}3, 3\Rightarrow{}x\mapsto{}3, 3*emp\]We can then apply the rule of \index{consequence}consequence\footnote{See definition \ref{definition:consequence} on page \pageref{definition:consequence}. }. \begin{prooftree}\AxiomC{$\left\{emp\right\}\,x:=cons\kern-2pt\left(3, 3\right)\,\left\{x\mapsto{}3, 3\right\}$}\AxiomC{$x\mapsto{}3, 3\Rightarrow{}x\mapsto{}3, 3*emp$}\LeftLabel{[conseq]}\BinaryInfC{$\left\{emp\right\}\,x:=cons\kern-2pt\left(3, 3\right)\,\left\{x\mapsto{}3, 3*emp\right\}$}\end{prooftree}Now we can use the \index{frame rule}frame rule to the second triple we found with $x\mapsto{}3, 3$ as a frame. \begin{prooftree}\AxiomC{$\left\{emp\right\}\,y:=cons\kern-2pt\left(4, 4\right)\,\left\{y\mapsto{}4, 4\right\}$}\LeftLabel{[frame]}\UnaryInfC{$\left\{x\mapsto{}3, 3*emp\right\}\,y:=cons\kern-2pt\left(4, 4\right)\,\left\{y\mapsto{}4, 4*x\mapsto{}3, 3\right\}$}\end{prooftree}Finally we can use the rule of \index{sequential composition}sequential composition\footnote{See definition \ref{definition:sequential-composition} on page \pageref{definition:sequential-composition}. } to combine the first two instructions. \begin{prooftree}\AxiomC{$\left\{emp\right\}\,x:=cons\kern-2pt\left(3, 3\right)\,\left\{x\mapsto{}3, 3*emp\right\}$}\AxiomC{$\left\{x\mapsto{}3, 3*emp\right\}\,y:=cons\kern-2pt\left(4, 4\right)\,\left\{y\mapsto{}4, 4*x\mapsto{}3, 3\right\}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\left\{emp\right\}\,\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\\end{array}\,\left\{y\mapsto{}4, 4*x\mapsto{}3, 3\right\}$}\end{prooftree}The next two instructions give us similar triples using the \index{heap mutation}heap mutation \index{axiom schema}axiom schema. \[\left\{x+1\mapsto{}3\right\}\,{\left[x+1\right]}_{h}:=y\,\left\{x+1\mapsto{}y\right\}\]\[\left\{y+1\mapsto{}4\right\}\,{\left[y+1\right]}_{h}:=x\,\left\{y+1\mapsto{}x\right\}\]We can apply the frame rule with frame $y\mapsto{}4, 4*x\mapsto{}3$. \begin{prooftree}\AxiomC{$\left\{x+1\mapsto{}3\right\}\,{\left[x+1\right]}_{h}:=y\,\left\{x+1\mapsto{}y\right\}$}\LeftLabel{[frame]}\UnaryInfC{$\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}3\right\}\,{\left[x+1\right]}_{h}:=y\,\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}y\right\}$}\end{prooftree}Now we can add the third instruction to the list using the \index{sequential composition}sequential composition rule. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\\end{array}\\\left\{y\mapsto{}4, 4*x\mapsto{}3, 3\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}3\right\}\\{\left[x+1\right]}_{h}:=y\\\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}y\right\}\\\end{array}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\left\{emp\right\}\,\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\\end{array}\,\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}y\right\}$}\end{prooftree}Analogously we can use the frame rule with frame $x\mapsto{}3, y*y\mapsto{}4$ and another application of sequential composition to move forward another instruction. \begin{prooftree}\AxiomC{$\left\{y+1\mapsto{}4\right\}\,{\left[y+1\right]}_{h}:=x\,\left\{y+1\mapsto{}x\right\}$}\LeftLabel{[frame]}\UnaryInfC{$\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}4\right\}\,{\left[y+1\right]}_{h}:=x\,\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}x\right\}$}\end{prooftree}\begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\\end{array}\\\left\{y\mapsto{}4, 4*x\mapsto{}3*x+1\mapsto{}y\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}4\right\}\\{\left[y+1\right]}_{h}:=x\\\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}x\right\}\\\end{array}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\left\{emp\right\}\,\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\\end{array}\,\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}x\right\}$}\end{prooftree}The forward assignment axiom schema\footnote{See definition \ref{definition:forward-assignment} on page \pageref{definition:forward-assignment}. } gives us a triple concerning the fifth instruction. \[\left\{x\mapsto{}3, y*y\mapsto{}4, x\right\}\,y:=x+1\,\left\{\left(x\mapsto{}3, {y}^{old}*{y}^{old}\mapsto{}4, x\right)\wedge{}\left(y=x+1\right)\right\}\]Another application of the rule of \index{sequential composition}sequential composition adds the fifth instruction to the list. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\\end{array}\\\left\{x\mapsto{}3, y*y\mapsto{}4*y+1\mapsto{}x\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\left\{x\mapsto{}3, y*y\mapsto{}4, x\right\}\\y:=x+1\\\left\{\left(x\mapsto{}3, {y}^{old}*{y}^{old}\mapsto{}4, x\right)\wedge{}\left(y=x+1\right)\right\}\\\end{array}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\\end{array}\\\left\{\left(x\mapsto{}3, {y}^{old}*{y}^{old}\mapsto{}4, x\right)\wedge{}\left(y=x+1\right)\right\}\\\end{array}$}\end{prooftree}We use the \index{disposal}disposal \index{axiom schema}axiom schema to get a triple involving the sixth instruction. \[\left\{x\mapsto{}3\right\}\,dispose\kern-2pt\left(x\right)\,\left\{emp\right\}\]We use the \index{frame rule}frame rule with frame $x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1$ as follows. \begin{prooftree}\AxiomC{$\left\{x\mapsto{}3\right\}\,dispose\kern-2pt\left(x\right)\,\left\{emp\right\}$}\LeftLabel{[frame]}\UnaryInfC{$\left\{x\mapsto{}3*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\,dispose\kern-2pt\left(x\right)\,\left\{emp*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}$}\end{prooftree}The rule of consequence allows us to rewrite that postcondition because $emp*P$ implies $P$ for any $P$. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{x\mapsto{}3*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\dispose\kern-2pt\left(x\right)\\\left\{emp*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}x\mapsto{}3*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\\ \Rightarrow{} \\x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\\\end{array}$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{x\mapsto{}3*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\dispose\kern-2pt\left(x\right)\\\left\{x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\\end{array}$}\end{prooftree}Once more, the rule of sequential composition allows us to add an instruction to our list. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\\end{array}\\\left\{\left(x\mapsto{}3, {y}^{old}*{y}^{old}\mapsto{}4, x\right)\wedge{}\left(y=x+1\right)\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\left\{x\mapsto{}3*x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\dispose\kern-2pt\left(x\right)\\\left\{x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\\end{array}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\\end{array}\\\left\{x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\\end{array}$}\end{prooftree}The forward assignment axiom gets us another triple, this time concerning the seventh instruction. \[\begin{array}{c}\left\{x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{x+1={y}^{newold}\wedge{}{y}^{newold}\mapsto{}{y}^{old}\wedge{}y={\left[{y}^{newold}\right]}_{h}\right\}\\\end{array}\]The postcondition in this triple can be rewritten by implication. \[\begin{array}{c}x+1={y}^{newold}\wedge{}{y}^{newold}\mapsto{}{y}^{old}\wedge{}y={\left[{y}^{newold}\right]}_{h}\\\Rightarrow{}\\x+1={y}^{newold}\wedge{}{y}^{newold}\mapsto{}{y}^{old}\wedge{}y={y}^{old}\\\Rightarrow{}\\x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\\\end{array}\]The rule of consequence then gets us the according triple. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{x+1={y}^{newold}\wedge{}{y}^{newold}\mapsto{}{y}^{old}\wedge{}y={\left[{y}^{newold}\right]}_{h}\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}x+1={y}^{newold}\wedge{}{y}^{newold}\mapsto{}{y}^{old}\wedge{}y={\left[{y}^{newold}\right]}_{h}\\\Rightarrow{}\\x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\\\end{array}$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\end{prooftree}The frame rule now lets us add a frame: ${y}^{old}\mapsto{}4, x$. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\LeftLabel{[frame]}\UnaryInfC{$\begin{array}{c}\left\{{y}^{old}\mapsto{}4, x*x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{{y}^{old}\mapsto{}4, x*x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\end{prooftree}Finally we can use the rule of \index{sequential composition}sequential composition to get a triple about concerning the entire program. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\\end{array}\\\left\{x+1\mapsto{}{y}^{old}*{y}^{old}\mapsto{}4, x\wedge{}y=x+1\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}\left\{{y}^{old}\mapsto{}4, x*x+1=y\wedge{}y\mapsto{}{y}^{old}\right\}\\y:={\left[y\right]}_{h}\\\left\{{y}^{old}\mapsto{}4, x*x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\LeftLabel{[seqcomp]}\BinaryInfC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\y:={\left[y\right]}_{h}\\\end{array}\\\left\{{y}^{old}\mapsto{}4, x*x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\end{prooftree}One last application of the rule of \index{consequence}consequence completes the proof. \begin{prooftree}\AxiomC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\y:={\left[y\right]}_{h}\\\end{array}\\\left\{{y}^{old}\mapsto{}4, x*x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\right\}\\\end{array}$}\AxiomC{$\begin{array}{c}{y}^{old}\mapsto{}4, x*x+1\mapsto{}{y}^{old}\wedge{}y={y}^{old}\\\Rightarrow{}\\y\mapsto{}4*true\\\end{array}$}\LeftLabel{[conseq]}\BinaryInfC{$\begin{array}{c}\left\{emp\right\}\\\begin{array}{l}x:=cons\kern-2pt\left(3, 3\right)\\y:=cons\kern-2pt\left(4, 4\right)\\{\left[x+1\right]}_{h}:=y\\{\left[y+1\right]}_{h}:=x\\y:=x+1\\dispose\kern-2pt\left(x\right)\\y:={\left[y\right]}_{h}\\\end{array}\\\left\{y\mapsto{}4*true\right\}\\\end{array}$}\end{prooftree}\end{ex}\section{Temporal Logic}\subsection{Linear temporal logic}\begin{de}\label{definition:temporal-logic}\index{linear temporal logic}\textbf{linear temporal logic} is an extension of \index{Propositional logic}Propositional logic\footnote{See definition \ref{definition:propositional-logic} on page \pageref{definition:propositional-logic}. } with temporal operators. Let $P$ be a fixed set of propositions. The \index{grammar}grammar of \index{linear temporal logic}linear temporal logic is defined as follow. \[p\mid{}\neg{}f\mid{}f\wedge{}g\mid{}f\vee{}g\mid{}\textbigcircle{}f\mid{}f\,\mathcal{U}\,g\]Here, $p$ is an atomic proposition from $P$. \begin{itemize}\item{}Next: $\textbigcircle{}f$. \item{}Until: $f\,\mathcal{U}\,g$. \end{itemize}There are also some shorthand notations excluding the propositional shorthands. \begin{itemize}\item{}Eventually: $\Diamond{}f\quad{}\equiv{}\quad{}true\,\mathcal{U}\,f$. \item{}Always: $\Box{}f\quad{}\equiv{}\quad{}\neg{}\Diamond{}\neg{}f$. \end{itemize}The semantics are defined as follows. In \index{linear temporal logic}linear temporal logic truth is evaluated with respect to a(n input) string. Let $w={w}_{1}{w}_{2}\dotsc{}{w}_{n}\in{}{P}^{*}$ be a string. $w$ satisfies a linear temporal logic \index{formula}formula $f$ at position $i$, de:: Noted as $w, i\vDash{}f$ under the following conditions. \begin{itemize}\item{}$w, i\vDash{}p\Leftrightarrow{}p={w}_{i}$\item{}$w, i\vDash{}\neg{}f\Leftrightarrow{}\neg{}w, i\vDash{}f$\item{}$w, i\vDash{}f\wedge{}g\Leftrightarrow{}\left(w, i\vDash{}f\wedge{}w, i\vDash{}g\right)$\item{}$w, i\vDash{}f\vee{}g\Leftrightarrow{}\left(w, i\vDash{}f\vee{}w, i\vDash{}g\right)$\item{}$w, i\vDash{}\textbigcircle{}f\Leftrightarrow{}\left(i<n\wedge{}w, i+1\vDash{}f\right)$\item{}$w, i\vDash{}f\,\mathcal{U}\,g\Leftrightarrow{}\left(\exists{}i, j:\ \left(i<j\leq{}n\right)\wedge{}w, j\vDash{}g\wedge{}\left(\forall{}k:\ \left(i\leq{}k<j\right)\Rightarrow{}w, k\vDash{}f\right)\right)$\end{itemize}\end{de}\begin{thm}Let $P$ be a fixed set of propositions. Let $w={w}_{1}{w}_{2}\dotsc{}{w}_{n}\in{}{P}^{*}$ be a string. The semantics of $w, i\vDash{}\Diamond{}f$ can be rewritten as follows. \[w, i\vDash{}\Diamond{}f=\exists{}i, j:\ \left(i\leq{}j\leq{}n\right)\wedge{}w, j\vDash{}f\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}Let $P$ be a fixed set of propositions. Let $w={w}_{1}{w}_{2}\dotsc{}{w}_{n}\in{}{P}^{*}$ be a string. The semantics of $w, i\vDash{}\Box{}f$ can be rewritten as follows. \[w, i\vDash{}\Box{}f=\forall{}i, j:\ \left(i\leq{}j\leq{}n\right)\Rightarrow{}w, j\vDash{}f\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}The \index{language}\textbf{language} of a \index{linear temporal logic}linear temporal logic \index{formula}formula is the sets of strings that satisfy the formula. \[{L}_{f}\quad{}\equiv{}\quad{}\left\{w\in{}{P}^{*}\mid{}w, 1\vDash{}f\right\}\]\end{de}\chapter{Sets}\section{Set Basics}\begin{de}\label{definition:set}\label{definition:element}A \index{set}\textbf{set} is a \index{collection}collection of distinct objects, considered as an object in its own right. These objects are called the \index{element}\textbf{element}s of the \index{set}set. \end{de}\begin{de}The fact that a \index{set}set $A$ contains a certain \index{element}element $a$ is denoted as $a\in{}A$. \end{de}\begin{de}A formal description of a \index{set}set using a \index{predicate}predicate $p$ is written as follows. \[\left\{x\mid{}p\kern-2pt\left(x\right)\right\}\]This is the \index{set}set of all objects that have the \index{property}property $P$. \end{de}\begin{de}\label{definition:sets-equality}Two sets $A$ and $B$ are \index{equal}\textbf{equal} if and only if they contain the same elements. \[A\underset{set}{=}B\quad{}\equiv{}\quad{}\forall{}x:\ x\in{}A\wedge{}x\in{}B\]\end{de}\begin{thm}The \index{transitivity}\textbf{transitivity} of `$\underset{set}{=}$'. \newline{}Let $A$, $B$ and $C$ be sets. \[\left(A\underset{set}{=}B\right)\wedge{}\left(B\underset{set}{=}C\right)\Rightarrow{}A\underset{set}{=}C\]\begin{proof}\begin{align*}\left(A\underset{set}{=}B\right)\wedge{}\left(B\underset{set}{=}C\right)&\Leftrightarrow{}\left(\forall{}x:\ x\in{}A\Leftrightarrow{}x\in{}B\right)\wedge{}\left(\forall{}x:\ x\in{}B\Leftrightarrow{}x\in{}C\right)\\&\Rightarrow{}\forall{}x:\ \left(x\in{}A\Leftrightarrow{}x\in{}B\right)\wedge{}\left(x\in{}B\Leftrightarrow{}x\in{}C\right)\\&\Leftrightarrow{}\forall{}x:\ x\in{}A\Leftrightarrow{}x\in{}C\end{align*}\end{proof}\end{thm}\begin{de}\label{definition:subset}A \index{set}set $A$ is a \index{subset}\textbf{subset} of a \index{set}set $B$ if and only if $B$ contains all elements of $A$. \[A\subseteq{}B\quad{}\equiv{}\quad{}\forall{}x:\ x\in{}A\Rightarrow{}x\in{}B\]\end{de}\begin{thm}The \index{anti-symmetry}\textbf{anti-symmetry} of `$\subseteq{}$': \newline{} Let $A$ and $B$ be sets. \[\left(A\subseteq{}B\wedge{}B\subseteq{}A\right)\Leftrightarrow{}A\underset{set}{=}B\]\begin{proof}\begin{align*}A\subseteq{}B\wedge{}B\subseteq{}A&\Leftrightarrow{}\left(\forall{}x:\ x\in{}A\Rightarrow{}x\in{}B\right)\wedge{}\left(\forall{}x:\ x\in{}B\Rightarrow{}x\in{}A\right)\\&\Leftrightarrow{}\forall{}x:\ \left(x\in{}A\Rightarrow{}x\in{}B\right)\wedge{}\left(x\in{}B\Rightarrow{}x\in{}A\right)\\&\Leftrightarrow{}\forall{}x:\ x\in{}A\Leftrightarrow{}x\in{}B&A\underset{set}{=}B\end{align*}\end{proof}\end{thm}\begin{thm}The \index{transitivity}\textbf{transitivity} of `$\subseteq{}$': Let $A$, $B$ and $C$ be sets. \[A\subseteq{}B\wedge{}B\subseteq{}C\Rightarrow{}A\subseteq{}C\]\begin{proof}\begin{align*}A\subseteq{}B\wedge{}B\subseteq{}C&\Leftrightarrow{}\left(\forall{}x:\ x\in{}A\Rightarrow{}x\in{}B\right)\wedge{}\left(\forall{}x:\ x\in{}A\Rightarrow{}x\in{}B\right)\\&\Rightarrow{}\forall{}x:\ \left(x\in{}A\Rightarrow{}x\in{}B\right)\wedge{}\left(x\in{}B\Rightarrow{}x\in{}C\right)\\&\Leftrightarrow{}\forall{}x:\ x\in{}A\Rightarrow{}x\in{}C&\Leftrightarrow{}A\subseteq{}C\end{align*}\end{proof}\end{thm}\begin{de}A \index{set}set is a \index{strict subset}\textbf{strict subset} of another \index{set}set if and only if $A$ is a \index{subset}subset of $B$ and not equal to $B$. \[A\subsetneq{}B\quad{}\equiv{}\quad{}A\subseteq{}B\wedge{}A\underset{set}{\neq{}}B\]\end{de}\begin{de}The \index{universal set}\textbf{universal set} is the \index{set}set of all objects. \[\Omega{}\quad{}\equiv{}\quad{}\left\{x\mid{}true\right\}\]\end{de}\begin{nte}Note that this is well defined as this \index{set}set would have to include itself. We will ignore this for now as the \index{universal set }universal set  is usually restricted to a domain that will be clear from the context. \end{nte}\begin{thm}\label{theorem:sets-every-set-is-a-subset-of-the-universe}Every set $A$ is a \index{subset}subset of the \index{universal set}universal set $setuniv$. \[\forall{}A:\ A\subseteq{}\Omega{}\]\begin{proof}Let $A$ be a set. Every element of $A$ is contained in $\Omega{}$. \[\forall{}x:\ \left(x\in{}A\right)\Rightarrow{}\left(x\in{}\Omega{}\right)\]\end{proof}\end{thm}\begin{de}The \index{empty set}\textbf{empty set} $\emptyset{}$ is the \index{set}set that contains no elements. \[\emptyset{}\quad{}\equiv{}\quad{}\left\{x\mid{}false\right\}\]\end{de}\begin{thm}The \index{empty set}empty set $\emptyset{}$ is a \index{subset}subset of all sets. \begin{proof}Let $A$ be a set.. Every element of $\emptyset{}$ is also an element of $A$. \[\forall{}x:\ x\in{}\emptyset{}\Rightarrow{}x\in{}A\]This is vacuously true.\end{proof}\end{thm}\begin{de}A \index{set}set with exactly one element is called a \index{singleton}\textbf{singleton}. \end{de}\begin{de}\label{definition:predicate}A \index{predicate}\textbf{predicate} $P$ over a \index{set}set $A$ is a \index{subset}subset of $A$. Using a little notational overloading, $P\kern-2pt\left(a\right)$ is said to hold if $a$ is an element of $A$. \[P\kern-2pt\left(a\right)\quad{}\equiv{}\quad{}a\in{}P\]\end{de}\begin{de}A \index{partition}\textbf{partition} is a \index{set}set $B$ of subsets of a set $A$ with the following properties. \begin{enumerate}\item{}$\emptyset{}\not{\in{}}B$\item{}${\bigcup_{{b\in{}B}}}{b}=A$\item{}$\forall{}{b}_{1}, {b}_{2}\in{}B:\ {b}_{1}\underset{set}{\neq{}}{b}_{2}\Rightarrow{}{b}_{1}\cap{}{b}_{2}\underset{set}{=}\emptyset{}$\end{enumerate}\end{de}\section{The algebra of sets}\subsection{Set union}\begin{de}\label{definition:union}The \index{union}\textbf{union} $A\cup{}B$ of two sets $A$ and $B$ is the set of all elements of both $A$ and $B$. \[A\cup{}B\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\]\end{de}\begin{prop}The \index{set}set \index{union}union is \index{associative}associative\footnote{See definition \ref{definition:associative} on page \pageref{definition:associative}. }. \[A\cup{}\left(B\cup{}C\right)\underset{set}{=}\left(A\cup{}B\right)\cup{}C\]\begin{proof}\begin{align*}A\cup{}\left(B\cup{}C\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}\left(B\cup{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\vee{}x\in{}\left\{y\mid{}y\in{}B\vee{}y\in{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\vee{}y\in{}C\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\vee{}y\in{}B\right\}\right\}\vee{}x\in{}C\\&\underset{set}{=}\left\{x\mid{}x\in{}\left(A\cup{}B\right)\right\}\vee{}x\in{}C\\&\underset{set}{=}\left(A\cup{}B\right)\cup{}C\end{align*}\end{proof}\end{prop}\begin{prop}The \index{set}set \index{union}union is \index{commutative}commutative. \[A\cup{}B\underset{set}{=}B\cup{}A\]\begin{proof}$A\cup{}B\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\underset{set}{=}\left\{x\mid{}x\in{}B\vee{}x\in{}A\right\}\underset{set}{=}B\cup{}A$\end{proof}\end{prop}\begin{prop}The \index{set}set \index{union}union is \index{idempotent}idempotent. \[A\cup{}A\underset{set}{=}A\]\begin{proof}$A\cup{}A\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}A\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{prop}\begin{thm}The \index{set}set \index{union}union of two sets $A$ and $B$ is a superset of $A$. \[A\subseteq{}A\cup{}B\]\begin{proof}$A\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\subseteq{}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\underset{set}{=}A\cup{}B$\end{proof}\end{thm}\begin{thm}\[A\subseteq{}B\Leftrightarrow{}A\cup{}B\underset{set}{=}A\]\begin{proof}Let $B$ be a set and $A$ a subset of $B$. \[A\cup{}B\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\]\end{proof}\end{thm}\begin{thm}The \index{identity law}\textbf{identity law} for the \index{set}set \index{union}union. \[A\cup{}\emptyset{}\underset{set}{=}A\]\begin{proof}$A\cup{}\emptyset{}\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}\emptyset{}\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}false\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{thm}\begin{thm}The \index{domination law}\textbf{domination law} for the \index{set}set \index{union}union. \[A\cup{}\Omega{}\underset{set}{=}\Omega{}\]\begin{proof}$A\cup{}\Omega{}\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}\Omega{}\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}true\right\}\underset{set}{=}\left\{x\mid{}true\right\}\underset{set}{=}\Omega{}$\end{proof}\end{thm}\subsection{Set intersection}\begin{de}The \index{intersection}\textbf{intersection} $A\cup{}B$ of two sets $A$ and $B$ is the set of all elements of both $A$ and $B$. \[A\cup{}B\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\]\end{de}\begin{prop}\label{property:intersection-associative}The set \index{intersection}intersection is \index{associative}associative\footnote{See definition \ref{definition:associative} on page \pageref{definition:associative}. }. \[A\cap{}\left(B\cap{}C\right)\underset{set}{=}\left(A\cap{}B\right)\cap{}C\]\begin{proof}\begin{align*}A\cap{}\left(B\cap{}C\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\left(B\cap{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\left\{y\mid{}y\in{}B\wedge{}y\in{}C\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\wedge{}y\in{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\wedge{}x\in{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left(A\cap{}B\right)\wedge{}x\in{}C\right\}\\&\underset{set}{=}\left(A\cap{}B\right)\cap{}C\end{align*}\end{proof}\end{prop}\begin{prop}The set \index{intersection}intersection is \index{commutative}commutative. \[A\cap{}B\underset{set}{=}B\cap{}A\]\begin{proof}$A\cap{}B\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\underset{set}{=}\left\{x\mid{}x\in{}B\wedge{}x\in{}A\right\}\underset{set}{=}B\cap{}A$\end{proof}\end{prop}\begin{prop}The set \index{intersection}intersection is \index{idempotent}idempotent. \[A\cap{}A\underset{set}{=}A\]\begin{proof}$A\cap{}A\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}A\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{prop}\begin{thm}The set \index{intersection}intersection of two sets $A$ and $B$ is a subset of $A$. \[A\cap{}B\subseteq{}A\]\begin{proof}$A\cap{}B\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\subseteq{}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{thm}\begin{thm}\[A\subseteq{}B\Leftrightarrow{}A\cap{}B\underset{set}{=}B\]\begin{proof}Let $B$ be a set and $A$ a subset of $B$. \[A\cap{}B\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\underset{set}{=}\left\{x\mid{}x\in{}B\right\}\underset{set}{=}B\]\end{proof}\end{thm}\begin{thm}The \index{identity law}\textbf{identity law} for the set \index{intersection}intersection. \[A\cap{}\Omega{}\underset{set}{=}A\]\begin{proof}$A\cap{}\emptyset{}\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\Omega{}\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}true\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{thm}\begin{thm}The \index{domination law}\textbf{domination law} for the set \index{intersection}intersection. \[A\cap{}\Omega{}\underset{set}{=}A\]\begin{proof}$A\cap{}\Omega{}\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\Omega{}\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}true\right\}\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\underset{set}{=}A$\end{proof}\end{thm}\begin{de}Two sets $A$ and $B$ are \index{disjunct}\textbf{disjunct} if they have no elements in common. \[A\cap{}B\underset{set}{=}\emptyset{}\]\end{de}\begin{thm}The first \index{absorption law}\textbf{absorption law}. \[A\cup{}\left(A\cap{}B\right)\underset{set}{=}A\]\begin{proof}\begin{align*}A\cup{}\left(A\cap{}B\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}A\cap{}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}A\right)\vee{}\left(x\in{}A\wedge{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}\left(x\in{}A\wedge{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\\&\underset{set}{=}A\end{align*}\end{proof}\end{thm}\begin{thm}The second \index{absorption law}\textbf{absorption law}. \[A\cap{}\left(A\cup{}B\right)\underset{set}{=}A\]\begin{proof}\begin{align*}A\cap{}\left(A\cup{}B\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}A\cup{}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\left\{y\mid{}y\in{}A\vee{}y\in{}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}A\right)\wedge{}\left(x\in{}A\vee{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}\left(x\in{}A\vee{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\\&\underset{set}{=}A\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:dristribution-law-1}The set \index{intersection}intersection is \index{distributive}distributive with respect to the set \index{union}union. \[A\cap{}\left(B\cup{}C\right)\underset{set}{=}\left(A\cup{}B\right)\cap{}\left(A\cup{}C\right)\]\begin{proof}\begin{align*}A\cap{}\left(B\cup{}C\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\cup{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}\left\{y\mid{}y\in{}B\vee{}y\in{}C\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}B\right)\wedge{}\left(x\in{}A\vee{}x\in{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}B\right)\wedge{}\left(x\in{}A\vee{}x\in{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}\left(y\in{}A\vee{}y\in{}B\right)\right\}\wedge{}x\in{}\left\{y\mid{}\left(y\in{}A\vee{}y\in{}C\right)\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}B\right)\right\}\cap{}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}C\right)\right\}\\&\underset{set}{=}\left(A\cup{}B\right)\cap{}\left(A\cup{}C\right)\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:dristribution-law-2}The set \index{union}union is \index{distributive}distributive with respect to the set \index{intersection}intersection. \[A\cup{}\left(B\cap{}C\right)\underset{set}{=}\left(A\cap{}B\right)\cup{}\left(A\cap{}C\right)\]\begin{proof}\begin{align*}A\cup{}\left(B\cap{}C\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\cap{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}\left\{y\mid{}y\in{}B\wedge{}y\in{}C\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}B\right)\vee{}\left(x\in{}A\wedge{}x\in{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}B\right)\vee{}\left(x\in{}A\wedge{}x\in{}C\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}\left(y\in{}A\wedge{}y\in{}B\right)\right\}\vee{}x\in{}\left\{y\mid{}\left(y\in{}A\wedge{}y\in{}C\right)\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}B\right)\right\}\cup{}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}C\right)\right\}\\&\underset{set}{=}\left(A\cap{}B\right)\cup{}\left(A\cap{}C\right)\end{align*}\end{proof}\end{thm}\subsection{Set difference}\begin{de}The set \index{difference}\textbf{difference} between sets $A$ and $B$ is the set of all elements of $A$ that are not in $B$. \[A\setminus{}B\quad{}\equiv{}\quad{}\left\{x\mid{}x\in{}A\wedge{}x\not{\in{}}B\right\}\]\end{de}\begin{thm}Let $A$ and $B$ be sets. \[\left(A\cap{}B\right)\cap{}\left(A\setminus{}B\right)\underset{set}{=}\emptyset{}\]\begin{proof}\begin{align*}\left(A\cap{}B\right)\cap{}\left(A\setminus{}B\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\cap{}\left\{x\mid{}x\in{}A\wedge{}x\not{\in{}}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\wedge{}x\in{}\left\{y\mid{}y\in{}A\right\}\wedge{}y\not{\in{}}B\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}B\right)\wedge{}\left(x\in{}A\wedge{}x\not{\in{}}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\wedge{}x\not{\in{}}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}false\right\}\\&\underset{set}{=}\left\{x\mid{}false\right\}\\&\underset{set}{=}\emptyset{}\end{align*}\end{proof}\end{thm}\begin{thm}Let $A$ and $B$ be sets. \[\left(A\setminus{}B\right)\cap{}\left(B\setminus{}A\right)\underset{set}{=}\emptyset{}\]\begin{proof}\begin{align*}\left(A\setminus{}B\right)\cap{}\left(B\setminus{}A\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\not{\in{}}B\right\}\cap{}\left\{x\mid{}x\in{}B\wedge{}x\not{\in{}}A\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\not{\in{}}B\right\}\wedge{}x\in{}\left\{y\mid{}y\in{}B\wedge{}y\not{\in{}}A\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\not{\in{}}B\right)\wedge{}\left(x\in{}B\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}A\not{\in{}}A\right)\wedge{}\left(x\in{}B\wedge{}x\not{\in{}}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}false\wedge{}false\right\}\\&\underset{set}{=}\left\{x\mid{}false\right\}\\&\underset{set}{=}\emptyset{}\end{align*}\end{proof}\end{thm}\begin{de}The \index{symmetric difference}\textbf{symmetric difference} of two sets $A$ and $B$ is the set of all element that are in either $A$ or $B$ but not both. \[A\ \Delta{}\ B\quad{}\equiv{}\quad{}\left\{x\mid{}\left(x\in{}A\wedge{}x\not{\in{}}B\right)\vee{}\left(x\not{\in{}}A\wedge{}x\in{}B\right)\right\}\]\end{de}\begin{de}Let $A$ and $B$ be sets. \[A\ \Delta{}\ B\underset{set}{=}\left(A\setminus{}B\right)\cup{}\left(B\setminus{}A\right)\]\begin{proof}\begin{align*}\left(A\setminus{}B\right)\cup{}\left(B\setminus{}A\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\not{\in{}}B\right\}\cup{}\left\{x\mid{}x\in{}B\wedge{}x\not{\in{}}A\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\not{\in{}}B\right\}\vee{}x\in{}\left\{y\mid{}y\in{}B\wedge{}y\not{\in{}}A\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\not{\in{}}B\right)\vee{}\left(x\in{}B\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}A\ \Delta{}\ B\end{align*}\end{proof}\end{de}\begin{thm}\label{theorem:sets-symmetric-difference-in-terms-of-union-and-intersection}Let $A$ and $B$ be sets. \[A\ \Delta{}\ B\underset{set}{=}\left(A\cup{}B\right)\setminus{}\left(A\cap{}B\right)\]\begin{proof}\begin{align*}\left(A\cup{}B\right)\setminus{}\left(A\cap{}B\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}B\right\}\setminus{}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\vee{}y\in{}B\right\}\wedge{}x\not{\in{}}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}B\right)\wedge{}\neg{}\left(x\in{}A\wedge{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}B\right)\wedge{}\left(x\not{\in{}}A\vee{}x\not{\in{}}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\not{\in{}}B\right)\vee{}\left(x\in{}B\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}A\ \Delta{}\ B\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:intersection-over-difference}Let $A$, $B$ and $C$ be sets. \[A\cap{}\left(B\setminus{}C\right)\underset{set}{=}\left(A\cap{}B\right)\setminus{}C\]\begin{proof}\begin{align*}A\cap{}\left(B\setminus{}C\right)&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\setminus{}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\left\{y\mid{}y\in{}B\wedge{}y\not{\in{}}C\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}B\wedge{}x\not{\in{}}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\wedge{}x\not{\in{}}C\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left(A\cap{}B\right)\wedge{}x\not{\in{}}C\right\}\\&\underset{set}{=}\left(A\cap{}B\right)\setminus{}C\end{align*}\end{proof}\end{thm}\subsection{Set complement}\begin{de}The \index{complement}\textbf{complement} of a set $A$ relative to a set $B$ is the set of all elements of $B$ that are not in $A$. \[{{{A}}^{C}}_{B}\quad{}\equiv{}\quad{}B\setminus{}A\]When $B$ is clear from the context (i.e. there is a universe in play), we just speak about the \index{complement}\textbf{complement}. \[{{A}}^{C}\quad{}\equiv{}\quad{}{{{A}}^{C}}_{\Omega{}}\]\end{de}\begin{thm}Let $A$ be a set. \[{{{{A}}^{C}}}^{C}\underset{set}{=}A\]\begin{proof}\begin{align*}{{{{A}}^{C}}}^{C}&\underset{set}{=}{{{{{{A}}^{C}}_{\Omega{}}}}^{C}}_{\Omega{}}\\&\underset{set}{=}\Omega{}\setminus{}\left(\Omega{}\setminus{}A\right)\\&\underset{set}{=}\left\{x\mid{}x\in{}\Omega{}\wedge{}x\not{\in{}}\left\{y\mid{}y\in{}\Omega{}\wedge{}y\not{\in{}}A\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\Omega{}\wedge{}\neg{}\left(x\in{}\Omega{}\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\Omega{}\wedge{}\left(x\not{\in{}}\Omega{}\vee{}x\in{}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}\Omega{}\wedge{}x\not{\in{}}\Omega{}\right)\vee{}\left(x\in{}\Omega{}\wedge{}x\in{}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}false\vee{}\left(true\wedge{}x\in{}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\right\}\\&\underset{set}{=}A\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:complementary-law-union}The \index{complementary law}\textbf{complementary law} for the set \index{union}union. Let $A$ and $B$ be sets. \[A\cup{}{{A}}^{C}\underset{set}{=}\Omega{}\]\begin{proof}\begin{align*}A\cup{}{{A}}^{C}&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}{{A}}^{C}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}x\in{}\left\{y\mid{}y\in{}\Omega{}\wedge{}y\not{\in{}}A\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\vee{}\left(x\in{}\Omega{}\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}x\in{}\Omega{}\right)\wedge{}\left(x\in{}A\vee{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\vee{}true\right)\wedge{}true\right\}\\&\underset{set}{=}\left\{x\mid{}true\right\}\\&\underset{set}{=}\Omega{}\end{align*}\end{proof}\end{thm}\begin{thm}The \index{complementary law}\textbf{complementary law} for the set \index{intersection}intersection. Let $A$ and $B$ be sets. \[A\cap{}{{A}}^{C}\underset{set}{=}\emptyset{}\]\begin{proof}\begin{align*}A\cap{}{{A}}^{C}&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}{{A}}^{C}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}x\in{}\left\{y\mid{}y\in{}\Omega{}\wedge{}y\not{\in{}}A\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}A\wedge{}\left(x\in{}\Omega{}\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}x\in{}\Omega{}\right)\wedge{}\left(x\in{}A\wedge{}x\not{\in{}}A\right)\right\}\\&\underset{set}{=}\left\{x\mid{}\left(x\in{}A\wedge{}true\right)\wedge{}false\right\}\\&\underset{set}{=}\left\{x\mid{}false\right\}\\&\underset{set}{=}\emptyset{}\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:first-law-of-de-morgan}The \index{first law of De Morgan}\textbf{first law of De Morgan}. \[{{\left(A\cup{}B\right)}}^{C}\underset{set}{=}{{A}}^{C}\cap{}{{B}}^{C}\]\begin{proof}\begin{align*}{{\left(A\cup{}B\right)}}^{C}&\underset{set}{=}\left\{x\mid{}x\not{\in{}}\left(A\cup{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\not{\in{}}\left\{y\mid{}y\in{}A\vee{}y\in{}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\neg{}\left(x\in{}A\vee{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\not{\in{}}A\wedge{}x\not{\in{}}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\not{\in{}}A\right\}\wedge{}x\in{}\left\{y\mid{}y\not{\in{}}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}{{A}}^{C}\wedge{}x\in{}{{B}}^{C}\right\}\\&\underset{set}{=}{{A}}^{C}\cap{}{{B}}^{C}\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:second-law-of-de-morgan}The \index{second law of De Morgan}\textbf{second law of De Morgan}. \[{{\left(A\cap{}B\right)}}^{C}\underset{set}{=}{{A}}^{C}\cup{}{{B}}^{C}\]\begin{proof}\begin{align*}{{\left(A\cap{}B\right)}}^{C}&\underset{set}{=}\left\{x\mid{}x\not{\in{}}\left(A\cap{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\not{\in{}}\left\{y\mid{}y\in{}A\wedge{}y\in{}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}\neg{}\left(x\in{}A\wedge{}x\in{}B\right)\right\}\\&\underset{set}{=}\left\{x\mid{}x\not{\in{}}A\vee{}x\not{\in{}}B\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}\left\{y\mid{}y\not{\in{}}A\right\}\vee{}x\in{}\left\{y\mid{}y\not{\in{}}B\right\}\right\}\\&\underset{set}{=}\left\{x\mid{}x\in{}{{A}}^{C}\vee{}x\in{}{{B}}^{C}\right\}\\&\underset{set}{=}{{A}}^{C}\cup{}{{B}}^{C}\end{align*}\end{proof}\end{thm}\begin{thm}\label{theorem:set-difference-equivalent-definition}Let $A$ and $B$ be sets. \[A\setminus{}B\underset{set}{=}A\cap{}{{B}}^{C}\]\begin{proof}$A\setminus{}B\underset{set}{=}A\cap{}\Omega{}\setminus{}B\underset{set}{=}A\setminus{}B$\footnote{See theorem \ref{theorem:intersection-over-difference} on page \pageref{theorem:intersection-over-difference}. }\end{proof}\end{thm}\begin{de}\label{definition:carthesian-product}The \index{Carthesian product}\textbf{Carthesian product} of two sets $A$ and $B$ is the set of all tuples of elements in $A$ and $B$ respectively . \[\left\{\left(x, y\right)\mid{}x\in{}A\wedge{}y\in{}B\right\}\]\end{de}\section{Pointed Sets}\begin{de}\label{definition:pointed-set}Let $X$ be a \index{set}set and let ${x}_{0}$ be an \index{element}element of $X$. The tuple $\left(X, {x}_{0}\right)$ is called a \index{pointed set}\textbf{pointed set}. \end{de}\chapter{Relations}\section{Basics}\begin{de}\label{definition:relation}A \index{relation}\textbf{relation} between $n$ sets ${X}_{1}, {X}_{2}, \dotsc{}, {X}_{n}$ is a subset of their \index{Carthesian product}Carthesian product\footnote{See definition \ref{definition:carthesian-product} on page \pageref{definition:carthesian-product}. }. \end{de}\begin{de}A binary \index{relation}relation $R$ is a relation between two sets. If a binary relation $R$ between sets $A$ and $B$ contains a tuple $\left(v, w\right)$ then that is often denoted as $v\,R\,w$. \end{de}\begin{de}A ternary \index{relation}relation is a relation between three sets. \end{de}\begin{de}Let $X$ be a set. The $n$-ary \index{unit relation}\textbf{unit relation} ${I}_{n}$ is the set of all tuples of the same element. \[{I}_{2}\quad{}\equiv{}\quad{}\left\{\left(v, v\right)\mid{}v\in{}X\right\}\]\[{I}_{n}\quad{}\equiv{}\quad{}\left\{\left(v, v, \dotsc{}, v\right)\mid{}v\in{}X\right\}\]\end{de}\begin{de}Let $R$ be a binary relation on the sets $A$ and $B$. The \index{inverse relation}\textbf{inverse relation} ${R}^{-1}$ of $R$ is the following relation. \[{R}^{-1}\quad{}\equiv{}\quad{}\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}R\right\}\]\end{de}\begin{thm}\label{theorem:inverse-of-inverse-relation-is-normal}Let $R$ be a binary relation on the sets $A$ and $B$. \[{{R}^{-1}}^{-1}=R\]\begin{proof}\begin{align*}{{R}^{-1}}^{-1}&=\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}{R}^{-1}\right\}\\&=\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}R\right\}\right\}\\&=\left\{\left(y, x\right)\mid{}\left(y, x\right)\in{}R\right\}&=R\end{align*}\end{proof}\end{thm}\subsection{Properties of relations}\begin{de}\label{definition:reflexive}A \index{relation}relation $R$ between a set $X$ and itself is called \index{reflexive}\textbf{reflexive} if it has the following property. \[\forall{}x\in{}X:\ \left(x, x\right)\in{}R\]\end{de}\begin{de}\label{definition:transitive}A \index{relation}relation $R$ between a set $X$ and itself is called \index{transitive}\textbf{transitive} if it has the following property. \[\forall{}x, y, z\in{}X:\ \left(\left(x, y\right)\in{}R\wedge{}\left(y, z\right)\in{}R\right)\Rightarrow{}\left(x, z\right)\in{}R\]\end{de}\begin{de}\label{definition:symmetric}A \index{relation}relation $R$ between a set $X$ and itself is called \index{symmetric}\textbf{symmetric} if it has the following property. \[\forall{}x, y\in{}X:\ \left(x, y\right)\in{}R\Leftrightarrow{}\left(y, x\right)\in{}R\]\end{de}\begin{de}\label{definition:total}A binary \index{relation}relation $R$ is called \index{total}\textbf{total} if it has the following property. \[\forall{}x, y:\ x\,R\,y\vee{}y\,R\,x\]\end{de}\begin{thm}Every total relation is reflexive. \begin{proof}Let $R$ be a total relation on a set $X$ and $x$ an element of $X$. Because $R$ is total, either $x\,R\,x$ or $x\,R\,x$ must be true. This means that $x\,R\,x$ must hold and $R$ must therefore be reflexive. \end{proof}\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsection{Domain and Image}\begin{de}\label{definition:domain}The \index{domain}\textbf{domain} of a binary relation $R$ between sets $A$ and $B$ is the following subset of $A$. \[\left\{x\mid{}\exists{}y:\ \left(x, y\right)\in{}R\right\}\]\end{de}\begin{de}\label{definition:image}The \index{image}\textbf{image} or \index{range}\textbf{range} of a binary relation $R$ between sets $A$ and $B$ is the following subset of $B$. \[\left\{y\mid{}\exists{}x:\ \left(x, y\right)\in{}R\right\}\]\end{de}\begin{thm}The \index{domain}domain of a \index{relation}relation is the image of its inverse. \[Dom\kern-2pt\left(R\right)=Img\kern-2pt\left({R}^{-1}\right)\]\begin{proof}\begin{align*}Img\kern-2pt\left({R}^{-1}\right)&=\left\{y\mid{}\exists{}x:\ \left(x, y\right)\in{}{R}^{-1}\right\}\\&=\left\{y\mid{}\exists{}x:\ \left(x, y\right)\in{}\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}R\right\}\right\}\\&=\left\{x\mid{}\exists{}y:\ \left(x, y\right)\in{}R\right\}\\&=Dom\kern-2pt\left(R\right)\end{align*}\end{proof}\end{thm}\begin{thm}The \index{image}image of a \index{relation}relation is the \index{domain}domain of its inverse. \[Img\kern-2pt\left(R\right)=Dom\kern-2pt\left({R}^{-1}\right)\]\begin{proof}\begin{align*}Dom\kern-2pt\left({R}^{-1}\right)&=\left\{x\mid{}\exists{}y:\ \left(x, y\right)\in{}{R}^{-1}\right\}\\&=\left\{x\mid{}\exists{}y:\ \left(x, y\right)\in{}\left\{\left(y, x\right)\mid{}\left(x, y\right)\in{}R\right\}\right\}\\&=\left\{y\mid{}\exists{}x:\ \left(x, y\right)\in{}R\right\}\\&=Img\kern-2pt\left(R\right)\end{align*}\end{proof}\end{thm}\section{Preorders}\begin{de}\label{definition:preorder}A \index{relation}relation $\sqsubseteq $ between a set $X$ and itself is called an \index{preorder}\textbf{preorder} if it is \index{reflexive}reflexive\footnote{See definition \ref{definition:reflexive} on page \pageref{definition:reflexive}. } and \index{transitive}transitive\footnote{See definition \ref{definition:transitive} on page \pageref{definition:transitive}. }. \end{de}\section{Equivalence Relations}\begin{de}\label{definition:equivalence-relation}A \index{symmetric}symmetric\footnote{See definition \ref{definition:symmetric} on page \pageref{definition:symmetric}. } \index{preorder}preorder is called an \index{equivalence relation}\textbf{equivalence relation}. \end{de}\subsection{Equivalence Classes}\begin{de}Let $\sim{}\mkern-3mu$ be an \index{equivalence relation}equivalence relation on a set $X$ and let $x$ be an element of $X$. The \index{equivalence class}\textbf{equivalence class} ${\left[x\right]}_{\sim{}\mkern-3mu}$ of $x$ in $\sim{}\mkern-3mu$ is the set of all elements that are equivalent to $x$. \[{\left[x\right]}_{\sim{}\mkern-3mu}\quad{}\equiv{}\quad{}\left\{y\in{}X\mid{}x\,\sim{}\mkern-3mu\,y\right\}\]\end{de}\todo[color=red,inline,size=\small]{TODO, theorem: The equivalence class of an element contains that element}\todo[color=red,inline,size=\small]{TODO, theorem: If two elements are equivalent, then their equivalence classes are equal}\begin{thm}Let $\sqsubseteq $ be a preord\_{}er on a set $X$. The relation ${\sim{}\mkern-3mu}_{\sqsubseteq }$ is an equivalence relation. \[{\sim{}\mkern-3mu}_{\sqsubseteq }\quad{}\equiv{}\quad{}\left\{\left(a, b\right)\mid{}a\,\sqsubseteq \,b\wedge{}b\,\sqsubseteq \,a\right\}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}Let $\sim{}\mkern-3mu$ be an \index{equivalence relation}equivalence relation on a set $X$. The \index{quotient set}\textbf{quotient set} $X/\sim{}\mkern-3mu$ of $X$ with respect to $\sim{}\mkern-3mu$ is the set of all equivalennce classes of $X$ in $\sim{}\mkern-3mu$. \[X/\sim{}\mkern-3mu\quad{}\equiv{}\quad{}\left\{{\left[x\right]}_{\sim{}\mkern-3mu}\mid{}x\in{}X\right\}\]\end{de}\todo[color=red,inline,size=\small]{TODO, theorem: The quotient set is a partition}\todo[color=red,inline,size=\small]{TODO, theorem: A partition induces an equivalence relation}\nocite{order-theory-for-computer-scientists}\section{Orders}\begin{de}\label{definition:antisymmetric}Let $X$ be a set and $\sim{}\mkern-3mu$ an \index{equivalence relation}equivalence relation\footnote{See definition \ref{definition:equivalence-relation} on page \pageref{definition:equivalence-relation}. } on $X$. Let $R$ be a binary \index{relation}relation on $X$. $R$ is called \index{antisymmetric}\textbf{antisymmetric} if it has the following property. \[\forall{}a, b\in{}X:\ \left(a\,R\,b\wedge{}b\,R\,a\right)\Rightarrow{}a\,\sim{}\mkern-3mu\,b\]\end{de}\subsection{Partial orders}\begin{de}\label{definition:partial-order}A \index{partial order}\textbf{partial order} is an \index{antisymmetric}antisymmetric \index{preorder}preorder\footnote{See definition \ref{definition:preorder} on page \pageref{definition:preorder}. }. \end{de}\begin{de}\label{definition:poset}A \index{partially ordered set}\textbf{partially ordered set} or \index{poset}\textbf{poset} is a tuple $\left(X, \sqsubseteq \right)$ of a set and a partial order on that set. \end{de}\begin{thm}\label{theorem:cross-poset-lift}Let $\left({X}_{1}, {\sqsubseteq }_{1}\right), \left({X}_{2}, {\sqsubseteq }_{2}\right), \dotsc{}, \left({X}_{n}, {\sqsubseteq }_{n}\right)$ be \index{poset}posets. $\left({X}_{1}\times{}{X}_{2}\times{}\dotsb{}\times{}{X}_{n}, \sqsubseteq \right)$ is a \index{poset}poset where $\sqsubseteq $ is defined as follows. \[a\sqsubseteq b\Leftrightarrow{}\forall{}i:\ {{a}_{i}\ {\sqsubseteq }_{i}\ b}_{i}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}\label{theorem:powerset-poset-induces-preorder}Let $\left(X, \sqsubseteq \right)$ be a poset. $\left(\mathcal{P}\kern-2pt\left(X\right), \sqsubseteq \right)$, where $\sqsubseteq $ is defined as follows, is a \index{preorder}preorder\footnote{See definition \ref{definition:preorder} on page \pageref{definition:preorder}. }. \[P\sqsubseteq Q\Leftrightarrow{}\forall{}x\in{}P:\ \exists{}y\in{}Q:\ x\sqsubseteq y\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{cex}In general, this preorder is not a partial order. \begin{proof}Consider sets of natural numbers with the natural total order.. \[\left\{1, 2, 3\right\}\sqsubseteq \left\{3\right\}\quad{}\text{and}\quad{}\left\{3\right\}\sqsubseteq \left\{1, 2, 3\right\}\quad{}\text{but}\quad{}\left\{3\right\}\underset{set}{\neq{}}\left\{1, 2, 3\right\}\]This violates antisymmetry. \end{proof}\end{cex}\begin{thm}\label{theorem:partial-orders-from-preorders}Given a preordered set $\left(X, \sqsubseteq \right)$, it is possible to lift the \index{preorder}preorder $\sqsubseteq $ to a \index{partial order}partial order $\left(X/\sim{}\mkern-3mu, \sqsubseteq \right)$. Here, $\sim{}\mkern-3mu$ is defined naturally. \[x\,\sim{}\mkern-3mu\,y\Leftrightarrow{}x\sqsubseteq y\wedge{}y\sqsubseteq x\]$\sqsubseteq $ is then defined accross equivalence classes. \[{\left[x\right]}_{\sim{}\mkern-3mu}\sqsubseteq {\left[y\right]}_{\sim{}\mkern-3mu}\Leftrightarrow{}x\sqsubseteq y\]\todo[color=red,inline,size=\small]{This is in fact a partial order\newline{}There is a proof missing here.}\end{thm}\subsection{Total orders}\begin{de}A \index{total order}\textbf{total order} is a binary relation that is \index{total}total\footnote{See definition \ref{definition:total} on page \pageref{definition:total}. }, \index{transitive}transitive\footnote{See definition \ref{definition:transitive} on page \pageref{definition:transitive}. } and \index{antisymmetric}antisymmetric. \end{de}\subsection{Extremes}\begin{de}\label{definition:greatest-element}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{greatest element}\textbf{greatest element} $a\in{}X$ is an element such that all other elements are smaller. \[\forall{}x\in{}X:\ x\sqsubseteq a\]\end{de}\begin{de}\label{definition:smallest-element}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{smallest element}\textbf{smallest element} $a\in{}X$ is an element such that all other elements are greater. \[\forall{}x\in{}X:\ a\sqsubseteq x\]\end{de}\begin{de}\label{definition:maximal-element}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{maximal element}\textbf{maximal element} $a\in{}X$ is an element such that there exists no greater element. \[\neg{}\exists{}x\in{}X:\ a\sqsubseteq x\]\end{de}\begin{de}\label{definition:minimal-element}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{minimal element}\textbf{minimal element} $a\in{}X$ is an element such that there exists no greater element. \[\neg{}\exists{}x\in{}X:\ a\sqsubseteq x\]\end{de}\begin{de}\label{definition:upper-bound}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. An \index{upper bound}\textbf{upper bound} $a$ is an element (not necessarily in $X$) that is greater than every element in $X$. \[\forall{}x\in{}X:\ x\sqsubseteq a\]\end{de}\begin{de}\label{definition:lower-bound}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. An \index{lower bound}\textbf{lower bound} $a$ is an element (not necessarily in $X$) that is smaller than every element in $X$. \[\forall{}x\in{}X:\ a\sqsubseteq x\]\end{de}\begin{de}\label{definition:supremum}\label{definition:join}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{supremum}\textbf{supremum} or \index{join}\textbf{join} of $X$ is a smallest \index{upper bound}upper bound of $X$. That is to say, all other upper bounds of $X$ are larger. \end{de}\begin{de}\label{definition:infimum}\label{definition:meet}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset. A \index{infimum}\textbf{infimum} or \index{meet}\textbf{meet} of $X$ is a greatest \index{lower bound}lower bound of $X$. That is to say, all other lower bounds of $X$ are smaller. \end{de}\begin{thm}If an supremum/infimum exists for a poset $\left(X, \sqsubseteq \right)$, then it is unique. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsection{Lattices}\begin{de}\label{definition:meet-semilattice}A \index{meet semilattice}\textbf{meet semilattice} is a \index{poset}poset $\left(X, \sqsubseteq \right)$ for which any two elements $a$ and $b$ have an \index{infimum}infimum $a\sqcup{}b$ as follows. \begin{itemize}\item{}$a\sqcup{}b\sqsubseteq a\wedge{}a\sqcup{}b\sqsubseteq b$\item{}$\forall{}c\in{}X:\ c\sqsubseteq a\wedge{}c\sqsubseteq b\Rightarrow{}c\sqsubseteq a\sqcup{}b$\end{itemize}\end{de}\begin{de}\label{definition:join-semilattice}A \index{join semilattice}\textbf{join semilattice} is a \index{poset}poset $\left(X, \sqsubseteq \right)$ for which any two elements $a$ and $b$ have a \index{supremum}supremum $a\sqcap{}b$ as follows. \begin{itemize}\item{}$a\sqsubseteq a\sqcap{}b\wedge{}b\sqsubseteq a\sqcap{}b$\item{}$\forall{}c\in{}X:\ a\sqsubseteq c\wedge{}b\sqsubseteq c\Rightarrow{}a\sqcap{}b\sqsubseteq c$\end{itemize}\end{de}\begin{de}\label{definition:lattice}If a \index{poset}poset is both a \index{meet semilattice}meet semilattice and a \index{join semilattice}join semilattice, it is called a \index{lattice}\textbf{lattice}. \end{de}\begin{ex}Let $S$ be a set. $\left(\mathcal{P}\kern-2pt\left(S\right), \subseteq{}\right)$ is a lattice. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{ex}\begin{thm}\label{theorem:cross-lattice-lift}Let $\left({X}_{1}, {\sqsubseteq }_{1}\right), \left({X}_{2}, {\sqsubseteq }_{2}\right), \dotsc{}, \left({X}_{n}, {\sqsubseteq }_{n}\right)$ be \index{lattice}lattices. The poset $\left({X}_{1}\times{}{X}_{2}\times{}\dotsb{}\times{}{X}_{n}, \sqsubseteq \right)$\footnote{See theorem \ref{theorem:cross-poset-lift} on page \pageref{theorem:cross-poset-lift}. } is a \index{lattice}lattice where the following properties hold. \[a\sqcup{}b={\bigsqcup_{{i}}}{{{a}_{i}\sqcup{}b}_{i}}\quad{}\text{and}\quad{}a\sqcap{}b={\bigsqcap_{{i}}}{{{a}_{i}\sqcap{}b}_{i}}\]\[\bot{}=\left({\bot{}}_{{X}_{1}}, {\bot{}}_{{X}_{2}}, \dotsc{}, {\bot{}}_{{X}_{n}}\right)\quad{}\text{and}\quad{}\top{}=\left({\top{}}_{{X}_{1}}, {\top{}}_{{X}_{2}}, \dotsc{}, {\top{}}_{{X}_{n}}\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}\label{definition:bounded-lattice}A \index{lattice}lattice$\left(X, \sqsubseteq \right)$ is called a \index{bounded lattice}bounded lattice if there exists both a \index{maximal element}maximal element $\top{}$ and a \index{minimal element}minimal element $\bot{}$ in $X$ as follows. \[\forall{}x\in{}X:\ x\sqsubseteq \top{}\wedge{}\bot{}\sqsubseteq x\]\end{de}\begin{de}\label{definition:complete-lattice}A \index{lattice}lattice$\left(X, \sqsubseteq \right)$ is called a \index{complete lattice}complete lattice if every (possibly infinite) subset $L$ of $X$ has an \index{infimum}infimum $Inf\kern-2pt\left(L\right)$ and a \index{supremum}supremum $Sup\kern-2pt\left(L\right)$. \end{de}\begin{thm}Every \index{complete lattice}complete lattice$\left(X, \sqsubseteq \right)$ is a \index{bounded lattice}bounded lattice where the \index{maximal element}maximal element is the \index{supremum}supremum of $X$ and the \index{minimal element}minimal element is the \index{infimum}infimum of $X$. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}Let $X$ be a \index{set}set. $X$ can be lifted to be a \index{poset}poset $\left(X, \bot{}\right)$ by adding a $\bot{}$ \index{element}element or  $\left(X, \top{}\right)$ by adding a $\top{}$ \index{element}element. \[\left({X}_{\bot{}}, \bot{}\right)\quad{}\left({X}^{\top{}}, \top{}\right)\]The \index{partial order}partial orders ${\sqsubseteq }_{\left({X}_{\bot{}}, \bot{}\right)}$ and ${\sqsubseteq }_{\left({X}^{\top{}}, \top{}\right)}$ are then defined as follows. \[{\sqsubseteq }_{\left({X}_{\bot{}}, \bot{}\right)}=\left\{\left(\bot{}, x\right)\mid{}x\in{}X\right\}\quad{}\text{and}\quad{}{\sqsubseteq }_{\left({X}^{\top{}}, \top{}\right)}=\left\{\left(x, \top{}\right)\mid{}x\in{}X\right\}\]\end{de}\begin{de}Let $X$ be a \index{set}set. $X$ can be lifted to be a so-called \index{flat lattice}\textbf{flat lattice}$\left(X_{\bot{}}^{\top{}}, \sqsubseteq \right)$ by defining the \index{partial order}partial order $\sqsubseteq $ as follows. \[\sqsubseteq =\left\{\left(\top{}, x\right)\mid{}x\in{}X_{\bot{}}^{\top{}}\right\}\cup{}\left\{\left(\bot{}, x\right)\mid{}x\in{}X_{\bot{}}^{\top{}}\right\}\cup{}\left\{\left(x, x\right)\mid{}x\in{}X_{\bot{}}^{\top{}}\right\}\]\end{de}\chapter{Functions}\section{Basics}\begin{de}A \index{partial function}\textbf{partial function} or \index{function}\textbf{function} $f$ is a triple $\left(f, A, B\right)$ where $A$ and B are sets and $f$ is a single-valued binary \index{relation}relation\footnote{See definition \ref{definition:relation} on page \pageref{definition:relation}. } between $A$ and $B$. Each of the sets $A$ and B have an equivalence relation defined on them, both written as $=$. \[\forall{}x, {y}_{1}, {y}_{2}:\ \left(\left(x, {y}_{1}\right)\in{}f\wedge{}\left(x, {y}_{2}\right)\in{}f\right)\Rightarrow{}{y}_{1}={y}_{2}\]The triple $\left(f, A, B\right)$ is often written as $f\kern-2pt:\, A\rightarrow{}B$. $A$ is called the \index{corange}\textbf{corange}, $B$ is called the \index{codomain}\textbf{codomain}. \end{de}\begin{de}A \index{function}function $f\kern-2pt:\, A\rightarrow{}B$ is called \index{total}\textbf{total} if its \index{corange}corange $A$ is equal to the \index{domain}domain\footnote{See definition \ref{definition:domain} on page \pageref{definition:domain}. } of $f$. \end{de}\begin{de}A \index{function}function $f\kern-2pt:\, A\rightarrow{}B$ is called \index{surjective}\textbf{surjective} if its \index{codomain}codomain $B$ is equal to the \index{image}image\footnote{See definition \ref{definition:image} on page \pageref{definition:image}. } of $f$. \end{de}\begin{nte}In mathematical literature, ``function'' often means ``total function''. \end{nte}\begin{de}\label{definition:binary-function}A \index{binary function}\textbf{binary function} is a function $f\kern-2pt:\, A\rightarrow{}B$ where $A$ is a set of tuples. \end{de}\begin{de}\label{definition:ternary-function}A \index{ternary function}\textbf{ternary function} is a function $f\kern-2pt:\, A\rightarrow{}B$ where $A$ is a set of triples. \end{de}\begin{de}Let $f\kern-2pt:\, A\rightarrow{}B$ be a \index{function}function. The \index{member-wise application}\textbf{member-wise application} of $f\kern-2pt:\, A\rightarrow{}B$ to a \index{subset}subset $S$ of $A$ is the set of all applications of $f$ to members of $S$ and is denoted as $f\,\square{}\,S$. \[f\,\square{}\,S\quad{}\equiv{}\quad{}\left\{f\kern-2pt\left(s\right)\mid{}s\in{}S\right\}\]\end{de}\begin{de}Let $f\kern-2pt:\, A\rightarrow{}B$ be a \index{function}function. The \index{member-wise application}\textbf{member-wise application} of $f\kern-2pt:\, A\rightarrow{}B$ to a \index{subset}subset $S$ of $A$ is the set of all applications of $f$ to members of $S$ and is denoted as $f\,\square{}\,S$. \[f\,\square{}\,S\quad{}\equiv{}\quad{}\left\{f\kern-2pt\left(s\right)\mid{}s\in{}S\right\}\]\end{de}\section{Binary operations}\begin{de}\label{definition:binary-operation}A \index{binary operation}\textbf{binary operation} is a \index{binary function}binary function as follows. \[f\kern-2pt:\, A\times{}A\rightarrow{}A\]\end{de}\begin{de}\label{definition:associative}A \index{binary operation}binary operation $\star{}$ is called \index{associative}\textbf{associative} if `placement of parentheses does not matter'. \[\forall{}a, b, c\in{}A:\ \left(a\star{}b\right)\star{}c=a\star{}\left(b\star{}c\right)\]\todo[color=red,inline,size=\small]{There is an example missing here.}\end{de}\section{Functions and orders}\begin{de}\label{definition:monotonic}Let $\left(X, {\sqsubseteq }_{X}\right)$ and $\left(Y, {\sqsubseteq }_{Y}\right)$ each be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}Y$ a function. $f\kern-2pt:\, X\rightarrow{}Y$ is said to be \index{monotonic}\textbf{monotonic} if it has the following property. \[\forall{}{X}_{1}, {X}_{2}\in{}X:\ {X}_{1}\,{\sqsubseteq }_{X}\,{X}_{2}\Rightarrow{}f\kern-2pt\left({X}_{1}\right)\,{\sqsubseteq }_{Y}\,f\kern-2pt\left({X}_{2}\right)\]\end{de}\begin{de}\label{definition:scott-continuous}Let $\left(X, {\sqsubseteq }_{X}\right)$ and $\left(Y, {\sqsubseteq }_{Y}\right)$ each be a \index{lattice}lattice\footnote{See definition \ref{definition:lattice} on page \pageref{definition:lattice}. } and $f\kern-2pt:\, X\rightarrow{}Y$ a function. $f\kern-2pt:\, X\rightarrow{}Y$ is called \index{Scott continuous}\textbf{Scott continuous} if it has the following property. \[\forall{}S\subseteq{}X:\ f\kern-2pt\left(Sup\kern-2pt\left(S\right)\right)=Sup\kern-2pt\left(f\,\square{}\,S\right)\]\end{de}\begin{de}\label{definition:fixed-point}Let $X$ and $Y$ be \index{set}sets $f\kern-2pt:\, X\rightarrow{}Y$ be a function. An element $a$ of $X$ is called a \index{fixed point}\textbf{fixed point} of $f$ if $f$ leaves a unchanged. \[f\kern-2pt\left(a\right)=a\]\end{de}\begin{de}\label{definition:least-fixed-point}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{function}function. The \index{least fixed point}\textbf{least fixed point} $lfp\kern-2pt\left(f\right)$ of $f$ is defined as follows. \[lfp\kern-2pt\left(f\right)\quad{}\equiv{}\quad{}Inf\kern-2pt\left(Fix\kern-2pt\left(f\right)\right)\]\end{de}\begin{de}\label{definition:greatest-fixed-point}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{function}function. The \index{greatest fixed point}\textbf{greatest fixed point} $gfp\kern-2pt\left(f\right)$ of $f$ is defined as follows. \[gfp\kern-2pt\left(f\right)\quad{}\equiv{}\quad{}Sup\kern-2pt\left(Fix\kern-2pt\left(f\right)\right)\]\end{de}\subsection{Regions}\begin{de}\label{definition:fixed-point-region}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{function}function. The \index{fixed point region}\textbf{fixed point region} $Fix\kern-2pt\left(f\right)$ is the \index{set}set of \index{fixed point}fixed points of $X$. \[Fix\kern-2pt\left(f\right)\quad{}\equiv{}\quad{}\left\{x\in{}X\mid{}x=f\kern-2pt\left(x\right)\right\}\]\end{de}\begin{de}\label{definition:ascending-region}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{function}function. The \index{ascending region}\textbf{ascending region} $Asc\kern-2pt\left(f\right)$ is the following \index{set}set. \[Asc\kern-2pt\left(f\right)\quad{}\equiv{}\quad{}\left\{x\in{}X\mid{}x\sqsubseteq f\kern-2pt\left(x\right)\right\}\]\end{de}\begin{de}\label{definition:descending-region}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{function}function. The \index{descending region}\textbf{descending region} $Desc\kern-2pt\left(f\right)$ is the following \index{set}set. \[Desc\kern-2pt\left(f\right)\quad{}\equiv{}\quad{}\left\{x\in{}X\mid{}f\kern-2pt\left(x\right)\sqsubseteq x\right\}\]\end{de}\begin{thm}\label{theorem:ascending-region-is-closed-under-application}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{monotonic}monotonic \index{function}function. \[\forall{}x\in{}X:\ X\in{}Asc\kern-2pt\left(f\right)\Rightarrow{}f\kern-2pt\left(X\right)\in{}Asc\kern-2pt\left(f\right)\]\begin{proof}Let $x$ be an element of $Asc\kern-2pt\left(f\right)$. Because $x\sqsubseteq f\kern-2pt\left(x\right)$ holds, and because $f$ is monotonic, $f\kern-2pt\left(x\right)\sqsubseteq f\kern-2pt\left(f\kern-2pt\left(x\right)\right)$ must also hold. This means that $f\kern-2pt\left(x\right)$ is in the ascending region. \end{proof}\end{thm}\begin{thm}\label{theorem:descending-region-is-closed-under-application}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{monotonic}monotonic \index{function}function. \[\forall{}x\in{}X:\ X\in{}Desc\kern-2pt\left(f\right)\Rightarrow{}f\kern-2pt\left(X\right)\in{}Desc\kern-2pt\left(f\right)\]\begin{proof}Let $x$ be an element of $Desc\kern-2pt\left(f\right)$. Because $f\kern-2pt\left(x\right)\sqsubseteq x$ holds, and because $f$ is monotonic, $f\kern-2pt\left(f\kern-2pt\left(x\right)\right)\sqsubseteq f\kern-2pt\left(x\right)$ must also hold. This means that $f\kern-2pt\left(x\right)$ is in the descending region. \end{proof}\end{thm}\begin{thm}\label{theorem:top-element-is-in-descending-region}Let $\left(X, \sqsubseteq \right)$ be a \index{bounded lattice}bounded lattice\footnote{See definition \ref{definition:bounded-lattice} on page \pageref{definition:bounded-lattice}. } and let $f\kern-2pt:\, X\rightarrow{}X$ a \index{monotonic}monotonic \index{function}function. \[\bot{}\in{}Asc\kern-2pt\left(f\right)\]\begin{proof}$f\kern-2pt\left(\bot{}\right)$ is an element of $X$ and must therefore have the property $\bot{}\sqsubseteq f\kern-2pt\left(\bot{}\right)$. This means that $\bot{}$ is an element of the ascending region. \end{proof}\end{thm}\begin{thm}\label{theorem:bot-element-is-in-ascending-region}Let $\left(X, \sqsubseteq \right)$ be a \index{bounded lattice}bounded lattice\footnote{See definition \ref{definition:bounded-lattice} on page \pageref{definition:bounded-lattice}. } and let $f\kern-2pt:\, X\rightarrow{}X$ a \index{monotonic}monotonic \index{function}function. \[\top{}\in{}Desc\kern-2pt\left(f\right)\]\begin{proof}$f\kern-2pt\left(\top{}\right)$ is an element of $X$ and must therefore have the property $f\kern-2pt\left(\top{}\right)\sqsubseteq \top{}$. This means that $\top{}$ is an element of the descending region. \end{proof}\end{thm}\begin{thm}\label{theorem:fixed-point-region-is-intersection-of-ascending-region-and-descending-region}Let $\left(X, \sqsubseteq \right)$ be a \index{poset}poset\footnote{See definition \ref{definition:poset} on page \pageref{definition:poset}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{monotonic}monotonic \index{function}function. \[Fix\kern-2pt\left(f\right)=Asc\kern-2pt\left(f\right)\cap{}Desc\kern-2pt\left(f\right)\]\begin{proof}\noindent{}\begin{itemize}\item{}\framebox[1.5\width]{$\subseteq{}$}\newline{}Let $a$ be an element of $Fix\kern-2pt\left(f\right)$. By definition of $Fix\kern-2pt\left(f\right)$, $f\kern-2pt\left(a\right)$ is equal to $a$. Because $\sqsubseteq $ is \index{reflexive}reflexive\footnote{See definition \ref{definition:reflexive} on page \pageref{definition:reflexive}. }\footnote{See definition \ref{definition:partial-order} on page \pageref{definition:partial-order}. }\footnote{See definition \ref{definition:preorder} on page \pageref{definition:preorder}. }, $a\sqsubseteq a$ must hold. This means that $a$ is both an element of $Asc\kern-2pt\left(f\right)$ and of $Desc\kern-2pt\left(f\right)$ and therefore in their intersection. \item{}\framebox[1.5\width]{$\supseteq{}$}\newline{}Let $a$ be an element of both $Asc\kern-2pt\left(f\right)$ and $Desc\kern-2pt\left(f\right)$. This means that both $a\sqsubseteq f\kern-2pt\left(a\right)$ and $f\kern-2pt\left(a\right)\sqsubseteq a$ hold. Because $\sqsubseteq $ is \index{antisymmetric}antisymmetric\footnote{See definition \ref{definition:antisymmetric} on page \pageref{definition:antisymmetric}. }, that means that $a$ equals $f\kern-2pt\left(a\right)$ which entails that $a$ is a fixed point of $f$. \end{itemize}\end{proof}\end{thm}\begin{de}\label{definition:kleene-chain}Let $\left(X, \sqsubseteq \right)$ be a \index{lattice}lattice\footnote{See definition \ref{definition:lattice} on page \pageref{definition:lattice}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{Scott continuous}Scott continuous function. The \index{Kleene chain}\textbf{Kleene chain} starting at a point $x\in{}X$ is the set $K\kern-2pt\left(x\right)$. \[K\kern-2pt\left(x\right)\quad{}\equiv{}\quad{}\left\{i\in{}\mathbb{N}\mid{}{f}_{i}\kern-2pt\left(X\right)\right\}\]\end{de}\begin{thm}\index{Kleene's fixed point theorem}\textbf{Kleene's fixed point theorem}\newline{}Let $\left(X, \sqsubseteq \right)$ be a \index{complete lattice}complete lattice\footnote{See definition \ref{definition:complete-lattice} on page \pageref{definition:complete-lattice}. } and $f\kern-2pt:\, X\rightarrow{}X$ a \index{Scott continuous}Scott continuous function. \[lfp\kern-2pt\left(f\right)=Sup\kern-2pt\left(K\kern-2pt\left(\bot{}\right)\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{nte}This gives us an algorithm to compute the least fixed point.. Repeatedly applying $f$ to bot until we find a fixed point is enough to find $lfp\kern-2pt\left(f\right)$. \end{nte}\begin{thm}\label{theorem:lattices-over-functions}Let $\left(X, \sqsubseteq \right)$ be a \index{lattice}lattice and $Y$ a set. $\left(X\rightarrow{}Y, \sqsubseteq \right)$ is a \index{lattice}lattice where $\sqsubseteq $ is defined as follows. \[f\sqsubseteq g\Leftrightarrow{}\forall{}a\in{}Dom\kern-2pt\left(f\right):\ f\kern-2pt\left(a\right)\sqsubseteq g\kern-2pt\left(a\right)\]This also implies the following. \[\left(f\sqcup{}g\right)\kern-2pt\left(a\right)=f\kern-2pt\left(a\right)\sqcup{}g\kern-2pt\left(a\right)\]\[\left(f\sqcap{}g\right)\kern-2pt\left(a\right)=f\kern-2pt\left(a\right)\sqcap{}g\kern-2pt\left(a\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\section{Distances}\subsection{Pseudometrics}\begin{de}\label{definition:distance}\label{definition:pseudometric}Let $S$ be a set. A \index{distance function}\textbf{distance function} $d$ for $S$ is a function $d\kern-2pt:\, S\times{}S\rightarrow{}{\mathbb{R}}^{+}$ with the following four properties. \begin{enumerate}\item{}$\forall{}x, y\in{}S\times{}S:\ d\kern-2pt\left(x, y\right)=0$\item{}$\forall{}x, y\in{}S\times{}S:\ d\kern-2pt\left(x, y\right)=d\kern-2pt\left(y, x\right)$\item{}The \index{triangle inequality}\textbf{triangle inequality}. \[\forall{}x, y, z\in{}\mathbb{R}:\ d\kern-2pt\left(x, y\right)+d\kern-2pt\left(y, z\right)\leq{}d\kern-2pt\left(x, z\right)\]\end{enumerate}A distance function is also called a \index{pseudometric}\textbf{pseudometric}. \end{de}\begin{ex}The \index{cosine distance}\textbf{cosine distance}. Let $q$ be a natural numbers. \[{d}_{cos}\kern-2pt:\, {\mathbb{R}}^{q}\times{}{\mathbb{R}}^{q}\rightarrow{}{\mathbb{R}}^{+}:\ \left(v, w\right)\mapsto{}\arcsin{}\kern-2pt\left(\frac{{v}^{T}w}{{\left\|{}v\right\|{}}_{2}{\left\|{}w\right\|{}}_{2}}\right)\]\todo[color=red,inline,size=\small]{Prove that this is actually a distance\newline{}There is a proof missing here.}\end{ex}\begin{de}\label{definition:jaccard-similarity}The \index{Jaccard similarity}\textbf{Jaccard similarity} of two sets $A$ and $B$ is defined as $J\kern-2pt\left(A, B\right)$. \[J\kern-2pt\left(A, B\right)\quad{}\equiv{}\quad{}\frac{\left|A\cap{}B\right|}{\left|A\cup{}B\right|}\]\end{de}\begin{de}\label{definition:jaccard-distance}The \index{Jaccard distance}\textbf{Jaccard distance} ${d}_{J}$ between two sets $A$ and $B$ is defined as $J\kern-2pt\left(A, B\right)-1$. \[{d}_{J}\kern-2pt\left(A, B\right)\quad{}\equiv{}\quad{}J\kern-2pt\left(A, B\right)-1\]\end{de}\todo[color=red,inline,size=\small]{prove that the Jaccard distance is in fact a distance}\begin{thm}Let $A$ and $B$ be sets. The \index{Jaccard similarity}Jaccard similarity of $A$ and $B$ is equal to the following expression. \[\frac{\left|A\cap{}B\right|}{\left|A\setminus{}B\right|+\left|B\setminus{}A\right|+\left|A\cap{}B\right|}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsection{Metrics}\begin{de}\label{definition:metric}A \index{metric}\textbf{metric} on a set $S$ is a distance function\footnote{See definition \ref{definition:distance} on page \pageref{definition:distance}. } $d$ on that set with one extra property. \[\forall{}v, w\in{}S:\ d\kern-2pt\left(v, w\right)=0\Leftrightarrow{}v=w\]\end{de}\begin{ex}The ${L}_{p}$-\index{metric}metrics. Let $p$ be a real number and $q$ a natural numbers. \[{L}_{p}\kern-2pt:\, {\mathbb{R}}^{q}\times{}{\mathbb{R}}^{q}\rightarrow{}{\mathbb{R}}^{+}:\ \left(v, w\right)\mapsto{}\sqrt[p]{\sum_{i=1}^{q}{{\left({v}_{i}-{w}_{i}\right)}^{p}}}\]\todo[color=red,inline,size=\small]{Prove that these are actually metrics\newline{}There is a proof missing here.}\end{ex}\begin{ex}The ${L}_{\infty{}}$-\index{metric}metric. \[{L}_{\infty{}}\kern-2pt:\, {\mathbb{R}}^{q}\times{}{\mathbb{R}}^{q}\rightarrow{}{\mathbb{R}}^{+}:\ \left(v, w\right)\mapsto{}{\max}_{i}\left|{v}_{i}-{w}_{i}\right|\]\todo[color=red,inline,size=\small]{Prove that this is actually a metric\newline{}There is a proof missing here.}\end{ex}\chapter{Groups}\chapter{Rings}\chapter{Fields}\chapter{Linear Algebra}\section{Vector Spaces}\begin{de}\label{definition:vector-space}\noindent{}\begin{enumerate}\item{}Let $\left(F, \star{}, \ast{}\right)$ be a field and let $V$ be a set. \todo[color=red,inline,size=\small]{There is a reference to ``field'' missing here. }\todo[color=red,inline,size=\small]{There is a reference to ``set'' missing here. }\item{}Let $\left(\bm{+}\right)$ be an internal binary operation on $V$. \[\left(\bm{+}\right)\kern-2pt:\, V\times{}V\rightarrow{}V\]\todo[color=red,inline,size=\small]{There is a reference to ``binary operation'' missing here. }\item{}Let $\left(\bm{\cdot{}}\right)$ be a binary operation. \[\left(\bm{\cdot{}}\right)\kern-2pt:\, F\times{}V\rightarrow{}V\]\end{enumerate}$\left(F, V, \bm{+}, \bm{\cdot{}}\right)$ is a \index{vector space}\textbf{vector space} over $F$ if the following properties hold. \begin{enumerate}\item{}$\left(V, \bm{+}\right)$ is a commutative group. \todo[color=red,inline,size=\small]{There is a reference to ``commutative group'' missing here. }\item{}$\left(V, \bm{\cdot{}}\right)$ is a monoid. \todo[color=red,inline,size=\small]{There is a reference to ``monoid'' missing here. }\item{}$\left(\bm{+}\right)$ is \index{distributive}distributive with respect to $\left(\bm{\cdot{}}\right)$. \item{}$\left(\bm{\cdot{}}\right)$ is \index{distributive}distributive with respect to $\left(\bm{+}\right)$. \item{}Mixed associativity:\[\forall{}r, s\in{}F:\ \left(r\ast{}s\right)\bm{\cdot{}}\vec{v}=r\bm{\cdot{}}\left(s\bm{\cdot{}}\vec{v}\right)\]\end{enumerate}\end{de}\section{Inproduct Spaces}\begin{de}Let $\left(\mathbb{C}, V, \bm{+}, \bm{\cdot{}}\right)$ be a vector space and let $\left\langle{}\cdot{}, \cdot{}\right\rangle{}\kern-2pt:\, V\times{}V\rightarrow{}\mathbb{C}$ be a binary operator. $\left\langle{}\cdot{}, \cdot{}\right\rangle{}$ is called a \index{semi-inner product}\textbf{semi-inner product} if it has the following properties. \begin{enumerate}\item{}\index{conjugate symmetry}\textbf{conjugate symmetry}\[\forall{}v, w\in{}V:\ \left\langle{}v, w\right\rangle{}=\overline{\left\langle{}w, v\right\rangle{}}\]\item{}\index{linearity in the first argument}\textbf{linearity in the first argument}\[\forall{}u, v, w\in{}V, \lambda{}\in{}\mathbb{C}:\ \left\langle{}\lambda{}\bm{\cdot{}}v\bm{+}u, w\right\rangle{}=\lambda{}\left\langle{}v, w\right\rangle{}+u\]\end{enumerate}\end{de}\begin{de}Let $\left(\mathbb{C}, V, \bm{+}, \bm{\cdot{}}\right)$ be a vector space and let $\left\langle{}\cdot{}, \cdot{}\right\rangle{}\kern-2pt:\, V\times{}V\rightarrow{}\mathbb{C}$ be a semi-inproduct. $\left\langle{}\cdot{}, \cdot{}\right\rangle{}$ is said to be an \index{inner product}\textbf{inner product} if it is also has the \index{positive-difiniteness}\textbf{positive-difiniteness} property. \[\forall{}v\in{}V:\ \left\langle{}v, v\right\rangle{}\geq{}0\wedge{}\left(\left\langle{}v, v\right\rangle{}=0\Leftrightarrow{}v=0\right)\]\end{de}\begin{ex}The following binary operation is an inproduct in $\left(\mathbb{R}, {\mathbb{R}}^{p}, +, \cdot{}\right)$. \[\left\langle{}\cdot{}, \cdot{}\right\rangle{}\kern-2pt:\, {\mathbb{R}}^{p}\times{}{\mathbb{R}}^{p}\rightarrow{}\mathbb{R}:\ \left(u, v\right)\mapsto{}{\sum_{{i}}}{{u}_{i}{v}_{i}}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}It is called the \index{vector dotproduct}\textbf{vector dotproduct}. \end{ex}\begin{de}\label{definition:inproduct-space}Let $\left(F, V, \bm{+}, \bm{\cdot{}}\right)$ be a vector space and $\left\langle{}\cdot{}, \cdot{}\right\rangle{}$ an inner product on it. $\left(F, V, \bm{+}, \bm{\cdot{}}, \left\langle{}\cdot{}, \cdot{}\right\rangle{}\right)$ is called an \index{inner product}\textbf{inner product} space. \end{de}\begin{ex}The field $\left(\mathbb{R}, {\mathbb{R}}^{p}, +, \cdot{}\right)$, equipped with the vector dotproduct, is an inner product space. \[\left(\mathbb{R}, {\mathbb{R}}^{p}, +, \cdot{}, \left\langle{}\cdot{}, \cdot{}\right\rangle{}\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}This is called the \index{Euclidean vector space}\textbf{Euclidean vector space} of dimension $p$. \end{ex}\chapter{Topology}\section{Topological Space}\begin{de}Let $X$ be a set. A \index{topology}\textbf{topology} $\tau{}$ on $X$ is a collection of subsets of $X$with the following properties. \begin{enumerate}\item{}$\emptyset{}\in{}\tau{}$\item{}$X\in{}\tau{}$\item{}Let $A$ be a subset of $\tau{}$. \[{\bigcup_{{a\in{}A}}}{a}\in{}\tau{}\]The union of any collection of open sets is an open set. \item{}Let $B$ be a finite subset of $\tau{}$. \[{\bigcap_{{b\in{}B}}}{b}\in{}\tau{}\]The intersection of any finite collection of open sets in an open set. \end{enumerate}These sets are called the \index{open}\textbf{open} sets of $X$. \end{de}\begin{de}Let $X$ be a set and $\tau{}$ a topology on $X$. $\left(X, \tau{}\right)$ is called a \index{topological space}\textbf{topological space}. \end{de}\begin{nte}In a sence, the sets in $\tau{}$ describe very abstractly which sets in $X$ are `close' to eachother without describing \emph{how} close they are. \end{nte}\begin{de}Let $\left(X, \tau{}\right)$ be a topological space. A subset $x$ of $X$ is called \index{closed}\textbf{closed} if $X\setminus{}x$ is open. \end{de}\begin{de}The \index{trivial topology}\textbf{trivial topology} on a set $X$ is the set $\left\{\emptyset{}, X\right\}$. This topology is also called the \index{indiscrete topology}\textbf{indiscrete topology}. \end{de}\begin{de}The \index{discrete topology}\textbf{discrete topology} on a set $X$ is the powerset $\mathcal{P}\kern-2pt\left(X\right)$ of $X$. \end{de}\begin{ex}The set $\left\{\emptyset{}, \left\{a\right\}, \left\{b\right\}\right\}$ is not a topology on the set $\left\{a, b\right\}$ because it does not contain the union of the sets $\left\{a\right\}$ and $\left\{b\right\}$. \end{ex}\begin{ex}On the set $\left\{a, b\right\}$ the following sets are all the possible topologies. \begin{itemize}\item{}$\left\{\emptyset{}, \left\{a, b\right\}\right\}$\item{}$\left\{\emptyset{}, \left\{a\right\}, \left\{a, b\right\}\right\}$\item{}$\left\{\emptyset{}, \left\{b\right\}, \left\{a, b\right\}\right\}$\item{}$\left\{\emptyset{}, \left\{a\right\}, \left\{b\right\}, \left\{a, b\right\}\right\}$\end{itemize}\end{ex}\begin{de}Let $\left(X, \tau{}\right)$ be a topological space. Let $Y$ be a subset of $X$. The set ${\tau{}}_{Y}$ is a topology on $Y$. \[{\tau{}}_{Y}=\left\{O\cap{}Y\mid{}O\in{}\tau{}\right\}\]${\tau{}}_{Y}$ is called the \index{induced topology}\textbf{induced topology} on $Y$ by $\tau{}$. \end{de}\begin{proof}\noindent{}\begin{itemize}\item{}The set $\emptyset{}\cap{}Y=\emptyset{}$ is in ${\tau{}}_{Y}$. \item{}The set $X\cap{}Y=Y$ is in ${\tau{}}_{Y}$. \item{}Let $A$ be a subset of ${\tau{}}_{Y}$. Per construction, this means that there exists a subset $B$ as follows . \[{\bigcup_{{a\in{}A}}}{a}={\bigcup_{{b\in{}B}}}{a\cap{}Y}\]Because of the distributivity of the union,\footnote{See theorem \ref{theorem:dristribution-law-2} on page \pageref{theorem:dristribution-law-2}. } this is equal to the following set. \[\left({\bigcup_{{b\in{}B}}}{a}\right)\cap{}Y\]Per construction of ${\tau{}}_{Y}$ this means that ${\bigcup_{{a\in{}A}}}{a}$ is in ${\tau{}}_{Y}$. \item{}Let $B$ be a finite subset of ${\tau{}}_{Y}$. Per construction, this means that there exists a subset $B$ as follows . \[{\bigcap_{{a\in{}A}}}{a}={\bigcap_{{a\in{}A}}}{a\cap{}Y}\]Because of the associativity of the intersection,\footnote{See property \ref{property:intersection-associative} on page \pageref{property:intersection-associative}. } this is equal to the following set. \[\left({\bigcap_{{a\in{}A}}}{a}\right)\cap{}Y\]Per construction of ${\tau{}}_{Y}$ this means that ${\bigcap_{{a\in{}A}}}{a}$ is in ${\tau{}}_{Y}$. \end{itemize}\end{proof}\begin{de}Two topologies ${\tau{}}_{1}$ and ${\tau{}}_{2}$ on a set $X$ are called \index{comparable}\textbf{comparable} if either is a subset of the other. \end{de}\begin{de}Let ${\tau{}}_{1}$ and ${\tau{}}_{2}$ be two comparable topologies on a set $X$ where ${{\tau{}}_{1}\subseteq{}\tau{}}_{2}$ holds. ${\tau{}}_{1}$ is called \index{finer}\textbf{finer} than ${\tau{}}_{2}$ and ${\tau{}}_{2}$ is called \index{coarser}\textbf{coarser} than ${\tau{}}_{1}$. If ${\tau{}}_{1}$ is a proper subset of ${\tau{}}_{2}$ then we would call them \index{strictly finer}\textbf{strictly finer} and \index{strictly coarser}\textbf{strictly coarser} respectively. \end{de}\section{Metric Spaces}\subsection{Pseudometric Spaces}\begin{de}\label{definition:pseudometric-space}Let $X$ be a set and $d$ a \index{pseudometric}pseudometric\footnote{See definition \ref{definition:distance} on page \pageref{definition:distance}. } on $X$. The tuple $\left(X, d\right)$ is called a \index{pseudometric space}\textbf{pseudometric space}. \end{de}\subsection{Metric Spaces}\begin{de}\label{definition:metric-space}Let $X$ be a set and $d$ a \index{metric}metric\footnote{See definition \ref{definition:metric} on page \pageref{definition:metric}. } on $X$. The tuple $\left(X, d\right)$ is called a \index{pseudometric space}\textbf{pseudometric space}. \end{de}\begin{ex}$\left(\mathbb{R}, d\kern-2pt:\, \mathbb{R}\times{}\mathbb{R}\rightarrow{}{\mathbb{R}}^{+}:\ \left(a, b\right)\mapsto{}\left|a-b\right|\right)$ is a metric space. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{ex}\chapter{Computability}\section{Symbols and strings}\begin{de}\label{definition:symbol}A \index{symbol}\textbf{symbol} is a representation of an abstract mathematical object.. The only prerequisite of a \index{symbol}symbol is that there is an equivalence relation $\underset{s}{=}$ defined on it. \todo[color=red,inline,size=\small]{There is a reference to ``equivalence relation'' missing here. }\end{de}\begin{de}\label{definition:alphabet}An \index{alphabet}\textbf{alphabet} $\Sigma{}$ is a finite \index{set}set of \index{symbol}symbols. \end{de}\begin{de}\label{definition:string}A \index{string}\textbf{string} $s$ over an \index{alphabet}alphabet $\Sigma{}$ is a ordered sequence of symbols ${a}_{i}$ in $\Sigma{}$. \[s={a}_{1}\dotsc{}{a}_{n}\]\end{de}\label{definition:empty-string}\begin{de}The \index{empty string}\textbf{empty string} $\epsilon{}$ is the \index{string}string of no symbols. \end{de}\begin{nte}$\epsilon{}$ is just the notation for the empty string. It is only used because writing down `nothing', even that word, is impractical. \end{nte}\begin{de}The \index{concatenation}\textbf{concatenation} $xy$ of two strings $x$ and $y$ is the following \index{string}string. \[xy\quad{}\equiv{}\quad{}{x}_{1}{x}_{2}\dotsc{}{x}_{m}{y}_{1}{y}_{2}\dotsc{}{y}_{n}\]\end{de}\begin{thm}The \index{concatenation}concatenation of strings is \index{associative}associative\footnote{See definition \ref{definition:associative} on page \pageref{definition:associative}. }. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}The \index{concatenation}concatenation of strings is \emph{not} \index{commutative}commutative. \todo[color=red,inline,size=\small]{There is an counter example missing here.}\end{thm}\begin{de}The \index{set}set of all strings over an \index{alphabet}alphabet $\Sigma{}$ is de:: Noted as ${\Sigma{}}^{*}$. \[{\Sigma{}}^{*}\quad{}\equiv{}\quad{}\left\{{a}_{1}{a}_{2}\dotsc{}{a}_{n}\mid{}{a}_{i}\in{}\Sigma{}, n, i\in{}\mathbb{N}\right\}\]\end{de}\begin{de}The \index{set}set $\Sigma{}\cup{}\left\{\epsilon{}\right\}$ is sometimes written more consisely as ${\Sigma{}}_{\epsilon{}}$. \end{de}\begin{nte}This is not just a set of symbols because $\epsilon{}$ is a string. \end{nte}\begin{de}\label{definition:reverse-string}The \index{reverse string}\textbf{reverse string} ${s}^{R}$ of a \index{string}string $s={a}_{1}\dotsc{}{a}_{n}$ is the \index{string}string wherein the symbols of $s$ are ordered in reverse. \[{s}^{R}\quad{}\equiv{}\quad{}{a}_{n}\dotsc{}{a}_{1}\]\end{de}\section{Languages}\begin{de}\label{definition:language}A \index{language}\textbf{language} over an \index{alphabet}alphabet $\Sigma{}$ is a \index{set}set of finite strings over that \index{alphabet}alphabet. \end{de}\begin{de}\label{definition:concatenation}The \index{concatenation}\textbf{concatenation} ${L}_{1}{L}_{2}$ of two languages ${L}_{1}$ and ${L}_{2}$ is the following \index{language}language. \[{L}_{1}{L}_{2}\quad{}\equiv{}\quad{}\left\{{s}_{1}{s}_{2}\mid{}{s}_{1}\in{}{L}_{1}, {s}_{2}\in{}{L}_{2}\right\}\]\end{de}\begin{thm}The \index{concatenation}concatenation of languages is \index{associative}associative\footnote{See definition \ref{definition:associative} on page \pageref{definition:associative}. }. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}The \index{concatenation}concatenation of languages is \emph{not} \index{commutative}commutative. \todo[color=red,inline,size=\small]{There is an counter example missing here.}\end{thm}\begin{de}The \index{concatenation}concatenation of a \index{language}language $L$ with itself $n$ times is de:: Noted as ${L}^{n}$. ${L}^{0}$ is defined as $\left\{\epsilon{}\right\}$. \[{L}^{n}\quad{}\equiv{}\quad{}L{L}^{n-1}\]\end{de}\begin{de}\label{definition:kleene-star}The \index{Kleene star}\textbf{Kleene star} ${L}^{*}$ of a \index{language}language $L$ is the \index{union}union of all the concatenations of $L$ with itself. \[{L}^{*}\quad{}\equiv{}\quad{}{\bigcup_{{n\in{}\mathbb{N}}}}{{L}^{n}}\]\end{de}\begin{de}${L}^{+}$ is defined as $L{L}^{*}$. \end{de}\begin{de}The \index{set}set of all languages over an \index{alphabet}alphabet $\Sigma{}$ is de:: Noted as follows. \[{L}_{\Sigma{}}\quad{}\equiv{}\quad{}\mathcal{P}\kern-2pt\left({\Sigma{}}^{*}\right)\]\end{de}\begin{thm}Infinite languages are countable. \todo[color=red,inline,size=\small]{There is a reference to ``countable'' missing here. }\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}There are uncountably infinitely many languages over a given \index{alphabet}alphabet. \todo[color=red,inline,size=\small]{There is a reference to ``uncountably infinite'' missing here. }\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}\label{definition:reverse-language}The \index{reverse language}\textbf{reverse language} ${L}^{R}$ is the \index{language}language of all reverse strings of the strings in $L$. \[{L}^{R}\quad{}\equiv{}\quad{}\left\{{s}^{R}\mid{}s\in{}L\right\}\]\end{de}\section{Regular Expressions}\begin{de}A \index{regular expression}\textbf{regular expression} (\index{RE}\textbf{RE}) over an \index{alphabet}alphabet $\Sigma{}$ is inductively defined as an expression of the following form. \begin{itemize}\item{}$\epsilon{}$\item{}$\phi{}$\item{}$a\text{ with }a\in{}\Sigma{}$\item{}$\left({E}_{1}{E}_{2}\right)$\item{}$\left({E}_{1}{E}_{2}\right)$\item{}${\left(E\right)}^{*}$\end{itemize}Here, $E, {E}_{1}, {E}_{2}$ must be regular expressions. \end{de}\begin{de}The \index{set}set of \index{regular expression}regular expressions over an \index{alphabet}alphabet $\Sigma{}$ is de:: Noted as ${RegEx}_{\Sigma{}}$. \end{de}\begin{de}\label{definition:language-of-a-regular-expression}The \index{language of a regular expression}\textbf{language of a regular expression} ${L}_{E}$ is inductively defined as follows. \begin{figure}[H]\centering{}$\begin{array}[c]{|c|c|c|c|c|c|}\hline E&{L}_{E}\\\hline \hline \epsilon{}&\epsilon{}\\\hline \phi{}&\emptyset{}\\\hline a\text{ with }a\in{}\Sigma{}&\left\{a\right\}\\\hline \left({E}_{1}{E}_{2}\right)&{L}_{{E}_{1}}{L}_{{E}_{2}}\\\hline \left({E}_{1}{E}_{2}\right)&{L}_{{E}_{1}}\cup{}{L}_{{E}_{2}}\\\hline {\left(E\right)}^{*}&{{L}_{E}}^{*}\\\hline \end{array}$\end{figure}\end{de}\begin{de}\label{definition:regular}A \index{language}language is called \index{regular}\textbf{regular} if it is the \index{language}language of a \index{regular expression}regular expression. \end{de}\begin{thm}If a \index{regular expression}regular expression does not contain an asterisk, its \index{language}language is finite. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{cex}The inverse of this theorem does not hold. ${L}_{{\left(\phi{}\right)}^{*}}$ is a counter example. \end{cex}\begin{de}The \index{set}set of all \index{regular}regular languages is de:: Noted as $RegLan$. \end{de}\begin{de}$RegLan$ is a subalgebra of ${L}_{\Sigma{}}$. \todo[color=red,inline,size=\small]{There is a reference to ``subalgebra'' missing here. }\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{de}\begin{thm}Every finite \index{language}language is \index{regular}regular. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}Let $L$ be a \index{language}language and $s$ be a \index{string}string over the same \index{alphabet}alphabet $\Sigma{}$. $L\cup{}\left\{s\right\}$ is \index{regular}regular. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}There exist non \index{regular}regular languages. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}For any \index{language}language $L$, ${L}^{R}$ is \index{regular}regular. \todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\section{Finite state automata}\subsection{NFSA}\begin{de}A \index{nondeterministic finite state automaton}\textbf{nondeterministic finite state automaton} is a $5$-tuple $hi$ where the following conents. \begin{enumerate}\item{}$Q$: a finite set of states. \item{}$\Sigma{}$: an alphabet. \item{}$\delta{}\kern-2pt:\, Q\times{}{\Sigma{}}_{\epsilon{}}\rightarrow{}\mathcal{P}\kern-2pt\left(Q\right)$: A transition function. \item{}${q}_{s}$: an initial state. \item{}$F$: a set of accepting states. \end{enumerate}\end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/1ed1c94283a6723a.eps}\caption{A \index{nondeterministic finite state automaton}nondeterministic finite state automaton. }\end{figure}\end{ex}\begin{de}\label{definition:accept}\label{definition:reject}A \index{nondeterministic finite state automaton}nondeterministic finite state automaton is said to \index{accept}\textbf{accept} a string $s={s}_{1}{s}_{2}\dotsc{}{s}_{n}$, also called an input word, if there exists a sequence of states ${r}_{1}{r}_{2}\dotsc{}{r}_{n}$ as follows. \begin{itemize}\item{}It starts in the initial state: ${r}_{1}={q}_{s}$. \item{}It ends in an accepting state: ${r}_{n}\in{}F$. \item{}It respects the transition function: \[\forall{}i\in{}\left\{1, \dotsc{}, n\right\}:\ {r}_{i+1}\in{}\delta{}\kern-2pt\left({r}_{i}, {s}_{i}\right)\]Note that this is slightly more simply specified than is actually the case. To make this simplification work, you must assume that between any two symbols in $s$ the empty string $\epsilon{}$ can be inserted if the transition function's symbol is $\epsilon{}$. \end{itemize}A \index{nondeterministic finite state automaton}nondeterministic finite state automaton is said to \index{reject}\textbf{reject} a string $s$ if it does not accept it. \end{de}\begin{ex}\noindent{}\begin{figure}[H]\centering{}\includegraphics[keepaspectratio=true,height=3.00000cm,width=0.5\textwidth{}]{/tmp/8166c19d8fd48cf7.eps}\caption{A \index{nondeterministic finite state automaton}nondeterministic finite state automaton. }\end{figure}This \index{nondeterministic finite state automaton}nondeterministic finite state automaton accepts the strings $p, pq, pqq, pqqq, ...$. \end{ex}\todo[color=red,inline,size=\small]{language of NFSA}\subsection{DFSA}\chapter{Probability}\begin{de}A \index{stochastic experiment}\textbf{stochastic experiment} is an \index{experiment}experiment of which the outcome is not known beforehand. \end{de}\begin{de}The \index{universe}\textbf{universe} of a \index{stochastic experiment}stochastic experiment is the set of all possible outcomes. It is de:: Noted as $\Omega{}$. \end{de}\begin{de}An \index{event}\textbf{event} of a \index{stochastic experiment}stochastic experiment is a \index{subset}subset of the \index{univers}univers. \end{de}\begin{de}\label{definition:bernoulli-experiment}A \index{Bernoulli experiment}\textbf{Bernoulli experiment} is a \index{stochastic experiment}stochastic experiment with only two possible outcomes. \end{de}\section{Sigma Algebra's}\begin{de}\label{definition:sigma-algebra}A \index{Sigma Algebra}\textbf{Sigma Algebra} or \index{$\sigma{}$-algebra}\textbf{$\sigma{}$-algebra} $\mathcal{A}$ is a \index{set}set of subsets of the \index{universe}universe $\Omega{}$ of a \index{stochastic experiment}stochastic experiment with the following three properties. \begin{enumerate}\item{}$\Omega{}\in{}\mathcal{A}$\item{}$\forall{}A:\ A\in{}\mathcal{A}\Rightarrow{}{{A}}^{C}\in{}\mathcal{A}$\item{}$\forall{}n\in{}\mathbb{N}:\ {A}_{n}\in{}\mathcal{A}\Rightarrow{}{\bigcup_{{n\in{}\mathbb{N}}}}{{A}_{n}}\in{}{A}_{n}$\end{enumerate}\end{de}\begin{de}$\left\{\emptyset{}, \Omega{}\right\}$ is called the \index{trivial $\sigma{}$-algebra}\textbf{trivial $\sigma{}$-algebra}\end{de}\begin{de}Let $\Omega{}$ be the \index{universe}universe of a \index{stochastic experiment}stochastic experiment and let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. The \index{tuple}tuple $\Omega{}, \mathcal{A}$ is called a \index{measurable spac}\textbf{measurable spac}. \end{de}\begin{de}$\Omega{}, \left\{\emptyset{}, \Omega{}\right\}$ is called the \index{trivial measurable space}\textbf{trivial measurable space}.\end{de}\begin{de}\label{definition:discrete-sigma-algebra}The \index{powerset}powerset of a \index{universe}universe $\Omega{}$ is called the \index{discrete $\sigma{}$-algebra}\textbf{discrete $\sigma{}$-algebra}. \end{de}\begin{thm}Let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[\emptyset{}\in{}\mathcal{A}\]\begin{proof}The first two axioms\footnote{See definition \ref{definition:sigma-algebra} on page \pageref{definition:sigma-algebra}. } together with ${{\Omega{}}}^{C}\underset{set}{=}\emptyset{}$ gives the theorem. \end{proof}\end{thm}\begin{thm}\label{theorem:sigma-algebra-finite-union}Let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[\left(\forall{}i\in{}\left\{1, \dotsc{}, n\right\}:\ {A}_{i}\in{}\mathcal{A}\right)\Rightarrow{}{\bigcup_{{i\in{}\left\{1, \dotsc{}, n\right\}}}}{{A}_{i}}\in{}\mathcal{A}\]\begin{proof}Use the third axiom\footnote{See definition \ref{definition:sigma-algebra} on page \pageref{definition:sigma-algebra}. } where only $n$ sets ${A}_{i}$ are non-empty. \end{proof}\end{thm}\begin{thm}\label{theorem:sigma-algebra-infinite-intersection}Let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[\left(\forall{}n\in{}\mathbb{N}:\ {A}_{n}\in{}\mathcal{A}\right)\Rightarrow{}{\bigcup_{{n\in{}\mathbb{N}}}}{{A}_{n}}\in{}\mathcal{A}\]\begin{proof}The first axiom\footnote{See definition \ref{definition:sigma-algebra} on page \pageref{definition:sigma-algebra}. }, together with the finite union of events of a \index{$\sigma{}$-algebra}$\sigma{}$-algebra\footnote{See theorem \ref{theorem:sigma-algebra-finite-union} on page \pageref{theorem:sigma-algebra-finite-union}. } and the second law of De Morgan\footnote{See theorem \ref{theorem:second-law-of-de-morgan} on page \pageref{theorem:second-law-of-de-morgan}. } give the proof. \end{proof}\end{thm}\begin{thm}\label{theorem:sigma-algebra-finite-intersection}Let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[\left(\forall{}i\in{}\left\{1, \dotsc{}, n\right\}:\ {A}_{i}\in{}\mathcal{A}\right)\Rightarrow{}{\bigcap_{{i\in{}\left\{1, \dotsc{}, n\right\}}}}{{A}_{i}}\in{}\mathcal{A}\]\begin{proof}Use the infinite intersection of events in a \index{$\sigma{}$-algebra}$\sigma{}$-algebra\footnote{See theorem \ref{theorem:sigma-algebra-infinite-intersection} on page \pageref{theorem:sigma-algebra-infinite-intersection}. } where only $n$ sets ${A}_{i}$ are not $\Omega{}$. \end{proof}\end{thm}\begin{thm}Let $\mathcal{A}$ be a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[\forall{}A, B\in{}\mathcal{A}:\ A\ \Delta{}\ B\in{}\mathcal{A}\]\begin{proof}The symmetric difference $A\ \Delta{}\ B$ is equal to $\left(A\cup{}{{B}}^{C}\right)\cap{}\left({{A}}^{C}\cup{}B\right)$.\footnote{See theorem \ref{theorem:sets-symmetric-difference-in-terms-of-union-and-intersection} on page \pageref{theorem:sets-symmetric-difference-in-terms-of-union-and-intersection}. }Now use the finite union\footnote{See theorem \ref{theorem:sigma-algebra-finite-union} on page \pageref{theorem:sigma-algebra-finite-union}. } and the finite intersection\footnote{See theorem \ref{theorem:sigma-algebra-finite-intersection} on page \pageref{theorem:sigma-algebra-finite-intersection}. } of sets in a \index{$\sigma{}$-algebra}$\sigma{}$-algebra together with the second axiom\footnote{See definition \ref{definition:sigma-algebra} on page \pageref{definition:sigma-algebra}. }. \end{proof}\end{thm}\begin{cex}The set union of two \index{$\sigma{}$-algebra}$\sigma{}$-algebra's is not necessarily a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \begin{proof}Take, for example, the \index{$\sigma{}$-algebra}$\sigma{}$-algebra's $B$ and $A$. \[B = \left\{\emptyset{}, \left\{0\right\}, \left\{1,2\right\}\left\{0,1,2\right\}\right\}\text{ and }C = \left\{\emptyset{}, \left\{1\right\}, \left\{0,2\right\}\left\{0,1,2\right\}\right\}\]The union of $B$ and $C$ is \emph{not} a \index{$\sigma{}$-algebra}$\sigma{}$-algebra. \[B\cup{}C\underset{set}{=}\left\{\emptyset{}, \left\{0\right\}, \left\{1\right\}, \left\{0,2\right\}, \left\{1,2\right\}, \left\{0,1,2\right\}\right\}\]The union of $\left\{0\right\}$ and $\left\{1\right\}$, for example, is not in $B\cup{}C$.\footnote{See theorem \ref{theorem:sigma-algebra-finite-union} on page \pageref{theorem:sigma-algebra-finite-union}. }. \end{proof}\end{cex}\subsection{Generating \index{$\sigma{}$-algebra}$\sigma{}$-algebras}\begin{de}Let $\mathcal{C}\subseteq{}\Omega{}$ be a \index{set}set of subsets of a \index{universe}universe. The \index{$\sigma{}$-algebra generated by }\textbf{$\sigma{}$-algebra generated by }$\mathcal{C}$ is the smallest \index{$\sigma{}$-algebra}$\sigma{}$-algebra that contains $\mathcal{C}$. \end{de}\begin{thm}The \index{$\sigma{}$-algebra generated by }$\sigma{}$-algebra generated by  a \index{set}set of subsets $\mathcal{C}$ of a \index{universe}universe $\Omega{}$ is unique. \begin{proof}This follows directly from the definition of equality of sets.\footnote{See definition \ref{definition:sets-equality} on page \pageref{definition:sets-equality}. }. \end{proof}\end{thm}\begin{thm}\textbf{Exam Question: Probability @ KU Leuven, June 2014}\newline{}The \index{$\sigma{}$-algebra generated by }$\sigma{}$-algebra generated by  a \index{set}set of subsets $\mathcal{C}$ of a \index{universe}universe $\Omega{}$ always exists. \begin{proof}Let $X$ be a set of sigma algebras as follows. \[X=\left\{\mathcal{A}\subseteq{}\mathcal{P}\kern-2pt\left(\Omega{}\right)\mid{}\mathcal{A}\text{ is a \index{$\sigma{}$-algebra}$\sigma{}$-algebra and }\mathcal{C}\subseteq{}\mathcal{A}\right\}\]$X$ is not empty because $\mathcal{P}\kern-2pt\left(\Omega{}\right)$ is always a $\sigma{}$-algebra.\footnote{See definition \ref{definition:discrete-sigma-algebra} on page \pageref{definition:discrete-sigma-algebra}. }. Let $\mathcal{B}$ be the following intersection. \[\mathcal{B}={\bigcap_{{\mathcal{A}\in{}X}}}{\mathcal{A}}\]$\mathcal{B}$ definitely contains $\mathcal{C}$ because $\mathcal{C}$ is a subset of all $\mathcal{A}\in{}X$.  We will now show that $\mathcal{B}$ is a $\sigma{}$-algebra. \begin{enumerate}\item{}$\Omega{}\in{}\mathcal{B}$\newline{}Every $\mathcal{A}$ in $\mathcal{P}\kern-2pt\left(\Omega{}\right)$ contains $\Omega{}$ because they are all $\sigma{}$-algebras. $\mathcal{B}$ must therefore contain $\Omega{}$. \item{}$\forall{}B\subseteq{}\Omega{}:\ B\in{}\mathcal{B}\Rightarrow{}{{B}}^{C}\in{}\mathcal{B}$\newline{}Let $B$ be a subset of $\Omega{}$ in $\mathcal{B}$. This means that $B$ is also contained in every $\mathcal{A}$ of $X$. Because every $\mathcal{A}$ in $X$ is a $\sigma{}$-algebra, $\mathcal{A}$ must also contain ${{B}}^{C}$.\footnote{See definition \ref{definition:sigma-algebra} on page \pageref{definition:sigma-algebra}. }. This means that $\mathcal{B}$ must also contains ${{B}}^{C}$. \item{}$\left(\forall{}n\in{}\mathbb{N}:\ {B}_{n}\in{}\mathcal{B}\right)\Rightarrow{}{\bigcup_{{n\in{}\mathbb{N}}}}{{B}_{n}}\in{}\mathcal{B}$\newline{}The reasoning for this part is analogous to the reasoning for the previous part.\end{enumerate}\end{proof}\end{thm}\section{Probability Measures}\begin{de}\label{definition:probability-measure}Let $\Omega{}, \mathcal{A}$ be a \index{measurable space}measurable space. A \index{probability measure}\textbf{probability measure} is a function $P$ with the following three properties:. \[P\kern-2pt:\, \mathcal{A}\rightarrow{}\interval{0}{1}\]\begin{enumerate}\item{}$P\kern-2pt\left(\Omega{}\right)=1$\item{}$\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\right)\geq{}0$\item{}\index{countable additivity}\textbf{countable additivity}\newline{}Let ${\left({A}_{n}\right)}_{n}$ be a countably infinite \index{sequence}sequence of pairwise disjunct sets. \[P\kern-2pt\left({\bigcup_{{n\in{}\mathbb{N}}}}{{A}_{n}}\right)={\sum_{{n\in{}\mathbb{N}}}}{P\kern-2pt\left({A}_{n}\right)}\]\end{enumerate}$P\kern-2pt\left(A\right)$ is called the \index{probability}\textbf{probability} that $A$ happens. \end{de}\begin{de}Let $\Omega{}, \mathcal{A}$ be a \index{measurable space}measurable space and $P$ a \index{probability measure}probability measure. $\Omega{}, \mathcal{A}, P$ is called a \index{probability space}\textbf{probability space}\end{de}\begin{thm}\label{theorem:probability-measure-finite-additivity}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space and let $\left\{{A}_{n}\mid{}n\in{}\left\{1, \dotsc{}, N\right\}\right\}$ be $N$ pairwise disjunct events of $\mathcal{A}$. \[P\kern-2pt\left(\bigcup_{n=1}^{N}{{A}_{n}}\right)=\sum_{n=1}^{N}{P\kern-2pt\left({A}_{n}\right)}\]\begin{proof}Use the \index{countable additivity}countable additivity property of probability measures\footnote{See definition \ref{definition:probability-measure} on page \pageref{definition:probability-measure}. } where only $n$ sets are non-empty. \end{proof}\end{thm}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left({{A}}^{C}\right)=1-P\kern-2pt\left(A\right)\]\begin{proof}Let $A$ be an event in $\mathcal{A}$. The union of $A$ and its complement is $\Omega{}$.\footnote{See theorem \ref{theorem:complementary-law-union} on page \pageref{theorem:complementary-law-union}. }. \begin{align*}\Omega{}&\underset{set}{=}A\cup{}{{A}}^{C}\\P\kern-2pt\left(\Omega{}\right)&=P\kern-2pt\left(A\cup{}{{A}}^{C}\right)\\1&=P\kern-2pt\left(A\right)+P\kern-2pt\left({{A}}^{C}\right)\\P\kern-2pt\left({{A}}^{C}\right)&=1-P\kern-2pt\left(A\right)\end{align*}Notice that the second equivalence only holds because of the finite additivity propertiy of probability measures.\footnote{See theorem \ref{theorem:probability-measure-finite-additivity} on page \pageref{theorem:probability-measure-finite-additivity}. }\end{proof}\begin{con}$P\kern-2pt\left(\emptyset{}\right)=0$\end{con}\end{thm}\begin{prop}\label{property:probability-partion-by-intersection}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A, B\in{}\mathcal{A}:\ P\kern-2pt\left(B\right)=P\kern-2pt\left(B\cap{}A\right)+P\kern-2pt\left(B\cap{}{{A}}^{C}\right)\]\begin{proof}Because $B\cap{}A$ and $B\cap{}{{A}}^{C}$ are disjunct, the theorem follows from the finite additivity property of probability measures. \footnote{See theorem \ref{theorem:probability-measure-finite-additivity} on page \pageref{theorem:probability-measure-finite-additivity}. }\end{proof}\end{prop}\begin{prop}\label{property:probability-set-union}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A, B\in{}\mathcal{A}:\ P\kern-2pt\left(A\cup{}B\right)=P\kern-2pt\left(A\right)+P\kern-2pt\left(B\right)-P\kern-2pt\left(A\cap{}B\right)\]\begin{proof}Let $A$ and $B$ be events in $\mathcal{A}$. \begin{align*}P\kern-2pt\left(A\cup{}B\right)&=P\kern-2pt\left(\left(A\cap{}{{B}}^{C}\right)\cup{}\left(A\cap{}B\right)\cup{}\left({{A}}^{C}\cap{}B\right)\right)\\&=P\kern-2pt\left(A\cap{}{{B}}^{C}\right)+P\kern-2pt\left(A\cap{}B\right)+P\kern-2pt\left({{A}}^{C}\cap{}B\right)\\&=P\kern-2pt\left(A\cap{}{{B}}^{C}\right)+P\kern-2pt\left(A\cap{}B\right)+P\kern-2pt\left({{A}}^{C}\cap{}B\right)+\left(P\kern-2pt\left(A\cap{}B\right)-P\kern-2pt\left(A\cap{}B\right)\right)\\&=\left(P\kern-2pt\left(A\cap{}{{B}}^{C}\right)+P\kern-2pt\left(A\cap{}B\right)\right)+\left(P\kern-2pt\left({{A}}^{C}\cap{}B\right)+P\kern-2pt\left(A\cap{}B\right)\right)-P\kern-2pt\left(A\cap{}B\right)\\&=P\kern-2pt\left(A\right)+P\kern-2pt\left(B\right)-P\kern-2pt\left(A\cap{}B\right)\end{align*}Note that we used the previous property in the last equation.\footnote{See property \ref{property:probability-partion-by-intersection} on page \pageref{property:probability-partion-by-intersection}. }\end{proof}\end{prop}\begin{prop}\label{property:probability-set-difference}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A, B\in{}\mathcal{A}:\ P\kern-2pt\left(A\setminus{}B\right)=P\kern-2pt\left(A\cup{}B\right)-P\kern-2pt\left(B\right)\]\begin{proof}Let $A$ and $B$ be events in $\mathcal{A}$. \[P\kern-2pt\left(A\cup{}B\right)=P\kern-2pt\left(B\setminus{}\left(B\cap{}{{A}}^{C}\right)\right)=P\kern-2pt\left(B\right)+P\kern-2pt\left(A\setminus{}B\right)\]Note that we used the equivalent definition of set difference in the first equation.\footnote{See theorem \ref{theorem:set-difference-equivalent-definition} on page \pageref{theorem:set-difference-equivalent-definition}. }\end{proof}\end{prop}\begin{prop}\label{property:probability-subset-implies-smaller-probability}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A, B\in{}\mathcal{A}:\ A\subseteq{}B\Rightarrow{}P\kern-2pt\left(A\right)\leq{}P\kern-2pt\left(B\right)\]\begin{proof}\[P\kern-2pt\left(A\right)=P\kern-2pt\left(B\setminus{}\left(B\cap{}A\right)\right)=P\kern-2pt\left(B\right)-P\kern-2pt\left(B\cap{}A\right)\leq{}P\kern-2pt\left(B\right)\]Note that in the first equation we used that $A$ is a subset of $B$ and in the second equation, we used the previous property. \footnote{See property \ref{property:probability-set-difference} on page \pageref{property:probability-set-difference}. }\end{proof}\end{prop}\begin{prop}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\right)\leq{}1\]\begin{proof}Every set $A$ is a subset of $\Omega{}$\footnote{See theorem \ref{theorem:sets-every-set-is-a-subset-of-the-universe} on page \pageref{theorem:sets-every-set-is-a-subset-of-the-universe}. }.  so $P\kern-2pt\left(A\right)$ must be smaller than $P\kern-2pt\left(\Omega{}\right)=1$\footnote{See definition \ref{definition:probability-measure} on page \pageref{definition:probability-measure}. }\footnote{See property \ref{property:probability-subset-implies-smaller-probability} on page \pageref{property:probability-subset-implies-smaller-probability}. }. \end{proof}\end{prop}\subsection{Traditional Probability Measures}\begin{de}The \index{uniforme probability measure}\textbf{uniforme probability measure} is a \index{probability measure}probability measure that is only defined for measurable spaces with a finite \index{universe}universe. \[P\kern-2pt:\, \mathcal{A}\rightarrow{}\interval{0}{1}\subseteq{}\mathbb{R}:\ A\mapsto{}\frac{\left|A\right|}{\left|\Omega{}\right|}\]\end{de}\begin{de}The \index{discrete probability measure}\textbf{discrete probability measure} is a \index{probability measure}probability measure that is only defined for measure spaces with a countable \index{universe}universe. \[P\kern-2pt:\, \mathcal{A}\rightarrow{}\interval{0}{1}\subseteq{}\mathbb{R}:\ {A}_{i}\mapsto{}{p}_{i}\]\end{de}\section{Conditional probibility}\subsection{Basics}\begin{de}\label{definition:conditional-probability}The \index{conditional probability}\textbf{conditional probability} of an \index{event}event$A\in{}\mathcal{A}$ given an \index{event}event$B\in{}\mathcal{A}$ with $P\kern-2pt\left(B\right)\not=0$ is de:: Noted as $P\kern-2pt\left(A\;\middle|\;B\right)$. \[P\kern-2pt\left(A\;\middle|\;B\right)\quad{}\equiv{}\quad{}\frac{P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(B\right)}\]\end{de}\begin{prop}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\;\middle|\;A\right)=1\]\begin{proof}\[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\;\middle|\;A\right)=\frac{P\kern-2pt\left(A\cap{}A\right)}{P\kern-2pt\left(A\right)}=\frac{P\kern-2pt\left(A\right)}{P\kern-2pt\left(A\right)}=1\]\end{proof}\end{prop}\begin{prop}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\;\middle|\;\Omega{}\right)=P\kern-2pt\left(A\right)\]We say that every event is independent of the universe.\begin{proof}\[\forall{}A\in{}\mathcal{A}:\ P\kern-2pt\left(A\;\middle|\;\Omega{}\right)=\frac{P\kern-2pt\left(A\cap{}\Omega{}\right)}{P\kern-2pt\left(\Omega{}\right)}=\frac{P\kern-2pt\left(A\right)}{P\kern-2pt\left(\Omega{}\right)}=\frac{P\kern-2pt\left(A\right)}{1}=P\kern-2pt\left(A\right)\]\end{proof}\end{prop}\subsection{Chain rule}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $\left\{{A}_{1}, {A}_{2}, \dotsc{}, {A}_{k}\right\}$ be more than one event in $\mathcal{A}$. \[P\kern-2pt\left(\bigcap_{i=1}^{k}{{A}_{i}}\right)=\prod_{i=1}^{k}{P\kern-2pt\left({A}_{i}\;\middle|\;\bigcap_{j=1}^{i-1}{{A}_{j}}\right)}\]\[P\kern-2pt\left({A}_{1}\cap{}{A}_{2}\cap{}\dotsb{}\cap{}{A}_{k}\right)=P\kern-2pt\left({A}_{1}\right)P\kern-2pt\left({A}_{2}\;\middle|\;{A}_{1}\right)P\kern-2pt\left({A}_{3}\;\middle|\;{A}_{1}\cap{}{A}_{2}\right)\dotsb{}P\kern-2pt\left({A}_{k}\;\middle|\;{A}_{1}\cap{}{A}_{2}\cap{}\dotsb{}\cap{}{A}_{k-1}\right)\]\begin{proof}Proof by induction on $\mathbb{N}\setminus{}\left\{1, 2\right\}$. \begin{itemize}\item{}The theorem holds for $k=2$\footnote{See definition \ref{definition:conditional-probability} on page \pageref{definition:conditional-probability}. }. \[P\kern-2pt\left({A}_{1}\;\middle|\;{A}_{2}\right)=\frac{P\kern-2pt\left({A}_{1}\cap{}{A}_{2}\right)}{P\kern-2pt\left({A}_{2}\right)}\Rightarrow{}P\kern-2pt\left({A}_{1}\cap{}{A}_{2}\right)=P\kern-2pt\left({A}_{1}\right)P\kern-2pt\left({A}_{2}\;\middle|\;{A}_{1}\right)\]\item{}From the theorem for $k=n$ the theorem for $k=n+1$ follows. \begin{align*}P\kern-2pt\left(\bigcap_{i=1}^{n+1}{{A}_{i}}\right)&=P\kern-2pt\left(\left(\bigcap_{i=1}^{n}{{A}_{i}}\right)\cap{}{A}_{n+1}\right)\\&=P\kern-2pt\left({A}_{n+1}\;\middle|\;\left(\bigcap_{i=1}^{n}{{A}_{i}}\right)\right)P\kern-2pt\left(\bigcap_{i=1}^{n}{{A}_{i}}\right)\\&=P\kern-2pt\left({A}_{n+1}\;\middle|\;\left(\bigcap_{i=1}^{n}{{A}_{i}}\right)\right)\prod_{i=1}^{n}{P\kern-2pt\left({A}_{i}\;\middle|\;\bigcap_{j=1}^{i-1}{{A}_{j}}\right)}\\&=\prod_{i=1}^{n+1}{P\kern-2pt\left({A}_{i}\;\middle|\;\bigcap_{j=1}^{i-1}{{A}_{j}}\right)}\end{align*}The base case is used in the second equation and the induction hypothesis is used in the second equation. \end{itemize}\end{proof}\end{thm}\subsection{Law of total probability}\begin{thm}\label{theorem:law-of-total-probability}\textbf{Exam Question: Probability @ KU Leuven, August 2013}\newline{}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $X$ be a \index{partition}partition of $\Omega{}$ in which $\forall{}A\in{}X:\ P\kern-2pt\left(A\right)>0$ holds. \[\forall{}B\in{}\mathcal{A}:\ P\kern-2pt\left(B\right)={\sum_{{A\in{}X}}}{P\kern-2pt\left(A\right)P\kern-2pt\left(B\;\middle|\;A\right)}\]\begin{proof}\begin{align*}{\sum_{{A\in{}X}}}{P\kern-2pt\left(A\right)P\kern-2pt\left(B\;\middle|\;A\right)}&={\sum_{{A\in{}X}}}{\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(A\right)}}\\&={\sum_{{A\in{}X}}}{P\kern-2pt\left(B\cap{}A\right)}\\&=P\kern-2pt\left({\bigcup_{{A\in{}X}}}{B\cap{}A}\right)\\&=P\kern-2pt\left(B\cap{}{\bigcup_{{A\in{}X}}}{A}\right)\\&=P\kern-2pt\left(B\cap{}\Omega{}\right)=P\kern-2pt\left(B\right)\end{align*}Note that the third equation only holds because $X$ is a partition of $\Omega{}$ and the sets $B\cap{}A$ are therefore disjunct . The fifth equation also only holds because $X$ is a partition of $\Omega{}$. \end{proof}\end{thm}\subsection{Bayes' theorem}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $X$ be a \index{partition}partition of $\Omega{}$ in which $\forall{}A\in{}X:\ P\kern-2pt\left(A\right)>0$ holds. Let $B$ be an event in $\mathcal{A}$ for which $P\kern-2pt\left(B\right)>0$ holds. \[\forall{}A\in{}X:\ P\kern-2pt\left(A\;\middle|\;B\right)=\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(B\;\middle|\;A\right)}{{\sum_{{C\in{}X}}}{P\kern-2pt\left(C\right)P\kern-2pt\left(B\;\middle|\;C\right)}}\]\begin{proof}Let $A$ be an event in $X$. \begin{align*}\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(B\;\middle|\;A\right)}{{\sum_{{C\in{}X}}}{P\kern-2pt\left(C\right)P\kern-2pt\left(B\;\middle|\;C\right)}}&=\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(B\;\middle|\;A\right)}{P\kern-2pt\left(B\right)}\\&=\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(B\cap{}A\right)}{P\kern-2pt\left(B\right)P\kern-2pt\left(A\right)}\\&=\frac{P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(B\right)}\\&=P\kern-2pt\left(A\;\middle|\;B\right)\end{align*}Note that the first equation holds by the law of total probability\footnote{See theorem \ref{theorem:law-of-total-probability} on page \pageref{theorem:law-of-total-probability}. }. \end{proof}\end{thm}\subsection{Handy rules of computation}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. \[\forall{}A, B\in{}\mathcal{A}:\ P\kern-2pt\left(A\;\middle|\;B\right)=\frac{P\kern-2pt\left(A\right)}{P\kern-2pt\left(B\right)}P\kern-2pt\left(B\;\middle|\;A\right)\]\begin{proof}\begin{align*}P\kern-2pt\left(A\;\middle|\;B\right)&=\frac{P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(B\right)}\\&=\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(B\right)P\kern-2pt\left(A\right)}\\&=\frac{P\kern-2pt\left(A\right)}{P\kern-2pt\left(B\right)}\frac{P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(A\right)}\\&=\frac{P\kern-2pt\left(A\right)}{P\kern-2pt\left(B\right)}P\kern-2pt\left(B\;\middle|\;A\right)\end{align*}\end{proof}\end{thm}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $X$ be a \index{partition}partition of $\Omega{}$ in which $\forall{}A\in{}X:\ P\kern-2pt\left(A\right)>0$ holds. \[\forall{}X\in{}\Omega{}:\ {\sum_{{A\in{}X}}}{P\kern-2pt\left(A\;\middle|\;X\right)}=1\]\begin{proof}Let $X$ be an event in $\Omega{}$. \begin{align*}{\sum_{{A\in{}X}}}{P\kern-2pt\left(A\;\middle|\;X\right)}&={\sum_{{A\in{}X}}}{\frac{P\kern-2pt\left(A\cap{}X\right)}{P\kern-2pt\left(X\right)}}\\&=\frac{{\sum_{{A\in{}X}}}{P\kern-2pt\left(A\cap{}X\right)}}{P\kern-2pt\left(X\right)}\\&=\frac{P\kern-2pt\left({\bigcup_{{A\in{}X}}}{A\cap{}X}\right)}{P\kern-2pt\left(X\right)}\\&=\frac{P\kern-2pt\left(\Omega{}\cap{}X\right)}{P\kern-2pt\left(X\right)}\\&=\frac{P\kern-2pt\left(X\right)}{P\kern-2pt\left(X\right)}=1\end{align*}\end{proof}\end{thm}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $\left\{{A}_{i}\mid{}i\in{}\mathbb{N}\right\}$ be a countable set in $\mathcal{A}$. \[P\kern-2pt\left({\bigcup_{{n\in{}\mathbb{N}}}}{{A}_{n}}\right)=P\kern-2pt\left({A}_{1}\right)+{\sum_{{n>1}}}{P\kern-2pt\left({A}_{n}\cap{}\bigcap_{i=1}^{n-1}{{{{A}_{i}}}^{C}}\right)}\]\begin{proof}Proof by induction on $\mathbb{N}$. \begin{itemize}\item{}The theorem holds for $n=1$. \[P\kern-2pt\left({A}_{1}\cup{}{A}_{2}\right)=P\kern-2pt\left({A}_{1}\cup{}\left({{{A}_{1}}}^{C}\cap{}{A}_{2}\right)\right)=P\kern-2pt\left({A}_{1}\right)+P\kern-2pt\left({{{A}_{1}}}^{C}\cap{}{A}_{2}\right)\]\item{}From the theorem for $k=n$ follows the theorem for $k+1$. \[P\kern-2pt\left(\bigcup_{i=1}^{n+1}{{A}_{i}}\right)=P\kern-2pt\left({A}_{n+1}\cup{}\bigcup_{i=1}^{n}{{A}_{i}}\right)=P\kern-2pt\left({{\left(\bigcup_{i=1}^{n}{{A}_{i}}\right)}}^{C}\cap{}{A}_{n+1}\right)+P\kern-2pt\left(\bigcup_{i=1}^{n}{{A}_{i}}\right)=P\kern-2pt\left({A}_{1}\right)+{\sum_{{n>1}}}{P\kern-2pt\left({A}_{n}\cap{}\bigcup_{i=1}^{n-1}{{{{A}_{i}}}^{C}}\right)}\]\end{itemize}\end{proof}\end{thm}\section{Independence}\begin{de}\label{definition:independence-events-in-probabiliy-space}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Two events $A$ and $B$ in $\mathcal{A}$ are called \index{independent}\textbf{independent} if the following equality holds. \[P\kern-2pt\left(A\cap{}B\right)=P\kern-2pt\left(A\right)P\kern-2pt\left(B\right)\]\end{de}\begin{de}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. If two events $A$ and $B$ in $\mathcal{A}$ are not \index{independent}independent, they are called \index{dependent}\textbf{dependent} events. This depedence is called... \begin{itemize}\item{}\index{positive dependence}\textbf{positive dependence} if $P\kern-2pt\left(A\cap{}B\right)>P\kern-2pt\left(A\right)P\kern-2pt\left(B\right)$ holds. \item{}\index{negative dependence}\textbf{negative dependence} if $P\kern-2pt\left(A\cap{}B\right)<P\kern-2pt\left(A\right)P\kern-2pt\left(B\right)$ holds. \end{itemize}\end{de}\begin{thm}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $A$ and $B$ be two \index{independent}independent events in $\mathcal{A}$. \[P\kern-2pt\left(A\;\middle|\;B\right)=P\kern-2pt\left(A\right)\]\begin{proof}\[P\kern-2pt\left(A\;\middle|\;B\right)=\frac{P\kern-2pt\left(A\cap{}B\right)}{P\kern-2pt\left(B\right)}=\frac{P\kern-2pt\left(A\right)P\kern-2pt\left(B\right)}{P\kern-2pt\left(B\right)}=P\kern-2pt\left(A\right)\]\end{proof}\end{thm}\begin{de}A set of events is called \index{pairwise independent}\textbf{pairwise independent} if every two events in the set are independent. \end{de}\begin{de}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. A set $X$ of events is called \index{mutual independence}\textbf{mutual independence} if the following holds. \[\forall{}Y\in{}\mathcal{P}\kern-2pt\left(X\right):\ P\kern-2pt\left({\bigcap_{{A\in{}Y}}}{A}\right)={\prod_{{A\in{}Y}}}{P\kern-2pt\left(A\right)}\]\end{de}\begin{thm}Mutual independence implies pairwise independence. \todo[color=red,inline,size=\small]{There either is a proof missing here or a confirmation that no proof is required at all.}\end{thm}\begin{de}An infinite set of events is called mutually independent if every finite subset is mutually independent. \end{de}\section{Random Variables}\begin{de}$\mathcal{B}\kern-2pt\left(\mathbb{R}\right)$ is the $\sigma{}$-algebra generated by $\left\{\interval[open left]{-\infty{}}{a}\mid{}-\infty{}<a<+\infty{}\right\}$. \end{de}\begin{de}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. A real function as follows is called a \index{random variable}\textbf{random variable} or \index{stochastic variable}\textbf{stochastic variable}. \[X\kern-2pt:\, \Omega{}\rightarrow{}\mathbb{R}\]\[\forall{}B\in{}\mathcal{B}\kern-2pt\left(\mathbb{R}\right):\ {X}^{-1}\kern-2pt\left(B\right)=\left\{\omega{}\mid{}X\kern-2pt\left(\omega{}\right)\in{}B\right\}\]\end{de}\begin{de}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $X$ be a random variable. $X$ is called a $\mathcal{A}$-\index{measure}\textbf{measure}. \end{de}\begin{de}A $\mathcal{B}\kern-2pt\left(\mathbb{R}\right)$-measure in the measurable space $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right)$ is called \index{Borel-measure}\textbf{Borel-measure}. \end{de}\begin{thm}A function $X\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}$ is a random variable in the measurable space $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right)$ if and only if the following holds. \[\forall{}A\in{}\mathbb{R}:\ {X}^{-1}\kern-2pt\left(\interval[open left]{-\infty{}}{A}\right)=\left\{\omega{}\mid{}X\kern-2pt\left(\omega{}\right)\leq{}A\right\}\in{}\mathcal{A}\]\end{thm}\begin{thm}A Borel-measurable function induces a probability measure ${P}_{X}$ on $\mathcal{B}\kern-2pt\left(\mathbb{R}\right)$ in $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$ as follows. \[{P}_{X}\kern-2pt\left(B\right)=P\kern-2pt\left(X\in{}B\right)=P\kern-2pt\left({X}^{-1}\kern-2pt\left(B\right)\right)\]\[{P}_{X}\kern-2pt\left(B\right)=P\kern-2pt\left(\left\{\omega{}\in{}\Omega{}\mid{}X\kern-2pt\left(\omega{}\right)\in{}B\right\}\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsection{Cumulative distribution function}\begin{de}Let $\Omega{}, \mathcal{A}, P$ be a \index{probability space}probability space. Let $X\kern-2pt:\, \Omega{}\rightarrow{}\mathbb{R}$ be a random variable. The \index{cumulative distribution function}\textbf{cumulative distribution function} (\index{CDF}\textbf{CDF}) or \index{termdistribution function}\textbf{termdistribution function} as follows. \[{F}_{X}\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}:\ a\mapsto{}{F}_{X}\kern-2pt\left(\interval[open left]{-\infty{}}{a}\right)=P\kern-2pt\left(\left\{\omega{}\mid{}X\kern-2pt\left(\omega{}\right)\right\}\right)=P\kern-2pt\left(X\leq{}a\right)\]\end{de}\begin{thm}Let $X$ be a random variable in $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$. A function ${F}_{X}\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}$ is a \index{cumulative distribution function}cumulative distribution function if and only if it has the following three properties. \begin{enumerate}\item{}${F}_{X}$ is monotonically increasing. \[\forall{}a, b\in{}\mathbb{R}:\ a\leq{}b\Rightarrow{}{F}_{X}\kern-2pt\left(a\right)\leq{}{F}_{X}\kern-2pt\left(b\right)\]\item{}\[{\lim}_{a\rightarrow{}-\infty{}}{F}_{X}\kern-2pt\left(a\right)=0\]\[{\lim}_{a\rightarrow{}+\infty{}}{F}_{X}\kern-2pt\left(a\right)=1\]\item{}${F}_{X}$ is right-continuous. \[\forall{}a\in{}\mathbb{R}:\ {\lim}_{h\overset{>}{\rightarrow{}}}0{F}_{X}\kern-2pt\left(a+h\right)={F}_{X}\kern-2pt\left(a\right)\]\todo[color=red,inline,size=\small]{There is a reference to ``right-continuous'' missing here. }\end{enumerate}\todo[color=red,inline,size=\small]{There either is a proof missing here or a confirmation that no proof is required at all.}\end{thm}\begin{thm}Let ${F}_{X}$ be a distribution function in $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$. \[\forall{}a, b\in{}\mathbb{R}:\ P\kern-2pt\left(a<X\leq{}b\right)={F}_{X}\kern-2pt\left(b\right)-{F}_{X}\kern-2pt\left(a\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}Let ${F}_{X}$ be a distribution function in $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$. \[\forall{}a\in{}\mathbb{R}:\ P\kern-2pt\left(X>a\right)=1-{F}_{X}\kern-2pt\left(a\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{de}Let $X$ and $Y$ be random variables in $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$. $X$ and $Y$ are called \index{independent}\textbf{independent} if and only if every two events $X\leq{}a$ and $Y\leq{}b$ are \index{independent}independent\footnote{See definition \ref{definition:independence-events-in-probabiliy-space} on page \pageref{definition:independence-events-in-probabiliy-space}. } events. \end{de}\subsection{The quantile function}\begin{de}The \index{quantile function}\textbf{quantile function} for $\mathbb{R}, \mathcal{B}\kern-2pt\left(\mathbb{R}\right), P$ is the inverse of the distribution function ${F}_{X}$. The value ${Q}_{X}\kern-2pt\left(p\right)$ is the smallest value $a\in{}\mathbb{R}$ for which ${F}_{X}\kern-2pt\left(a\right)\geq{}p$ holds. \end{de}\begin{de}The $0.25$, $0.5$ and $0.75$ quantile are respectively called the first, second and third \index{quartile}\textbf{quartile}. \end{de}\begin{de}The second quartile is called the \index{median}\textbf{median}. \end{de}\subsection{Types of random variables}\subsubsection{Discrete random variables}\begin{de}A random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ is called \index{discrete}\textbf{discrete} if the image under $X$ is non-zero in just a countable number of points. \[{p}_{i}=P\kern-2pt\left(\left\{\omega{}\in{}\Omega{}\mid{}X\kern-2pt\left(\omega{}\right)={x}_{i}\right\}\right)=P\kern-2pt\left(X={x}_{i}\right)\]\end{de}\begin{de}A \index{discrete distribution}\textbf{discrete distribution} ${\left({p}_{i}\right)}_{i}$ of a \index{discrete}discrete random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ is a sequence with the following properties. \begin{enumerate}\item{}$\forall{}i\in{}\mathbb{N}:\ {p}_{i}\geq{}0$\item{}${\sum_{{i}}}{{p}_{i}}=1$\end{enumerate}\end{de}\begin{thm}The distribution function ${F}_{X}$ of a \index{discrete}discrete random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ has a simpler formula. \[{F}_{X}\kern-2pt\left(a\right)=P\kern-2pt\left(X\leq{}a\right)={\sum_{{{x}_{i}\leq{}a}}}{{p}_{i}}\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\subsubsection{Continuous random variables}\begin{de}A random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ is called \index{continuous}\textbf{continuous} if the image of every point under $X$ is zero... \[\forall{}x\in{}\Omega{}:\ P\kern-2pt\left(\left\{x\right\}\right)=0\]... and the distribution function ${F}_{X}$ is a continuous function. \todo[color=red,inline,size=\small]{There is a reference to ``continuous function'' missing here. }\end{de}\begin{de}Let ${F}_{X}$ be a distribution function of a \index{continuous}continuous random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ that is continuous with a continuous derivative. The \index{probability density function}\textbf{probability density function} or \index{probability density}\textbf{probability density} ${f}_{X}$ is the following function. \[{f}_{X}\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}:\ x\mapsto{}{f}_{X}\kern-2pt\left(x\right)=\frac{d\;{F}_{X}\kern-2pt\left(x\right)}{dx}\]\end{de}\begin{thm}Let ${F}_{X}$ be a distribution function of a \index{continuous}continuous random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ that is continuous with a continuous derivative. Let ${f}_{X}$ be the probability density function of $X$. \[{F}_{X}\kern-2pt\left(a\right)=P\kern-2pt\left(x\leq{}a\right)={{\int}_{-\infty{}}}^{a}{f}_{X}\kern-2pt\left(x\right)\,x\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}Let ${F}_{X}$ be a distribution function of a \index{continuous}continuous random variable $X$ in a probability space $\Omega{}, \mathcal{A}, P$ that is continuous with a continuous derivative. Let ${f}_{X}$ be the probability density function of $X$. \[{F}_{X}\kern-2pt\left(x\right)-{F}_{X}\kern-2pt\left(a\right)=P\kern-2pt\left(a<X\leq{}b\right)={{\int}_{a}}^{b}{f}_{X}\kern-2pt\left(x\right)\,x\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\begin{thm}Let $X$ be a \index{continuous}continuous random variable in a probability space $\Omega{}, \mathcal{A}, P$ and let ${F}_{X}$ be the distribution function of $X$. \[{F}_{X}\kern-2pt\left(\interval[open]{a}{b}\right)={F}_{X}\kern-2pt\left(\interval[open left]{a}{b}\right)={F}_{X}\kern-2pt\left(\interval[open right]{a}{b}\right)={F}_{X}\kern-2pt\left(\interval{a}{b}\right)\]\todo[color=red,inline,size=\small]{There is a proof missing here.}\end{thm}\section{Important distributions}\subsection{Discrete distributions}\begin{de}The \index{discrete uniform distribution}\textbf{discrete uniform distribution} is defined only on a finite universe: $\Omega{}=\left\{{x}_{1}, {x}_{2}, \dotsc{}, {x}_{n}\right\}$. \[\forall{}i\in{}\left\{1, \dotsc{}, n\right\}:\ {p}_{i}=P\kern-2pt\left(\left\{{x}_{i}\right\}\right)=\frac{1}{n}\]\end{de}\begin{de}A \index{Bernoulli distribution}\textbf{Bernoulli distribution} is defined for a Bernoulli experiment\footnote{See definition \ref{definition:bernoulli-experiment} on page \pageref{definition:bernoulli-experiment}. }. \[X\sim{}\mathcal{B}\kern-2pt\left(1, p\right)\]\[\begin{cases}P\kern-2pt\left(X=1\right)&=p\\P\kern-2pt\left(X=0\right)&=q=1-p\end{cases}\]$p$ is called the \index{probability of success}\textbf{probability of success}. \end{de}\begin{de}A \index{binomial distribution}\textbf{binomial distribution} is the distribution of the sum $Y$ of $n$ times the same Bernoulli-distributed random variable $X$ with probability of success $p$. \[Y\sim{}\mathcal{B}\kern-2pt\left(n, p\right)\]\[Y=\sum_{i=1}^{n}{{X}_{i}}\]\end{de}\subsection{Continuous distributions}\chapter{Machine Learning}\begin{de}A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$ as measured by $P$ improves with experience. \end{de}\section{Supervised Learning}\subsection{The learning problem}Given a set of data points in an input space $X$, all tagged with a value in a certain output space $Y$, we search a function $f\kern-2pt:\, X\rightarrow{}Y$ that accurately predicts the output feature corresponding to the input value of new data points. \newline{}``Inputs'' is what they are called in machine learning. In statistical literature they are ofter called \index{predictors}\textbf{predictors}. In pattern recognition, these are called \index{feature}\textbf{feature}s. ``Outputs'' are called \index{dependent variables}\textbf{dependent variables} in statistics and \index{responses}\textbf{responses} in pattern recognition. The \index{measurement space}\textbf{measurement space} is the tuple $\left(X, Y\right)$. \subsection{Taxonomy of data}We are given an object space $\mathcal{O}$. A \index{measurement}\textbf{measurement} is a partial function from the object space to a \index{domain}\textbf{domain} $\mathbb{K}$. This measurement is used to gather data about the objects. Ideally the domain has some more convenient mathematical properties than the raw object spae. A carthesian product of object spaces can be an object space in itself. A single object space is called \index{monadic}\textbf{monadic}. A carthesian product of two object spaces is called \index{diadic}\textbf{diadic}. A carthesian product of multiple object spaces is called \index{polyadic}\textbf{polyadic}. \begin{ex}Let the object space be the set of all possible positions on the earth. The measurement could map a position into the temperature at that position. \[X\kern-2pt:\, \mathcal{O}\rightarrow{}\mathbb{R}\]\end{ex}\begin{ex}Let the object space be the carthesian product of the set of all websites ${\mathcal{O}}_{1}$ and  the set of all words ${\mathcal{O}}_{2}$. The measurement could be the amount of occurences of that word on that website. \[X\kern-2pt:\, {{\mathcal{O}}_{1}\times{}\mathcal{O}}_{2}\rightarrow{}\mathbb{N}\]\end{ex}\begin{ex}In preferential choice analysis, the object space is often the carthesian product of the set of test persons ${\mathcal{O}}_{1}$ with the set of choices ${\mathcal{O}}_{2}$ twice. The measurement then maps this space into a boolean choice. \[X\kern-2pt:\, {{{\mathcal{O}}_{1}\times{}\mathcal{O}}_{2}\times{}\mathcal{O}}_{2}\rightarrow{}\left\{left, right\right\}\]\end{ex}\subsection{Scales}Data are of different scales. This means that they have to be treated in different ways. Eventhough most measurements will be represented by numbers eventually, we cannot just treat them as numbers with all their properties depending on what the numbers represent. \begin{de}The \index{nominal scale}\textbf{nominal scale} describes qualitative measurements with a finite amount of possibilities. \end{de}\begin{ex}Data about presence or absence is nominal. \end{ex}\begin{ex}The taste categories: ``sweet, sour, salty, bitter'' are nominal. \end{ex}\begin{de}The \index{ordinal scale}\textbf{ordinal scale} describes data that is ranked with respect to an order. Only the order matters however, not the absolute values or the difference between values.. \end{de}\begin{ex}Typically self-assesment questions are on an ordinal scale. These may be questions like ``How happy are you?'' where you have to tick one of three boxes: ``A. Unhappy'', ``B. OK'', ``C. Happy''. \end{ex}\begin{de}The \index{interval scale}\textbf{interval scale} describes data where the difference between datapoints carries information. \end{de}\begin{ex}The Farenheit scale of temperature is an interval scale\end{ex}\begin{de}The \index{ratio scale}\textbf{ratio scale} describes data where the ``zero'' is meaningful but the measurement unit does not necessarily. \end{de}\begin{ex}The Kelvin scale of temperature is a ratio scale\end{ex}\begin{de}The \index{absolute scale}\textbf{absolute scale} describes data where also the measurement unit carries information. \end{de}\begin{ex}The amount of questions you got right on an exam is an absolute scale\end{ex}\subsection{Transformation Invariances}Data is sometimes transformed for various reasons. It is important that we realise that some transformations alter the data and some don't. If data is not altered by a transformation $t$ then that data is called $t$-invariant. \begin{figure}[H]\centering{}\begin{tabular}{|l|l|}\hline scale type&transformation invariances\\\hline \hline nominal&$\left\{f\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}\mid{}f\text{ is bijective.}\right\}$\\\hline ordinal&$\left\{f\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}\mid{}\forall{}{x}_{1}, {x}_{2}\in{}\mathbb{R}:\ f\kern-2pt\left({x}_{1}\right)<f\kern-2pt\left({x}_{2}\right)\right\}$\\\hline interval&$\left\{f\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}\mid{}\exists{}a\in{}{\mathbb{R}}^{+}, c\in{}\mathbb{R}:\ f\kern-2pt\left(x\right)=ax+c\right\}$\\\hline ratio&$\left\{f\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}\mid{}\exists{}a\in{}{\mathbb{R}}^{+}:\ f\kern-2pt\left(x\right)=ax\right\}$\\\hline absolute&$f\kern-2pt:\, \mathbb{R}\rightarrow{}\mathbb{R}:\ x\mapsto{}f\kern-2pt\left(x\right)=x$\\\hline \end{tabular}\caption{Transformation invariances for different scales}\end{figure}The quality of the estimation is assessed using a \index{loss function}\textbf{loss function} ${l}_{f}\kern-2pt:\, X\rightarrow{}{\mathbb{R}}^{+}$ that measures the difference between the actual output for a given data point and the predicted output. Examples of loss functions: \begin{itemize}\item{}\index{quadratic loss}\textbf{quadratic loss} (regression): ${l}_{f}\kern-2pt\left(x\right)={\left(y\kern-2pt\left(x\right)-f\kern-2pt\left(x\right)\right)}^{2}$\item{}\index{0-1 loss}\textbf{0-1 loss} (classification): ${l}_{f}\kern-2pt\left(x\right)={\mathbb{I}}_{y\neq{}x}$\item{}\index{exponential loss}\textbf{exponential loss} (classification): ${l}_{f}\kern-2pt\left(x\right)={e}^{-\beta{}Yf\kern-2pt\left(x\right)}$ for some $\beta{}$\end{itemize}\subsection{Regression}Regression is a supervised learning technique. It assumes that the input space is ${\mathbb{R}}^{p}$ and the output space is $\mathbb{R}$. It also assumes that the input $X$ the output $Y$ the parameters of the model $\theta{}$ and the noise on the observations $\epsilon{}$ can be modelled as random variables. \[Y=f\kern-2pt\left(\theta{}, X\right)+\epsilon{}\]\subsubsection{Linear Regression}\subsection{Linear Model and Least Squares}Let a data point be a $p$-dimensional vector of real numbers and let the output be a real number. Given a vector of inputs ${X}^{T}=\left({X}_{1}, {X}_{2}, \dotsc{}, {X}_{p}\right)$, we predict the output $Y$ via the following model. \[\hat{Y}={\hat{\beta{}}}_{0}+\sum_{j=1}^{p}{{X}_{j}{\hat{\beta{}}}_{j}}\]Here, ${\hat{\beta{}}}_{0}$ is called the \index{intercept}\textbf{intercept} or \index{bia}\textbf{bia}. To make equations easier, we often increase the size of the input vector by one by adding a constant $1$ in the zeroeth spot and representing the ${\hat{\beta{}}}_{j}$ in a vector ${\hat{\beta{}}}^{T}=\left({\hat{\beta{}}}_{0}, {\hat{\beta{}}}_{1}, \dotsc{}, {\hat{\beta{}}}_{p}\right)$. The linear model is then written in vector form as an inner product:. \[\hat{Y}={X}^{T}\hat{\beta{}}\]To fit the linear model to a set of training data, we pick the coefficients $\hat{\beta{}}$ sich that the \index{residual sum of squares}\textbf{residual sum of squares} \index{(RSS)}\textbf{(RSS)} is minimized. This is called the method of \index{least square}\textbf{least square}. \[RSS\kern-2pt\left(\beta{}\right)={\sum_{i=1}^{N}{\left({y}_{i}-{{x}_{i}}^{T}\beta{}\right)}}^{2}={\left(Y-X\beta{}\right)}^{T}\left(Y-X\beta{}\right)\]Differentiating with respect to $\beta{}$, we get the \index{normal equation}\textbf{normal equation}. \[{X}^{T}\left(y-X\beta{}\right)=0\]If ${X}^{T}X$ is invertible, then the unique solution is given by the folowing equation:. \[\hat{\beta{}}={\left({X}^{T}X\right)}^{-1}{X}^{T}Y\]The fitted value for input ${x}_{i}$ is then $\hat{{y}_{i}}={{x}^{T}}_{i}\hat{\beta{}}$ and the prediction for an arbitrary input $x$ would be ${\hat{y}\kern-2pt\left(x\right)}_{0}={x}^{T}\hat{\beta{}}$. Viewed as a function over the $p$-dimensional input space, the ideal model: $f$ is linear. \[f\kern-2pt:\, {\mathbb{R}}^{p}\rightarrow{}\mathbb{R}:\ X\mapsto{}{X}^{T}\beta{}\]\section{Unsupervised Learning}\subsection{The learning problem}Building an intelligent system for supervised learning consists of the following steps. \begin{itemize}\item{}Representation of objects\item{}Definition of a structure\item{}Optimization\item{}Validation\end{itemize}\begin{ex}Say you are asked to build a system that takes as input an image of a written digit and is supposed to output which digit it is. In this case, the objects are digits, numbers between 0 and 9, and measurements are the pixel values for an image. \end{ex}\chapter{Data Mining}\section{MapReduce}\subsection{The Concept}\subsubsection{The problem}The biggest problem with handling massive data sets is often not the complexity of the algorithm that we are dealing with but the fact that the data does not fit into memory. Secondary memory is multiple orders of magnitude slower than main memory. Often the data does not even fit onto one machine's hard disk. As a consequence, the bottleneck in a naive approach would be IO and networking.. Random access to the data is no longer an option when handling massive data sets. When using an algorithm that handles streaming data, IO ceases to be the bottleneck but only untill the machine has run out of data to process in its storage. A new way of processing data is required. \subsubsection{The solution}In a MapReduce framework, the computation is distributed accross multiple machines and the data is distributed on a distributed filesystem for those machines. The concept of MapReduce is is based on three key ideas. \begin{enumerate}\item{}Data is stored redundantly for reliability. \item{}Computational power is moved as close as possible to the data it is trying to process. \item{}Provide a unified programming model to simplify the massive parallelism. \end{enumerate}The MapReduce framework is optimised for the common use case. \begin{itemize}\item{}Massive amounts of data. \item{}Infrequent updates of those data. \item{}Frequent reads of and appends to those data. \end{itemize}\begin{de}A \index{map}\textbf{map} is a function $Map\kern-2pt:\, {\left(K\times{}V\right)}^{p}\rightarrow{}{\left(K'\times{}V'\right)}^{q}$. \end{de}\begin{de}A \index{reduce}\textbf{reduce} is a function $Reduce\kern-2pt:\, K'\times{}{V'}^{r}\rightarrow{}{\left(K''\times{}V''\right)}^{s}$. \end{de}\begin{nte}$K$ and $K'$ are sets of so-called \index{key}\textbf{key}s and $V$ and $V'$ are sets of so-called \index{value}\textbf{value}s. \end{nte}\begin{nte}In reality these functions will be implemented by a computation and can, in theory, have side-effects but for the sake of clarity and efficiency it is best to try to avoid such a situation. \end{nte}The programmer supplies both a map and a reduce function. The framework distributes the code and the data accross all the machines. The machines each run the map function over their part of the data. The framework then collects these data and sorts them by key before distributing it over the machines again. Lastly the machines run the reduce function over each set of values with the same key. \newline{}It turns out that a lot of problems can be solved using this framework\subsection{Examples}\begin{ex}Counting the number of occurrences of every word in a large corpus of documents is the prime example of a problem that can be solved using mapreduce. 
\begin{minted}{python}
# key: document name; value: text of document
map(key, value):
    for w in value:
        emit(w, 1)
\end{minted}

\begin{minted}{python}
# key: a word; value: an iterator over counts
reduce(key, values):
    result = 0
    for v in values:
        result += v
    emit(key, result)
\end{minted}
\end{ex}\nocite{data-mining-mapreduce}\section{Approximate Retrieval}\subsection{The problem}The general problem of approximate retrieval consists of retrieving items that are similar to a given query item. Abstractly these items are elements of a \index{pseudometric space}pseudometric space\footnote{See definition \ref{definition:pseudometric-space} on page \pageref{definition:pseudometric-space}. }. \subsection{Near Duplicate Detection}Given a set of items $V$ in a pseudometric space $\left(X, d\right)$ an item $w'$ in $V$ and a distance $\epsilon{}\in{}{\mathbb{R}}^{+}$, find all the items $w$ closer than $\epsilon{}$ to $w'$. For arbitrary pseudometric spaces, this can be arbitrarily hard. In reality only the item space is given so we can construct our own pseudometric that coincides of what naturally 'feels' like a distance and solve the problem for that specific case. \subsubsection{Locality Sensitive Hashing}Locality sensitive hashing is an solution approach for the problem of near duplicate detection.. It assumes that there exists a function $f\kern-2pt:\, V\rightarrow{}{{\mathbb{F}}_{2}}^{D}$ that transforms an item into a vector of bits. The \index{Jaccard similarity}Jaccard similarity\footnote{See definition \ref{definition:jaccard-similarity} on page \pageref{definition:jaccard-similarity}. } is then used as the distance between items (after applying $f$). \footnote{The Jaccard similarity is defined on sets, not Boolean vector spaces. That is not a problem because there exists a bijection from ${{\mathbb{F}}_{2}}^{D}$ to the powerset of any set with $D$ elements. In that case a `one' coordinate represents the presence of that element in the subsect. }\newline{}Naively, one could calculate this distance explicitly for every pair of items and declare duplicates whenever that distance is smaller than $\epsilon{}$. Since this solution has a quadratic time complexity, it is infeasable for even moderately large $N$. \newline{}If this problem was about exact duplicates then people would scream ``Use hashfunctions!'' because it is such a great solution. Locality sensitive hashing builds on that sentiment but uses so-called \index{locality sensitive hashfunction}\textbf{locality sensitive hashfunction}s instead of regular hash functions. The idea is that similar items should hash to similar values. \footnote{That's exactly the opposite of what you would want in cryptographic hash functions. }\bibliographystyle{plain}\bibliography{main}\printindex{}\listoftodos{}\end{document}